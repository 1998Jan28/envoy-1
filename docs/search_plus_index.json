{"./":{"url":"./","title":"介绍","keywords":"","body":"Envoy 官方文档中文版 Envoy 官方文档中文版，基于 Envoy 最新的 1.7 版本。 本项目文档地址：https://github.com/servicemesher/envoy 英文官方文档地址：https://www.envoyproxy.io/docs/envoy/latest GitHub Pages 阅读地址：https://servicemesher.github.io/envoy/ 参与翻译请先阅读翻译规范。 本文档使用 Gitbook 生成。 "},"about_docs.html":{"url":"about_docs.html","title":"关于本文档","keywords":"","body":"关于本文档 Envoy 文档由以下几个主要部分组成： 简介：本部分介绍 Envoy 的概况、体系结构概述、典型部署方式等。 入门：使用 Docker 快速开始使用 Envoy。 安装：如何使用 Docker 构建/安装 Envoy。 配置：遗留 v1 API 和新 v2 API 共同的详细配置指令。相关时，配置指南还包含有关统计信息、运行时配置和 API 的信息。 操作：有关如何操作 Envoy 的常规信息，包括命令行界面、热重启包装器、管理界面、常规统计概览等。 扩展 Envoy：有关如何为 Envoy 编写自定义过滤器的信息。 v1 API 参考：特定于遗留 v1 API 的配置详细信息。 v2 API 参考：特定于新 v2 API 的配置细节。 Envoy 常见问题：有疑问？希望我们的答案能让您满意。 "},"intro/what_is_envoy.html":{"url":"intro/what_is_envoy.html","title":"Envoy 是什么？","keywords":"","body":"Envoy 是什么？ Envoy is an L7 proxy and communication bus designed for large modern service oriented architectures. The project was born out of the belief that: The network should be transparent to applications. When network and application problems do occur it should be easy to determine the source of the problem. In practice, achieving the previously stated goal is incredibly difficult. Envoy attempts to do so by providing the following high level features: Out of process architecture: Envoy is a self contained process that is designed to run alongside every application server. All of the Envoys form a transparent communication mesh in which each application sends and receives messages to and from localhost and is unaware of the network topology. The out of process architecture has two substantial benefits over the traditional library approach to service to service communication: Envoy works with any application language. A single Envoy deployment can form a mesh between Java, C++, Go, PHP, Python, etc. It is becoming increasingly common for service oriented architectures to use multiple application frameworks and languages. Envoy transparently bridges the gap. As anyone that has worked with a large service oriented architecture knows, deploying library upgrades can be incredibly painful. Envoy can be deployed and upgraded quickly across an entire infrastructure transparently. Modern C++11 code base: Envoy is written in C++11. Native code was chosen because we believe that an architectural component such as Envoy should get out of the way as much as possible. Modern application developers already deal with tail latencies that are difficult to reason about due to deployments in shared cloud environments and the use of very productive but not particularly well performing languages such as PHP, Python, Ruby, Scala, etc. Native code provides generally excellent latency properties that don’t add additional confusion to an already confusing situation. Unlike other native code proxy solutions written in C, C++11 provides both excellent developer productivity and performance. L3/L4 filter architecture: At its core, Envoy is an L3/L4 network proxy. A pluggable filter chain mechanism allows filters to be written to perform different TCP proxy tasks and inserted into the main server. Filters have already been written to support various tasks such as raw TCP proxy,HTTP proxy, TLS client certificate authentication, etc. HTTP L7 filter architecture: HTTP is such a critical component of modern application architectures that Envoy supports an additional HTTP L7 filter layer. HTTP filters can be plugged into the HTTP connection management subsystem that perform different tasks such as buffering, rate limiting, routing/forwarding, sniffing Amazon’s DynamoDB, etc. First class HTTP/2 support: When operating in HTTP mode, Envoy supports both HTTP/1.1 and HTTP/2. Envoy can operate as a transparent HTTP/1.1 to HTTP/2 proxy in both directions. This means that any combination of HTTP/1.1 and HTTP/2 clients and target servers can be bridged. The recommended service to service configuration uses HTTP/2 between all Envoys to create a mesh of persistent connections that requests and responses can be multiplexed over. Envoy does not support SPDY as the protocol is being phased out. HTTP L7 routing: When operating in HTTP mode, Envoy supports a routing subsystem that is capable of routing and redirecting requests based on path, authority, content type, runtime values, etc. This functionality is most useful when using Envoy as a front/edge proxy but is also leveraged when building a service to service mesh. gRPC support: gRPC is an RPC framework from Google that uses HTTP/2 as the underlying multiplexed transport. Envoy supports all of the HTTP/2 features required to be used as the routing and load balancing substrate for gRPC requests and responses. The two systems are very complementary. MongoDB L7 support: MongoDB is a popular database used in modern web applications. Envoy supports L7 sniffing, statistics production, and logging for MongoDB connections. DynamoDB L7 support: DynamoDB is Amazon’s hosted key/value NOSQL datastore. Envoy supports L7 sniffing and statistics production for DynamoDB connections. Service discovery: Service discovery is a critical component of service oriented architectures. Envoy supports multiple service discovery methods including asynchronous DNS resolution and REST based lookup via a service discovery service. Health checking: The recommended way of building an Envoy mesh is to treat service discovery as an eventually consistent process. Envoy includes a health checking subsystem which can optionally perform active health checking of upstream service clusters. Envoy then uses the union of service discovery and health checking information to determine healthy load balancing targets. Envoy also supports passive health checking via an outlier detection subsystem. Advanced load balancing: Load balancing among different components in a distributed system is a complex problem. Because Envoy is a self contained proxy instead of a library, it is able to implement advanced load balancing techniques in a single place and have them be accessible to any application. Currently Envoy includes support for automatic retries, circuit breaking, global rate limiting via an external rate limiting service, request shadowing, and outlier detection. Future support is planned for request racing. Front/edge proxy support: Although Envoy is primarily designed as a service to service communication system, there is benefit in using the same software at the edge (observability, management, identical service discovery and load balancing algorithms, etc.). Envoy includes enough features to make it usable as an edge proxy for most modern web application use cases. This includes TLS termination, HTTP/1.1 and HTTP/2 support, as well as HTTP L7 routing. Best in class observability: As stated above, the primary goal of Envoy is to make the network transparent. However, problems occur both at the network level and at the application level. Envoy includes robust statistics support for all subsystems. statsd (and compatible providers) is the currently supported statistics sink, though plugging in a different one would not be difficult. Statistics are also viewable via the administration port. Envoy also supports distributed tracing via thirdparty providers. Dynamic configuration: Envoy optionally consumes a layered set of dynamic configuration APIs. Implementors can use these APIs to build complex centrally managed deployments if desired. 设计目标 A short note on the design goals of the code itself: Although Envoy is by no means slow (we have spent considerable time optimizing certain fast paths), the code has been written to be modular and easy to test versus aiming for the greatest possible absolute performance. It’s our view that this is a more efficient use of time given that typical deployments will be alongside languages and runtimes many times slower and with many times greater memory usage. "},"intro/arch_overview/terminology.html":{"url":"intro/arch_overview/terminology.html","title":"术语","keywords":"","body":"术语 在深入主架构文档之前的一些定义。部分定义在行业中略有争议，下面将展开在Envoy文档和代码库中如何使用它们。 Host/主机: 能够进行网络通信的实体（如手机，服务器等上的应用程序）。在此文档中，主机是逻辑网络应用程序。一块物理硬件上可能运行有多个主机，只要他们可以独立寻址。 Downstream/下游: 下游主机连接到Envoy, 发送请求并接收响应。 Upstream/上游: 上游主机接收来自Envoy的连接和请求，并返回响应。 Listener/监听器: 监听器是命名网地址(例如, 端口, unix domain socket, 等)，可以被下游客户端连接。Envoy暴露一个或者多个监听器给下游主机连接。 Cluster/集群: 群集是指Envoy连接到的逻辑上相同的一组上游主机。Envoy通过服务发现来发现集群的成员。可以可选择的通过主动健康检查来确定集群成员的健康状态。Envoy通过负载均衡策略决定将请求路由到哪个集群成员。 Mesh/网格: 一组主机，协调好以提供一致的网络拓扑。在本文档中，“Envoy mesh”是一组Envoy代理，它们构成了分布式系统的消息传递基础，这个分布式系统由很多不同服务和应用程序平台组成。 Runtime configuration/运行时配置: 外置实时配置系统，和Envoy一起部署。可以更改配置设置，影响操作，而无需重启Envoy或更改主要配置。 "},"intro/arch_overview/threading_model.html":{"url":"intro/arch_overview/threading_model.html","title":"线程模型","keywords":"","body":"线程模型 Envoy uses a single process with multiple threads architecture. A single master thread controls various sporadic coordination tasks while some number of worker threads perform listening, filtering, and forwarding. Once a connection is accepted by a listener, the connection spends the rest of its lifetime bound to a single worker thread. This allows the majority of Envoy to be largely single threaded (embarrassingly parallel) with a small amount of more complex code handling coordination between the worker threads. Generally Envoy is written to be 100% non-blocking and for most workloads we recommend configuring the number of worker threads to be equal to the number of hardware threads on the machine. "},"intro/arch_overview/listeners.html":{"url":"intro/arch_overview/listeners.html","title":"监听器","keywords":"","body":"监听器 Envoy配置支持在单个进程中启用任意数量的监听器。通常建议每台机器运行单个Envoy，而不必介意配置的监听器数量。这样运维更简单，而且只有单个统计来源。目前Envoy只支持TCP监听器。 每个监听器都独立配置有一些（L3 / L4）网络级别的过滤器。当监听器接接收到新连接时，配置好的连接本地过滤器将被实例化，并开始处理后续事件。通用监听器架构用于执行绝大多数不同的代理任务（例如，限速，TLS客户端认证, HTTP 连接管理, MongoDB sniffing, 原始 TCP 代理等）。 监听器也可以选择性的配置某些监听器过滤器。 这些过滤器的处理在网络级别过滤器之前进行，并有机会操纵连接元数据，通常会影响后续过滤器或集群处理连接的方式。 监听器也可以通过监听器发现服务 (LDS)动态获取。 监听器 配置. "},"intro/arch_overview/listener_filters.html":{"url":"intro/arch_overview/listener_filters.html","title":"监听器过滤器","keywords":"","body":"监听器过滤器 在监听器一节中讨论到，监听器过滤器可以用于操纵连接元数据。监听器过滤器的主要目的是来更方便地添加系统集成功能，而无需更改Envoy核心功能，并且还可以让多个此类功能之间的交互更加明确。 监听器过滤器的API相对简单，因为最终这些过滤器在新接收的套接字上操作。链中的过滤器可以停止后面的过滤器并随后继续支持。这允许更复杂的场景，例如调用限速服务等。Envoy包含多个监听器过滤器，这些过滤器在此架构概述以及配置参考中都有记录。 "},"intro/arch_overview/network_filters.html":{"url":"intro/arch_overview/network_filters.html","title":"Network (L3/L4) filter","keywords":"","body":"网络 (L3/L4) 过滤器 如监听器一节所述，网络级（L3/L4）过滤器构成Envoy连接处理的核心。过滤器API允许混合不同的过滤器组合，并匹配和附加到给定的监听器。有三种不同类型的网络过滤器： 读: 当Envoy从下游连接接收数据时，调用读过滤器。 写: 当Envoy要发送数据到下游连接时，调用写过滤器。 读/写: 当Envoy从下游连接接收数据和要发送数据到下游连接时，调用读/写过滤器。 网络级过滤器的API相对简单，因为最终过滤器只操作原始字节和少量连接事件（例如，TLS握手完成，连接在本地或远程断开等）。链中的过滤器可以停止后续的过滤器，并随后继续。这可以实现更复杂的场景，例如调用限速服务等。Envoy包含多个网络级的过滤器，这些过滤器在此架构概述和配置参考中都有说明。 "},"intro/arch_overview/http_connection_management.html":{"url":"intro/arch_overview/http_connection_management.html","title":"HTTP 连接管理","keywords":"","body":"HTTP 连接管理 HTTP is such a critical component of modern service oriented architectures that Envoy implements a large amount of HTTP specific functionality. Envoy has a built in network level filter called theHTTP connection manager. This filter translates raw bytes into HTTP level messages and events (e.g., headers received, body data received, trailers received, etc.). It also handles functionality common to all HTTP connections and requests such as access logging, request ID generation and tracing, request/response header manipulation, route table management, and statistics. HTTP connection manager configuration. HTTP 协议 Envoy’s HTTP connection manager has native support for HTTP/1.1, WebSockets, and HTTP/2. It does not support SPDY. Envoy’s HTTP support was designed to first and foremost be an HTTP/2 multiplexing proxy. Internally, HTTP/2 terminology is used to describe system components. For example, an HTTP request and response take place on a stream. A codec API is used to translate from different wire protocols into a protocol agnostic form for streams, requests, responses, etc. In the case of HTTP/1.1, the codec translates the serial/pipelining capabilities of the protocol into something that looks like HTTP/2 to higher layers. This means that the majority of the code does not need to understand whether a stream originated on an HTTP/1.1 or HTTP/2 connection. HTTP header sanitizing The HTTP connection manager performs various header sanitizing actions for security reasons.路由表 路由表配置 Each HTTP connection manager filter has an associated route table. The route table can be specified in one of two ways: Statically. Dynamically via the RDS API. "},"intro/arch_overview/http_filters.html":{"url":"intro/arch_overview/http_filters.html","title":"HTTP filter","keywords":"","body":"HTTP filter Much like the network level filter stack, Envoy supports an HTTP level filter stack within the connection manager. Filters can be written that operate on HTTP level messages without knowledge of the underlying physical protocol (HTTP/1.1, HTTP/2, etc.) or multiplexing capabilities. There are three types of HTTP level filters: Decoder: Decoder filters are invoked when the connection manager is decoding parts of the request stream (headers, body, and trailers). Encoder: Encoder filters are invoked when the connection manager is about to encode parts of the response stream (headers, body, and trailers). Decoder/Encoder: Decoder/Encoder filters are invoked both when the connection manager is decoding parts of the request stream and when the connection manager is about to encode parts of the response stream. The API for HTTP level filters allows the filters to operate without knowledge of the underlying protocol. Like network level filters, HTTP filters can stop and continue iteration to subsequent filters. This allows for more complex scenarios such as health check handling, calling a rate limiting service, buffering, routing, generating statistics for application traffic such as DynamoDB, etc. Envoy already includes several HTTP level filters that are documented in this architecture overview as well as the configuration reference. "},"intro/arch_overview/http_routing.html":{"url":"intro/arch_overview/http_routing.html","title":"HTTP 路由","keywords":"","body":"HTTP 路由 Envoy includes an HTTP router filter which can be installed to perform advanced routing tasks. This is useful both for handling edge traffic (traditional reverse proxy request handling) as well as for building a service to service Envoy mesh (typically via routing on the host/authority HTTP header to reach a particular upstream service cluster). Envoy also has the ability to be configured as forward proxy. In the forward proxy configuration, mesh clients can participate by appropriately configuring their http proxy to be an Envoy. At a high level the router takes an incoming HTTP request, matches it to an upstream cluster, acquires a connection pool to a host in the upstream cluster, and forwards the request. The router filter supports the following features: Virtual hosts that map domains/authorities to a set of routing rules. Prefix and exact path matching rules (both case sensitive and case insensitive). Regex/slug matching is not currently supported, mainly because it makes it difficult/impossible to programmatically determine whether routing rules conflict with each other. For this reason we don’t recommend regex/slug routing at the reverse proxy level, however we may add support in the future depending on demand. TLS redirection at the virtual host level. Path/host redirection at the route level. Direct (non-proxied) HTTP responses at the route level. Explicit host rewriting. Automatic host rewriting based on the DNS name of the selected upstream host. Prefix rewriting. Websocket upgrades at route level. Request retries specified either via HTTP header or via route configuration. Request timeout specified either via HTTP header or via route configuration. Traffic shifting from one upstream cluster to another via runtime values (see traffic shifting/splitting). Traffic splitting across multiple upstream clusters using weight/percentage-based routing (see traffic shifting/splitting). Arbitrary header matching routing rules. Virtual cluster specifications. A virtual cluster is specified at the virtual host level and is used by Envoy to generate additional statistics on top of the standard cluster level ones. Virtual clusters can use regex matching. Priority based routing. Hash policy based routing. Absolute urls are supported for non-tls forward proxies. 路由表 The configuration for the HTTP connection manager owns the route table that is used by all configured HTTP filters. Although the router filter is the primary consumer of the route table, other filters also have access in case they want to make decisions based on the ultimate destination of the request. For example, the built in rate limit filter consults the route table to determine whether the global rate limit service should be called based on the route. The connection manager makes sure that all calls to acquire a route are stable for a particular request, even if the decision involves randomness (e.g. in the case of a runtime configuration route rule). 重试语义 Envoy allows retries to be configured both in the route configuration as well as for specific requests via request headers. The following configurations are possible: Maximum number of retries: Envoy will continue to retry any number of times. An exponential backoff algorithm is used between each retry. Additionally, all retries are contained within the overall request timeout. This avoids long request times due to a large number of retries. Retry conditions: Envoy can retry on different types of conditions depending on application requirements. For example, network failure, all 5xx response codes, idempotent 4xx response codes, etc. Note that retries may be disabled depending on the contents of the x-envoy-overloaded. 优先级路由 Envoy supports priority routing at the route level. The current priority implementation uses different connection pool and circuit breaking settings for each priority level. This means that even for HTTP/2 requests, two physical connections will be used to an upstream host. In the future Envoy will likely support true HTTP/2 priority over a single connection. The currently supported priorities are default and high. 直接响应 Envoy supports the sending of “direct” responses. These are preconfigured HTTP responses that do not require proxying to an upstream server. There are two ways to specify a direct response in a Route: Set the direct_response field. This works for all HTTP response statuses. Set the redirect field. This works for redirect response statuses only, but it simplifies the setting of the Location header. A direct response has an HTTP status code and an optional body. The Route configuration can specify the response body inline or specify the pathname of a file containing the body. If the Route configuration specifies a file pathname, Envoy will read the file upon configuration load and cache the contents. Attention If a response body is specified, it must be no more than 4KB in size, regardless of whether it is provided inline or in a file. Envoy currently holds the entirety of the body in memory, so the 4KB limit is intended to keep the proxy’s memory footprint from growing too large. If response_headers_to_add has been set for the Route or the enclosing Virtual Host, Envoy will include the specified headers in the direct HTTP response. "},"intro/arch_overview/grpc.html":{"url":"intro/arch_overview/grpc.html","title":"gRPC","keywords":"","body":"gRPC gRPC is an RPC framework from Google. It uses protocol buffers as the underlying serialization/IDL format. At the transport layer it uses HTTP/2 for request/response multiplexing. Envoy has first class support for gRPC both at the transport layer as well as at the application layer: gRPC makes use of HTTP/2 trailers to convey request status. Envoy is one of very few HTTP proxies that correctly supports HTTP/2 trailers and is thus one of the few proxies that can transport gRPC requests and responses. The gRPC runtime for some languages is relatively immature. Envoy supports a gRPC bridge filter that allows gRPC requests to be sent to Envoy over HTTP/1.1. Envoy then translates the requests to HTTP/2 for transport to the target server. The response is translated back to HTTP/1.1. When installed, the bridge filter gathers per RPC statistics in addition to the standard array of global HTTP statistics. gRPC-Web is supported by a filter that allows a gRPC-Web client to send requests to Envoy over HTTP/1.1 and get proxied to a gRPC server. It’s under active development and is expected to be the successor to the gRPC bridge filter. gRPC-JSON transcoder is supported by a filter that allows a RESTful JSON API client to send requests to Envoy over HTTP and get proxied to a gRPC service. gRPC 服务 In addition to proxying gRPC on the data plane, Envoy make use of gRPC for its control plane, where it fetches configuration from management server(s) and also in filters, for example for rate limiting or authorization checks. We refer to these as gRPC services. When specifying gRPC services, it’s necessary to specify the use of either the Envoy gRPC client or the Google C++ gRPC client. We discuss the tradeoffs in this choice below. The Envoy gRPC client is a minimal custom implementation of gRPC that makes use of Envoy’s HTTP/2 upstream connection management. Services are specified as regular Envoy clusters, with regular treatment of timeouts, retries, endpoint discovery/load balancing/failover/load reporting, circuit breaking, health checks, outlier detection. They share the same connection poolingmechanism as the Envoy data plane. Similarly, cluster statistics are available for gRPC services. Since the client is minimal, it does not include advanced gRPC features such as OAuth2 or gRPC-LBlookaside. The Google C++ gRPC client is based on the reference implementation of gRPC provided by Google at https://github.com/grpc/grpc. It provides advanced gRPC features that are missing in the Envoy gRPC client. The Google C++ gRPC client performs its own load balancing, retries, timeouts, endpoint management, etc, independent of Envoy’s cluster management. The Google C++ gRPC client also supports custom authentication plugins. It is recommended to use the Envoy gRPC client in most cases, where the advanced features in the Google C++ gRPC client are not required. This provides configuration and monitoring simplicity. Where necessary features are missing in the Envoy gRPC client, the Google C++ gRPC client should be used instead. "},"intro/arch_overview/websocket.html":{"url":"intro/arch_overview/websocket.html","title":"WebSocket 支持","keywords":"","body":"WebSocket 支持 Envoy supports upgrading a HTTP/1.1 connection to a WebSocket connection. Connection upgrade will be allowed only if the downstream client sends the correct upgrade headers and the matching HTTP route is explicitly configured to use WebSockets (use_websocket). If a request arrives at a WebSocket enabled route without the requisite upgrade headers, it will be treated as any regular HTTP/1.1 request. Since Envoy treats WebSocket connections as plain TCP connections, it supports all drafts of the WebSocket protocol, independent of their wire format. Certain HTTP request level features such as redirects, timeouts, retries, rate limits and shadowing are not supported for WebSocket routes. However, prefix rewriting, explicit and automatic host rewriting, traffic shifting and splitting are supported. 连接语义 Even though WebSocket upgrades occur over HTTP/1.1 connections, WebSockets proxying works similarly to plain TCP proxy, i.e., Envoy does not interpret the websocket frames. The downstream client and/or the upstream server are responsible for properly terminating the WebSocket connection (e.g., by sending close frames) and the underlying TCP connection. When the connection manager receives a WebSocket upgrade request over a WebSocket-enabled route, it forwards the request to an upstream server over a TCP connection. Envoy will not know if the upstream server rejected the upgrade request. It is the responsibility of the upstream server to terminate the TCP connection, which would cause Envoy to terminate the corresponding downstream client connection. "},"intro/arch_overview/cluster_manager.html":{"url":"intro/arch_overview/cluster_manager.html","title":"Cluster manager","keywords":"","body":"Cluster manager Envoy’s cluster manager manages all configured upstream clusters. Just as the Envoy configuration can contain any number of listeners, the configuration can also contain any number of independently configured upstream clusters. Upstream clusters and hosts are abstracted from the network/HTTP filter stack given that upstream clusters and hosts may be used for any number of different proxy tasks. The cluster manager exposes APIs to the filter stack that allow filters to obtain a L3/L4 connection to an upstream cluster, or a handle to an abstract HTTP connection pool to an upstream cluster (whether the upstream host supports HTTP/1.1 or HTTP/2 is hidden). A filter stage determines whether it needs an L3/L4 connection or a new HTTP stream and the cluster manager handles all of the complexity of knowing which hosts are available and healthy, load balancing, thread local storage of upstream connection data (since most Envoy code is written to be single threaded), upstream connection type (TCP/IP, UDS), upstream protocol where applicable (HTTP/1.1, HTTP/2), etc. Clusters known to the cluster manager can be configured either statically, or fetched dynamically via the cluster discovery service (CDS) API. Dynamic cluster fetches allow more configuration to be stored in a central configuration server and thus requires fewer Envoy restarts and configuration distribution. Cluster manager configuration. CDS configuration. Cluster warming When clusters are initialized both at server boot as well as via CDS, they are “warmed.” This means that clusters do not become available until the following operations have taken place. Initial service discovery load (e.g., DNS resolution, EDS update, etc.). Initial active health check pass if active health checking is configured. Envoy will send a health check request to each discovered host to determine its initial health status. The previous items ensure that Envoy has an accurate view of a cluster before it begins using it for traffic serving. When discussing cluster warming, the cluster “becoming available” means: For newly added clusters, the cluster will not appear to exist to the rest of Envoy until it has been warmed. I.e., HTTP routes that reference the cluster will result in either a 404 or 503 (depending on configuration). For updated clusters, the old cluster will continue to exist and serve traffic. When the new cluster has been warmed, it will be atomically swapped with the old cluster such that no traffic interruptions take place. "},"intro/arch_overview/service_discovery.html":{"url":"intro/arch_overview/service_discovery.html","title":"服务发现","keywords":"","body":"Service discovery When an upstream cluster is defined in the configuration, Envoy needs to know how to resolve the members of the cluster. This is known as service discovery. Supported service discovery types Static Static is the simplest service discovery type. The configuration explicitly specifies the resolved network name (IP address/port, unix domain socket, etc.) of each upstream host. Strict DNS When using strict DNS service discovery, Envoy will continuously and asynchronously resolve the specified DNS targets. Each returned IP address in the DNS result will be considered an explicit host in the upstream cluster. This means that if the query returns three IP addresses, Envoy will assume the cluster has three hosts, and all three should be load balanced to. If a host is removed from the result Envoy assumes it no longer exists and will drain traffic from any existing connection pools. Note that Envoy never synchronously resolves DNS in the forwarding path. At the expense of eventual consistency, there is never a worry of blocking on a long running DNS query. Logical DNS Logical DNS uses a similar asynchronous resolution mechanism to strict DNS. However, instead of strictly taking the results of the DNS query and assuming that they comprise the entire upstream cluster, a logical DNS cluster only uses the first IP address returned when a new connection needs to be initiated. Thus, a single logical connection pool may contain physical connections to a variety of different upstream hosts. Connections are never drained. This service discovery type is optimal for large scale web services that must be accessed via DNS. Such services typically use round robin DNS to return many different IP addresses. Typically a different result is returned for each query. If strict DNS were used in this scenario, Envoy would assume that the cluster’s members were changing during every resolution interval which would lead to draining connection pools, connection cycling, etc. Instead, with logical DNS, connections stay alive until they get cycled. When interacting with large scale web services, this is the best of all possible worlds: asynchronous/eventually consistent DNS resolution, long lived connections, and zero blocking in the forwarding path. Original destination Original destination cluster can be used when incoming connections are redirected to Envoy either via an iptables REDIRECT or TPROXY target or with Proxy Protocol. In these cases requests routed to an original destination cluster are forwarded to upstream hosts as addressed by the redirection metadata, without any explicit host configuration or upstream host discovery. Connections to upstream hosts are pooled and unused hosts are flushed out when they have been idle longer thancleanup_interval_ms, which defaults to 5000ms. If the original destination address is is not available, no upstream connection is opened. Original destination service discovery must be used with the original destination load balancer. Service discovery service (SDS) The service discovery service is a generic REST based API used by Envoy to fetch cluster members. Lyft provides a reference implementation via the Python discovery service. That implementation uses AWS DynamoDB as the backing store, however the API is simple enough that it could easily be implemented on top of a variety of different backing stores. For each SDS cluster, Envoy will periodically fetch the cluster members from the discovery service. SDS is the preferred service discovery mechanism for a few reasons: Envoy has explicit knowledge of each upstream host (vs. routing through a DNS resolved load balancer) and can make more intelligent load balancing decisions. Extra attributes carried in the discovery API response for each host inform Envoy of the host’s load balancing weight, canary status, zone, etc. These additional attributes are used globally by the Envoy mesh during load balancing, statistic gathering, etc. Generally active health checking is used in conjunction with the eventually consistent service discovery service data to making load balancing and routing decisions. This is discussed further in the following section. On eventually consistent service discovery Many existing RPC systems treat service discovery as a fully consistent process. To this end, they use fully consistent leader election backing stores such as Zookeeper, etcd, Consul, etc. Our experience has been that operating these backing stores at scale is painful. Envoy was designed from the beginning with the idea that service discovery does not require full consistency. Instead, Envoy assumes that hosts come and go from the mesh in an eventually consistent way. Our recommended way of deploying a service to service Envoy mesh configuration uses eventually consistent service discovery along with active health checking (Envoy explicitly health checking upstream cluster members) to determine cluster health. This paradigm has a number of benefits: All health decisions are fully distributed. Thus, network partitions are gracefully handled (whether the application gracefully handles the partition is a different story). When health checking is configured for an upstream cluster, Envoy uses a 2x2 matrix to determine whether to route to a host: Discovery Status HC OK HC Failed Discovered Route Don’t Route Absent Route Don’t Route / Delete Host discovered / health check OK Envoy will route to the target host. Host absent / health check OK: Envoy will route to the target host. This is very important since the design assumes that the discovery service can fail at any time. If a host continues to pass health check even after becoming absent from the discovery data, Envoy will still route. Although it would be impossible to add new hosts in this scenario, existing hosts will continue to operate normally. When the discovery service is operating normally again the data will eventually re-converge. Host discovered / health check FAIL Envoy will not route to the target host. Health check data is assumed to be more accurate than discovery data. Host absent / health check FAIL Envoy will not route and will delete the target host. This is the only state in which Envoy will purge host data. "},"intro/arch_overview/health_checking.html":{"url":"intro/arch_overview/health_checking.html","title":"健康检查","keywords":"","body":"健康检查 Active health checking can be configured on a per upstream cluster basis. As described in the service discovery section, active health checking and the SDS service discovery type go hand in hand. However, there are other scenarios where active health checking is desired even when using the other service discovery types. Envoy supports three different types of health checking along with various settings (check interval, failures required before marking a host unhealthy, successes required before marking a host healthy, etc.): HTTP: During HTTP health checking Envoy will send an HTTP request to the upstream host. It expects a 200 response if the host is healthy. The upstream host can return 503 if it wants to immediately notify downstream hosts to no longer forward traffic to it. L3/L4: During L3/L4 health checking, Envoy will send a configurable byte buffer to the upstream host. It expects the byte buffer to be echoed in the response if the host is to be considered healthy. Envoy also supports connect only L3/L4 health checking. Redis: Envoy will send a Redis PING command and expect a PONG response. The upstream Redis server can respond with anything other than PONG to cause an immediate active health check failure. Optionally, Envoy can perform EXISTS on a user-specified key. If the key does not exist it is considered a passing healthcheck. This allows the user to mark a Redis instance for maintenance by setting the specified key to any value and waiting for traffic to drain. Seeredis_key. 被动健康检查 Envoy also supports passive health checking via outlier detection. 链接池交互 See here for more information. HTTP 健康检查过滤器 When an Envoy mesh is deployed with active health checking between clusters, a large amount of health checking traffic can be generated. Envoy includes an HTTP health checking filter that can be installed in a configured HTTP listener. This filter is capable of a few different modes of operation: No pass through: In this mode, the health check request is never passed to the local service. Envoy will respond with a 200 or a 503 depending on the current draining state of the server. No pass through, computed from upstream cluster health: In this mode, the health checking filter will return a 200 or a 503 depending on whether at least a specified percentage of the servers are healthy in one or more upstream clusters. (If the Envoy server is in a draining state, though, it will respond with a 503 regardless of the upstream cluster health.) Pass through: In this mode, Envoy will pass every health check request to the local service. The service is expected to return a 200 or a 503 depending on its health state. Pass through with caching: In this mode, Envoy will pass health check requests to the local service, but then cache the result for some period of time. Subsequent health check requests will return the cached value up to the cache time. When the cache time is reached, the next health check request will be passed to the local service. This is the recommended mode of operation when operating a large mesh. Envoy uses persistent connections for health checking traffic and health check requests have very little cost to Envoy itself. Thus, this mode of operation yields an eventually consistent view of the health state of each upstream host without overwhelming the local service with a large number of health check requests. Further reading: Health check filter configuration. /healthcheck/fail admin endpoint. /healthcheck/ok admin endpoint. 主动健康检查快速失败 When using active health checking along with passive health checking (outlier detection), it is common to use a long health checking interval to avoid a large amount of active health checking traffic. In this case, it is still useful to be able to quickly drain an upstream host when using the /healthcheck/fail admin endpoint. To support this, the router filter will respond to the x-envoy-immediate-health-check-fail header. If this header is set by an upstream host, Envoy will immediately mark the host as being failed for active health check. Note that this only occurs if the host’s cluster has active health checking configured. The health checking filter will automatically set this header if Envoy has been marked as failed via the /healthcheck/fail admin endpoint. 健康检查身份 Just verifying that an upstream host responds to a particular health check URL does not necessarily mean that the upstream host is valid. For example, when using eventually consistent service discovery in a cloud auto scaling or container environment, it’s possible for a host to go away and then come back with the same IP address, but as a different host type. One solution to this problem is having a different HTTP health checking URL for every service type. The downside of that approach is that overall configuration becomes more complicated as every health check URL is fully custom. The Envoy HTTP health checker supports the service_name option. If this option is set, the health checker additionally compares the value of the x-envoy-upstream-healthchecked-cluster response header to service_name. If the values do not match, the health check does not pass. The upstream health check filter appends x-envoy-upstream-healthchecked-cluster to the response headers. The appended value is determined by the --service-cluster command line option. "},"intro/arch_overview/connection_pooling.html":{"url":"intro/arch_overview/connection_pooling.html","title":"连接池","keywords":"","body":"连接池 For HTTP traffic, Envoy supports abstract connection pools that are layered on top of the underlying wire protocol (HTTP/1.1 or HTTP/2). The utilizing filter code does not need to be aware of whether the underlying protocol supports true multiplexing or not. In practice the underlying implementations have the following high level properties: HTTP/1.1 The HTTP/1.1 connection pool acquires connections as needed to an upstream host (up to the circuit breaking limit). Requests are bound to connections as they become available, either because a connection is done processing a previous request or because a new connection is ready to receive its first request. The HTTP/1.1 connection pool does not make use of pipelining so that only a single downstream request must be reset if the upstream connection is severed. HTTP/2 The HTTP/2 connection pool acquires a single connection to an upstream host. All requests are multiplexed over this connection. If a GOAWAY frame is received or if the connection reaches the maximum stream limit, the connection pool will create a new connection and drain the existing one. HTTP/2 is the preferred communication protocol as connections rarely if ever get severed. 健康检查交互 If Envoy is configured for either active or passive health checking, all connection pool connections will be closed on behalf of a host that transitions from a healthy state to an unhealthy state. If the host reenters the load balancing rotation it will create fresh connections which will maximize the chance of working around a bad flow (due to ECMP route or something else). "},"intro/arch_overview/load_balancing.html":{"url":"intro/arch_overview/load_balancing.html","title":"负载均衡","keywords":"","body":"负载均衡 When a filter needs to acquire a connection to a host in an upstream cluster, the cluster manager uses a load balancing policy to determine which host is selected. The load balancing policies are pluggable and are specified on a per upstream cluster basis in the configuration. Note that if no active health checking policy is configured for a cluster, all upstream cluster members are considered healthy. 支持的负载均衡器 Round robin This is a simple policy in which each healthy upstream host is selected in round robin order. If weights are assigned to endpoints in a locality, then a weighted round robin schedule is used, where higher weighted endpoints will appear more often in the rotation to achieve the effective weighting. Weighted least request The least request load balancer uses an O(1) algorithm which selects two random healthy hosts and picks the host which has fewer active requests (Research has shown that this approach is nearly as good as an O(N) full scan). If any host in the cluster has a load balancing weight greater than 1, the load balancer shifts into a mode where it randomly picks a host and then uses that host times. This algorithm is simple and sufficient for load testing. It should not be used where true weighted least request behavior is desired (generally if request durations are variable and long in length). We may add a true full scan weighted least request variant in the future to cover this use case. Ring hash The ring/modulo hash load balancer implements consistent hashing to upstream hosts. The algorithm is based on mapping all hosts onto a circle such that the addition or removal of a host from the host set changes only affect 1/N requests. This technique is also commonly known as “ketama” hashing. A consistent hashing load balancer is only effective when protocol routing is used that specifies a value to hash on. The minimum ring size governs the replication factor for each host in the ring. For example, if the minimum ring size is 1024 and there are 16 hosts, each host will be replicated 64 times. The ring hash load balancer does not currently support weighting. When priority based load balancing is in use, the priority level is also chosen by hash, so the endpoint selected will still be consistent when the set of backends is stable. Note The ring hash load balancer does not support locality weighted load balancing. Maglev The Maglev load balancer implements consistent hashing to upstream hosts. It uses the algorithm described in section 3.4 of this paper with a fixed table size of 65537 (see section 5.3 of the same paper). Maglev can be used as a drop in replacement for the ring hash load balancer any place in which consistent hashing is desired. Like the ring hash load balancer, a consistent hashing load balancer is only effective when protocol routing is used that specifies a value to hash on. In general, when compared to the ring hash (“ketama”) algorithm, Maglev has substantially faster table lookup build times as well as host selection times (approximately 10x and 5x respectively when using a large ring size of 256K entries). The downside of Maglev is that it is not as stable as ring hash. More keys will move position when hosts are removed (simulations show approximately double the keys will move). With that said, for many applications including Redis, Maglev is very likely a superior drop in replacement for ring hash. The advanced reader can use this benchmark to compare ring hash versus Maglev with different parameters. Random The random load balancer selects a random healthy host. The random load balancer generally performs better than round robin if no health checking policy is configured. Random selection avoids bias towards the host in the set that comes after a failed host. Original destination This is a special purpose load balancer that can only be used with an original destination cluster. Upstream host is selected based on the downstream connection metadata, i.e., connections are opened to the same address as the destination address of the incoming connection was before the connection was redirected to Envoy. New destinations are added to the cluster by the load balancer on-demand, and the cluster periodically cleans out unused hosts from the cluster. No other load balancing type can be used with original destination clusters. 恐慌阈值 During load balancing, Envoy will generally only consider healthy hosts in an upstream cluster. However, if the percentage of healthy hosts in the cluster becomes too low, Envoy will disregard health status and balance amongst all hosts. This is known as the panic threshold. The default panic threshold is 50%. This is configurable via runtime as well as in the cluster configuration. The panic threshold is used to avoid a situation in which host failures cascade throughout the cluster as load increases. Note that panic thresholds are per-priority. This means that if the percentage of healthy nodes in a single priority goes below the threshold, that priority will enter panic mode. In general it is discouraged to use panic thresholds in conjunction with priorities, as by the time enough nodes are unhealthy to trigger the panic threshold most of the traffic should already have spilled over to the next priority level. 优先级划分 During load balancing, Envoy will generally only consider hosts configured at the highest priority level. For each EDS LocalityLbEndpoints an optional priority may also be specified. When endpoints at the highest priority level (P=0) are healthy, all traffic will land on endpoints in that priority level. As endpoints for the highest priority level become unhealthy, traffic will begin to trickle to lower priority levels. Currently, it is assumed that each priority level is over-provisioned by a (hard-coded) factor of 1.4. So if 80% of the endpoints are healthy, the priority level is still considered healthy because 80*1.4 > 100. As the number of healthy endpoints dips below 72%, the health of the priority level goes below 100. At that point the percent of traffic equivalent to the health of P=0 will go to P=0 and remaining traffic will flow to P=1. Assume a simple set-up with 2 priority levels, P=1 100% healthy. P=0 healthy endpoints Percent of traffic to P=0 Percent of traffic to P=1 100% 100% 0% 72% 100% 0% 71% 99% 1% 50% 70% 30% 25% 35% 65% 0% 0% 100% If P=1 becomes unhealthy, it will continue to take spilled load from P=0 until the sum of the health P=0 + P=1 goes below 100. At this point the healths will be scaled up to an “effective” health of 100%. P=0 healthy endpoints P=1 healthy endpoints Traffic to P=0 Traffic to P=1 100% 100% 100% 0% 72% 72% 100% 0% 71% 71% 99% 1% 50% 50% 70% 30% 25% 100% 35% 65% 25% 25% 50% 50% As more priorities are added, each level consumes load equal to its “scaled” effective health, so P=2 would only receive traffic if the combined health of P=0 + P=1 was less than 100. P=0 healthy endpoints P=1 healthy endpoints P=2 healthy endpoints Traffic to P=0 Traffic to P=1 Traffic to P=2 100% 100% 100% 100% 0% 0% 72% 72% 100% 100% 0% 0% 71% 71% 100% 99% 1% 0% 50% 50% 100% 70% 30% 0% 25% 100% 100% 35% 65% 0% 25% 25% 100% 25% 25% 50% To sum this up in pseudo algorithms: load to P_0 = min(100, health(P_0) * 100 / total_health) health(P_X) = 140 * healthy_P_X_backends / total_P_X_backends total_health = min(100, Σ(health(P_0)...health(P_X)) load to P_X = 100 - Σ(percent_load(P_0)..percent_load(P_X-1)) Zone 感知路由 We use the following terminology: Originating/Upstream cluster: Envoy routes requests from an originating cluster to an upstream cluster. Local zone: The same zone that contains a subset of hosts in both the originating and upstream clusters. Zone aware routing: Best effort routing of requests to an upstream cluster host in the local zone. In deployments where hosts in originating and upstream clusters belong to different zones Envoy performs zone aware routing. There are several preconditions before zone aware routing can be performed: Both originating and upstream cluster are not in panic mode. Zone aware routing is enabled. The originating cluster has the same number of zones as the upstream cluster. The upstream cluster has enough hosts. See here for more information. The purpose of zone aware routing is to send as much traffic to the local zone in the upstream cluster as possible while roughly maintaining the same number of requests per second across all upstream hosts (depending on load balancing policy). Envoy tries to push as much traffic as possible to the local upstream zone as long as roughly the same number of requests per host in the upstream cluster are maintained. The decision of whether Envoy routes to the local zone or performs cross zone routing depends on the percentage of healthy hosts in the originating cluster and upstream cluster in the local zone. There are two cases with regard to percentage relations in the local zone between originating and upstream clusters: The originating cluster local zone percentage is greater than the one in the upstream cluster. In this case we cannot route all requests from the local zone of the originating cluster to the local zone of the upstream cluster because that will lead to request imbalance across all upstream hosts. Instead, Envoy calculates the percentage of requests that can be routed directly to the local zone of the upstream cluster. The rest of the requests are routed cross zone. The specific zone is selected based on the residual capacity of the zone (that zone will get some local zone traffic and may have additional capacity Envoy can use for cross zone traffic). The originating cluster local zone percentage is smaller than the one in upstream cluster. In this case the local zone of the upstream cluster can get all of the requests from the local zone of the originating cluster and also have some space to allow traffic from other zones in the originating cluster (if needed). Note that when using multiple priorities, zone aware routing is currently only supported for P=0. 所在地权重负载均衡 Another approach to determining how to weight assignments across different zones and geographical locations is by using explicit weights supplied via EDS in the LocalityLbEndpointsmessage. This approach is mutually exclusive with the above zone aware routing, since in the case of locality aware LB, we rely on the management server to provide the locality weighting, rather than the Envoy-side heuristics used in zone aware routing. When all endpoints are healthy, the locality is picked using a weighted round-robin schedule, where the locality weight is used for weighting. When some endpoints in a locality are unhealthy, we adjust the locality weight to reflect this. As with priority levels, we assume an over-provision factor (currently hardcoded at 1.4), which means we do not perform any weight adjustment when only a small number of endpoints in a locality are unhealthy. Assume a simple set-up with 2 localities X and Y, where X has a locality weight of 1 and Y has a locality weight of 2, L=Y 100% healthy. L=X healthy endpoints Percent of traffic to L=X Percent of traffic to L=Y 100% 33% 67% 70% 33% 67% 69% 32% 68% 50% 26% 74% 25% 15% 85% 0% 0% 100% To sum this up in pseudo algorithms: health(L_X) = 140 * healthy_X_backends / total_X_backends effective_weight(L_X) = locality_weight_X * min(100, health(L_X)) load to L_X = effective_weight(L_X) / Σ_c(effective_weight(L_c)) Note that the locality weighted pick takes place after the priority level is picked. The load balancer follows these steps: Pick priority level. Pick locality (as described in this section) within priority level from (1). Pick endpoint using cluster specified load balancer within locality from (2). Locality weighted load balancing is configured by setting locality_weighted_lb_config in the cluster configuration and providing weights in LocalityLbEndpoints via load_balancing_weight. This feature is not compatible with load balancer subsetting, since it is not straightforward to reconcile locality level weighting with sensible weights for individual subsets. Load Balancer Subsets Envoy may be configured to divide hosts within an upstream cluster into subsets based on metadata attached to the hosts. Routes may then specify the metadata that a host must match in order to be selected by the load balancer, with the option of falling back to a predefined set of hosts, including any host. Subsets use the load balancer policy specified by the cluster. The original destination policy may not be used with subsets because the upstream hosts are not known in advance. Subsets are compatible with zone aware routing, but be aware that the use of subsets may easily violate the minimum hosts condition described above. If subsets are configured and a route specifies no metadata or no subset matching the metadata exists, the subset load balancer initiates its fallback policy. The default policy is NO_ENDPOINT, in which case the request fails as if the cluster had no hosts. Conversely, the ANY_ENDPOINT fallback policy load balances across all hosts in the cluster, without regard to host metadata. Finally, the DEFAULT_SUBSET causes fallback to load balance among hosts that match a specific set of metadata. Subsets must be predefined to allow the subset load balancer to efficiently select the correct subset of hosts. Each definition is a set of keys, which translates to zero or more subsets. Conceptually, each host that has a metadata value for all of the keys in a definition is added to a subset specific to its key-value pairs. If no host has all the keys, no subsets result from the definition. Multiple definitions may be provided, and a single host may appear in multiple subsets if it matches multiple definitions. During routing, the route’s metadata match configuration is used to find a specific subset. If there is a subset with the exact keys and values specified by the route, the subset is used for load balancing. Otherwise, the fallback policy is used. The cluster’s subset configuration must, therefore, contain a definition that has the same keys as a given route in order for subset load balancing to occur. This feature can only be enabled using the V2 configuration API. Furthermore, host metadata is only supported when using the EDS discovery type for clusters. Host metadata for subset load balancing must be placed under the filter name \"envoy.lb\". Similarly, route metadata match criteria use the \"envoy.lb\" filter name. Host metadata may be hierarchical (e.g., the value for a top-level key may be a structured value or list), but the subset load balancer only compares top-level keys and values. Therefore when using structured values, a route’s match criteria will only match if an identical structured value appears in the host’s metadata. Examples We’ll use simple metadata where all values are strings. Assume the following hosts are defined and associated with a cluster: Host Metadata host1 v: 1.0, stage: prod host2 v: 1.0, stage: prod host3 v: 1.1, stage: canary host4 v: 1.2-pre, stage: dev The cluster may enable subset load balancing like this: --- name: cluster-name type: EDS eds_cluster_config: eds_config: path: '.../eds.conf' connect_timeout: seconds: 10 lb_policy: LEAST_REQUEST lb_subset_config: fallback_policy: DEFAULT_SUBSET default_subset: stage: prod subset_selectors: - keys: - v - stage - keys: - stage The following table describes some routes and the result of their application to the cluster. Typically the match criteria would be used with routes matching specific aspects of the request, such as the path or header information. Match Criteria Balances Over Reason stage: canary host3 Subset of hosts selected v: 1.2-pre, stage: dev host4 Subset of hosts selected v: 1.0 host1, host2 Fallback: No subset selector for “v” alone other: x host1, host2 Fallback: No subset selector for “other” (none) host1, host2 Fallback: No subset requested Metadata match criteria may also be specified on a route’s weighted clusters. Metadata match criteria from the selected weighted cluster are merged with and override the criteria from the route: Route Match Criteria Weighted Cluster Match Criteria Final Match Criteria stage: canary stage: prod stage: prod v: 1.0 stage: prod v: 1.0, stage: prod v: 1.0, stage: prod stage: canary v: 1.0, stage: canary v: 1.0, stage: prod v: 1.1, stage: canary v: 1.1, stage: canary (none) v: 1.0 v: 1.0 v: 1.0 (none) v: 1.0 Example Host With Metadata An EDS LbEndpoint with host metadata: --- endpoint: address: socket_address: protocol: TCP address: 127.0.0.1 port_value: 8888 metadata: filter_metadata: envoy.lb: version: '1.0' stage: 'prod' Example Route With Metadata Criteria An RDS Route with metadata match criteria: --- match: prefix: / route: cluster: cluster-name metadata_match: filter_metadata: envoy.lb: version: '1.0' stage: 'prod' "},"intro/arch_overview/outlier.html":{"url":"intro/arch_overview/outlier.html","title":"异常点检测","keywords":"","body":"异常点检测 Outlier detection and ejection is the process of dynamically determining whether some number of hosts in an upstream cluster are performing unlike the others and removing them from the healthyload balancing set. Performance might be along different axes such as consecutive failures, temporal success rate, temporal latency, etc. Outlier detection is a form of passive health checking. Envoy also supports active health checking. Passive and active health checking can be enabled together or independently, and form the basis for an overall upstream health checking solution. 逐出算法 Depending on the type of outlier detection, ejection either runs inline (for example in the case of consecutive 5xx) or at a specified interval (for example in the case of periodic success rate). The ejection algorithm works as follows: A host is determined to be an outlier. If no hosts have been ejected, Envoy will eject the host immediately. Otherwise, it checks to make sure the number of ejected hosts is below the allowed threshold (specified via theoutlier_detection.max_ejection_percent setting). If the number of ejected hosts is above the threshold, the host is not ejected. The host is ejected for some number of milliseconds. Ejection means that the host is marked unhealthy and will not be used during load balancing unless the load balancer is in a panicscenario. The number of milliseconds is equal to the outlier_detection.base_ejection_time_msvalue multiplied by the number of times the host has been ejected. This causes hosts to get ejected for longer and longer periods if they continue to fail. An ejected host will automatically be brought back into service after the ejection time has been satisfied. Generally, outlier detection is used alongside active health checking for a comprehensive health checking solution. 检测类型 Envoy supports the following outlier detection types: Consecutive 5xx If an upstream host returns some number of consecutive 5xx, it will be ejected. Note that in this case a 5xx means an actual 5xx respond code, or an event that would cause the HTTP router to return one on the upstream’s behalf (reset, connection failure, etc.). The number of consecutive 5xx required for ejection is controlled by the outlier_detection.consecutive_5xx value. Consecutive Gateway Failure If an upstream host returns some number of consecutive “gateway errors” (502, 503 or 504 status code), it will be ejected. Note that this includes events that would cause the HTTP router to return one of these status codes on the upstream’s behalf (reset, connection failure, etc.). The number of consecutive gateway failures required for ejection is controlled by the outlier_detection.consecutive_gateway_failure value. Success Rate Success Rate based outlier ejection aggregates success rate data from every host in a cluster. Then at given intervals ejects hosts based on statistical outlier detection. Success Rate outlier ejection will not be calculated for a host if its request volume over the aggregation interval is less than theoutlier_detection.success_rate_request_volume value. Moreover, detection will not be performed for a cluster if the number of hosts with the minimum required request volume in an interval is less than the outlier_detection.success_rate_minimum_hosts value. 逐出事件记录 A log of outlier ejection events can optionally be produced by Envoy. This is extremely useful during daily operations since global stats do not provide enough information on which hosts are being ejected and for what reasons. The log uses a JSON format with one object per line: { \"time\": \"...\", \"secs_since_last_action\": \"...\", \"cluster\": \"...\", \"upstream_url\": \"...\", \"action\": \"...\", \"type\": \"...\", \"num_ejections\": \"...\", \"enforced\": \"...\", \"host_success_rate\": \"...\", \"cluster_success_rate_average\": \"...\", \"cluster_success_rate_ejection_threshold\": \"...\" } time The time that the event took place. secs_since_last_action The time in seconds since the last action (either an ejection or unejection) took place. This value will be -1 for the first ejection given there is no action before the first ejection. cluster The cluster that owns the ejected host. upstream_url The URL of the ejected host. E.g., tcp://1.2.3.4:80. action The action that took place. Either eject if a host was ejected or uneject if it was brought back into service. type If action is eject, specifies the type of ejection that took place. Currently type can be one of 5xx, GatewayFailure or SuccessRate. num_ejections If action is eject, specifies the number of times the host has been ejected (local to that Envoy and gets reset if the host gets removed from the upstream cluster for any reason and then re-added). enforced If action is eject, specifies if the ejection was enforced. true means the host was ejected.false means the event was logged but the host was not actually ejected. host_success_rate If action is eject, and type is SuccessRate, specifies the host’s success rate at the time of the ejection event on a 0-100 range. cluster_success_rate_average If action is eject, and type is SuccessRate, specifies the average success rate of the hosts in the cluster at the time of the ejection event on a 0-100 range. cluster_success_rate_ejection_threshold If action is eject, and type is SuccessRate, specifies success rate ejection threshold at the time of the ejection event. 参考配置 Cluster manager global configuration Per cluster configuration Runtime settings Statistics reference "},"intro/arch_overview/circuit_breaking.html":{"url":"intro/arch_overview/circuit_breaking.html","title":"断路器","keywords":"","body":"断路器 Circuit breaking is a critical component of distributed systems. It’s nearly always better to fail quickly and apply back pressure downstream as soon as possible. One of the main benefits of an Envoy mesh is that Envoy enforces circuit breaking limits at the network level as opposed to having to configure and code each application independently. Envoy supports various types of fully distributed (not coordinated) circuit breaking: Cluster maximum connections: The maximum number of connections that Envoy will establish to all hosts in an upstream cluster. In practice this is only applicable to HTTP/1.1 clusters since HTTP/2 uses a single connection to each host. Cluster maximum pending requests: The maximum number of requests that will be queued while waiting for a ready connection pool connection. In practice this is only applicable to HTTP/1.1 clusters since HTTP/2 connection pools never queue requests. HTTP/2 requests are multiplexed immediately. If this circuit breaker overflows the upstream_rq_pending_overflowcounter for the cluster will increment. Cluster maximum requests: The maximum number of requests that can be outstanding to all hosts in a cluster at any given time. In practice this is applicable to HTTP/2 clusters since HTTP/1.1 clusters are governed by the maximum connections circuit breaker. If this circuit breaker overflows the upstream_rq_pending_overflow counter for the cluster will increment. Cluster maximum active retries: The maximum number of retries that can be outstanding to all hosts in a cluster at any given time. In general we recommend aggressively circuit breaking retries so that retries for sporadic failures are allowed but the overall retry volume cannot explode and cause large scale cascading failure. If this circuit breaker overflows theupstream_rq_retry_overflow counter for the cluster will increment. Each circuit breaking limit is configurable and tracked on a per upstream cluster and per priority basis. This allows different components of the distributed system to be tuned independently and have different limits. Note that circuit breaking will cause the x-envoy-overloaded header to be set by the router filter in the case of HTTP requests. "},"intro/arch_overview/global_rate_limiting.html":{"url":"intro/arch_overview/global_rate_limiting.html","title":"全局速率限制","keywords":"","body":"全局速率限制 Although distributed circuit breaking is generally extremely effective in controlling throughput in distributed systems, there are times when it is not very effective and global rate limiting is desired. The most common case is when a large number of hosts are forwarding to a small number of hosts and the average request latency is low (e.g., connections/requests to a database server). If the target hosts become backed up, the downstream hosts will overwhelm the upstream cluster. In this scenario it is extremely difficult to configure a tight enough circuit breaking limit on each downstream host such that the system will operate normally during typical request patterns but still prevent cascading failure when the system starts to fail. Global rate limiting is a good solution for this case. Envoy integrates directly with a global gRPC rate limiting service. Although any service that implements the defined RPC/IDL protocol can be used, Lyft provides a reference implementationwritten in Go which uses a Redis backend. Envoy’s rate limit integration has the following features: Network level rate limit filter: Envoy will call the rate limit service for every new connection on the listener where the filter is installed. The configuration specifies a specific domain and descriptor set to rate limit on. This has the ultimate effect of rate limiting the connections per second that transit the listener. Configuration reference. HTTP level rate limit filter: Envoy will call the rate limit service for every new request on the listener where the filter is installed and where the route table specifies that the global rate limit service should be called. All requests to the target upstream cluster as well as all requests from the originating cluster to the target cluster can be rate limited. Configuration reference Rate limit service configuration. "},"intro/arch_overview/ssl.html":{"url":"intro/arch_overview/ssl.html","title":"TLS","keywords":"","body":"TLS Envoy supports both TLS termination in listeners as well as TLS origination when making connections to upstream clusters. Support is sufficient for Envoy to perform standard edge proxy duties for modern web services as well as to initiate connections with external services that have advanced TLS requirements (TLS1.2, SNI, etc.). Envoy supports the following TLS features: Configurable ciphers: Each TLS listener and client can specify the ciphers that it supports. Client certificates: Upstream/client connections can present a client certificate in addition to server certificate verification. Certificate verification and pinning: Certificate verification options include basic chain verification, subject name verification, and hash pinning. Certificate revocation: Envoy can check peer certificates against a certificate revocation list (CRL) if one is provided. ALPN: TLS listeners support ALPN. The HTTP connection manager uses this information (in addition to protocol inference) to determine whether a client is speaking HTTP/1.1 or HTTP/2. SNI: SNI is supported for both server (listener) and client (upstream) connections. Session resumption: Server connections support resuming previous sessions via TLS session tickets (see RFC 5077). Resumption can be performed across hot restarts and between parallel Envoy instances (typically useful in a front proxy configuration). 底层实现 Currently Envoy is written to use BoringSSL as the TLS provider. 启用认证验证 Certificate verification of both upstream and downstream connections is not enabled unless the validation context specifies one or more trusted authority certificates. Example configuration static_resources: listeners: - name: listener_0 address: { socket_address: { address: 127.0.0.1, port_value: 10000 } } filter_chains: - filters: - name: envoy.http_connection_manager # ... tls_context: common_tls_context: validation_context: trusted_ca: filename: /usr/local/my-client-ca.crt clusters: - name: some_service connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: 127.0.0.2, port_value: 1234 }}] tls_context: common_tls_context: validation_context: trusted_ca: filename: /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt is the default path for the system CA bundle on Debian systems. This makes Envoy verify the server identity of 127.0.0.2:1234 in the same way as e.g. cURL does on standard Debian installations. Common paths for system CA bundles on Linux and BSD are /etc/ssl/certs/ca-certificates.crt (Debian/Ubuntu/Gentoo etc.) /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem (CentOS/RHEL 7) /etc/pki/tls/certs/ca-bundle.crt (Fedora/RHEL 6) /etc/ssl/ca-bundle.pem (OpenSUSE) /usr/local/etc/ssl/cert.pem (FreeBSD) /etc/ssl/cert.pem (OpenBSD) See the reference for UpstreamTlsContexts and DownstreamTlsContexts for other TLS options. 身份验证过滤器 Envoy provides a network filter that performs TLS client authentication via principals fetched from a REST VPN service. This filter matches the presented client certificate hash against the principal list to determine whether the connection should be allowed or not. Optional IP white listing can also be configured. This functionality can be used to build edge proxy VPN support for web infrastructure. Client TLS authentication filter configuration reference. "},"intro/arch_overview/statistics.html":{"url":"intro/arch_overview/statistics.html","title":"统计","keywords":"","body":"统计 One of the primary goals of Envoy is to make the network understandable. Envoy emits a large number of statistics depending on how it is configured. Generally the statistics fall into three categories: Downstream: Downstream statistics relate to incoming connections/requests. They are emitted by listeners, the HTTP connection manager, the TCP proxy filter, etc. Upstream: Upstream statistics relate to outgoing connections/requests. They are emitted by connection pools, the router filter, the TCP proxy filter, etc. Server: Server statistics describe how the Envoy server instance is working. Statistics like server uptime or amount of allocated memory are categorized here. A single proxy scenario typically involves both downstream and upstream statistics. The two types can be used to get a detailed picture of that particular network hop. Statistics from the entire mesh give a very detailed picture of each hop and overall network health. The statistics emitted are documented in detail in the operations guide. In the v1 API, Envoy only supports statsd as the statistics output format. Both TCP and UDP statsd are supported. As of the v2 API, Envoy has the ability to support custom, pluggable sinks. A few standard sink implementations are included in Envoy. Some sinks also support emitting statistics with tags/dimensions. Within Envoy and throughout the documentation, statistics are identified by a canonical string representation. The dynamic portions of these strings are stripped to become tags. Users can configure this behavior via the Tag Specifier configuration. Envoy emits three types of values as statistics: Counters: Unsigned integers that only increase and never decrease. E.g., total requests. Gauges: Unsigned integers that both increase and decrease. E.g., currently active requests. Histograms: Unsigned integers that are part of a stream of values that are then aggregated by the collector to ultimately yield summarized percentile values. E.g., upstream request time. Internally, counters and gauges are batched and periodically flushed to improve performance. Histograms are written as they are received. Note: what were previously referred to as timers have become histograms as the only difference between the two representations was the units. v1 API reference. v2 API reference. "},"intro/arch_overview/runtime.html":{"url":"intro/arch_overview/runtime.html","title":"运行时配置","keywords":"","body":"运行时配置 Envoy supports “runtime” configuration (also known as “feature flags” and “decider”). Configuration settings can be altered that will affect operation without needing to restart Envoy or change the primary configuration. The currently supported implementation uses a tree of file system files. Envoy watches for a symbolic link swap in a configured directory and reloads the tree when that happens. This type of system is very commonly deployed in large distributed systems. Other implementations would not be difficult to implement. Supported runtime configuration settings are documented in the relevant sections of the operations guide. Envoy will operate correctly with default runtime values and a “null” provider so it is not required that such a system exists to run Envoy. Runtime configuration. "},"intro/arch_overview/tracing.html":{"url":"intro/arch_overview/tracing.html","title":"追踪","keywords":"","body":"追踪 概览 Distributed tracing allows developers to obtain visualizations of call flows in large service oriented architectures. It can be invaluable in understanding serialization, parallelism, and sources of latency. Envoy supports three features related to system wide tracing: Request ID generation: Envoy will generate UUIDs when needed and populate the x-request-idHTTP header. Applications can forward the x-request-id header for unified logging as well as tracing. External trace service integration: Envoy supports pluggable external trace visualization providers. Currently Envoy supports LightStep, Zipkin or any Zipkin compatible backends (e.g. Jaeger). However, support for other tracing providers would not be difficult to add. Client trace ID joining: The x-client-trace-id header can be used to join untrusted request IDs to the trusted internal x-request-id. 如何初始化追踪 The HTTP connection manager that handles the request must have the tracing object set. There are several ways tracing can be initiated: By an external client via the x-client-trace-id header. By an internal service via the x-envoy-force-trace header. Randomly sampled via the random_sampling runtime setting. The router filter is also capable of creating a child span for egress calls via the start_child_spanoption. 追踪上下文传播 Envoy provides the capability for reporting tracing information regarding communications between services in the mesh. However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests. Whichever tracing provider is being used, the service should propagate the x-request-id to enable logging across the invoked services to be correlated. The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood. This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests. This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace. Alternatively the trace context can be manually propagated by the service: When using the LightStep tracer, Envoy relies on the service to propagate the x-ot-span-contextHTTP header while sending HTTP requests to other services. When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, and x-b3-flags). The x-b3-sampledheader can also be supplied by an external client to either enable or disable tracing for a particular request. 每条追踪中包含哪些数据 An end-to-end trace is comprised of one or more spans. A span represents a logical unit of work that has a start time and duration and can contain metadata associated with it. Each span generated by Envoy contains the following data: Originating service cluster set via --service-cluster. Start time and duration of the request. Originating host set via --service-node. Downstream cluster set via the x-envoy-downstream-service-cluster header. HTTP URL. HTTP method. HTTP response code. Tracing system-specific metadata. The span also includes a name (or operation) which by default is defined as the host of the invoked service. However this can be customized using a Decorator on the route. The name can also be overridden using the x-envoy-decorator-operation header. Envoy automatically sends spans to tracing collectors. Depending on the tracing collector, multiple spans are stitched together using common information such as the globally unique request ID x-request-id (LightStep) or the trace ID configuration (Zipkin). See v1 API reference v2 API reference for more information on how to setup tracing in Envoy. "},"intro/arch_overview/tcp_proxy.html":{"url":"intro/arch_overview/tcp_proxy.html","title":"TCP 代理","keywords":"","body":"TCP 代理 Since Envoy is fundamentally written as a L3/L4 server, basic L3/L4 proxy is easily implemented. The TCP proxy filter performs basic 1:1 network connection proxy between downstream clients and upstream clusters. It can be used by itself as an stunnel replacement, or in conjunction with other filters such as the MongoDB filter or the rate limit filter. The TCP proxy filter will respect the connection limits imposed by each upstream cluster’s global resource manager. The TCP proxy filter checks with the upstream cluster’s resource manager if it can create a connection without going over that cluster’s maximum number of connections, if it can’t the TCP proxy will not make the connection. TCP proxy filter configuration reference. "},"intro/arch_overview/access_logging.html":{"url":"intro/arch_overview/access_logging.html","title":"访问记录","keywords":"","body":"访问记录 The HTTP connection manager and tcp proxy supports extensible access logging with the following features: Any number of access logs per connection manager or tcp proxy. Asynchronous IO flushing architecture. Access logging will never block the main network processing threads. Customizable access log formats using predefined fields as well as arbitrary HTTP request and response headers. Customizable access log filters that allow different types of requests and responses to be written to different access logs. Access log configuration. "},"intro/arch_overview/mongo.html":{"url":"intro/arch_overview/mongo.html","title":"MongoDB","keywords":"","body":"MongoDB Envoy supports a network level MongoDB sniffing filter with the following features: MongoDB wire format BSON parser. Detailed MongoDB query/operation statistics including timings and scatter/multi-get counts for routed clusters. Query logging. Per callsite statistics via the $comment query parameter. Fault injection. The MongoDB filter is a good example of Envoy’s extensibility and core abstractions. At Lyft we use this filter between all applications and our databases. It provides an invaluable source of data that is agnostic to the application platform and specific MongoDB driver in use. MongoDB proxy filter configuration reference. "},"intro/arch_overview/dynamo.html":{"url":"intro/arch_overview/dynamo.html","title":"DynamoDB","keywords":"","body":"DynamoDB Envoy supports an HTTP level DynamoDB sniffing filter with the following features: DynamoDB API request/response parser. DynamoDB per operation/per table/per partition and operation statistics. Failure type statistics for 4xx responses, parsed from response JSON, e.g., ProvisionedThroughputExceededException. Batch operation partial failure statistics. The DynamoDB filter is a good example of Envoy’s extensibility and core abstractions at the HTTP layer. At Lyft we use this filter for all application communication with DynamoDB. It provides an invaluable source of data agnostic to the application platform and specific AWS SDK in use. DynamoDB filter configuration. "},"intro/arch_overview/redis.html":{"url":"intro/arch_overview/redis.html","title":"Redis","keywords":"","body":"Redis Envoy can act as a Redis proxy, partitioning commands among instances in a cluster. In this mode, the goals of Envoy are to maintain availability and partition tolerance over consistency. This is the key point when comparing Envoy to Redis Cluster. Envoy is designed as a best-effort cache, meaning that it will not try to reconcile inconsistent data or keep a globally consistent view of cluster membership. The Redis project offers a thorough reference on partitioning as it relates to Redis. See “Partitioning: how to split data among multiple Redis instances”. Features of Envoy Redis: Redis protocol codec. Hash-based partitioning. Ketama distribution. Detailed command statistics. Active and passive healthchecking. Planned future enhancements: Additional timing stats. Circuit breaking. Request collapsing for fragmented commands. Replication. Built-in retry. Tracing. Hash tagging. 配置 For filter configuration details, see the Redis proxy filter configuration reference. The corresponding cluster definition should be configured with ring hash load balancing. If active healthchecking is desired, the cluster should be configured with a Redis healthcheck. If passive healthchecking is desired, also configure outlier detection. For the purposes of passive healthchecking, connect timeouts, command timeouts, and connection close map to 5xx. All other responses from Redis are counted as a success. 支持的命令 At the protocol level, pipelines are supported. MULTI (transaction block) is not. Use pipelining wherever possible for the best performance. At the command level, Envoy only supports commands that can be reliably hashed to a server. PING is the only exception, which Envoy responds to immediately with PONG. Arguments to PING are not allowed. All other supported commands must contain a key. Supported commands are functionally identical to the original Redis command except possibly in failure scenarios. For details on each command’s usage see the official Redis command reference. Command Group PING Connection DEL Generic DUMP Generic EXISTS Generic EXPIRE Generic EXPIREAT Generic PERSIST Generic PEXPIRE Generic PEXPIREAT Generic PTTL Generic RESTORE Generic TOUCH Generic TTL Generic TYPE Generic UNLINK Generic GEOADD Geo GEODIST Geo GEOHASH Geo GEOPOS Geo GEORADIUS_RO Geo GEORADIUSBYMEMBER_RO Geo HDEL Hash HEXISTS Hash HGET Hash HGETALL Hash HINCRBY Hash HINCRBYFLOAT Hash HKEYS Hash HLEN Hash HMGET Hash HMSET Hash HSCAN Hash HSET Hash HSETNX Hash HSTRLEN Hash HVALS Hash LINDEX List LINSERT List LLEN List LPOP List LPUSH List LPUSHX List LRANGE List LREM List LSET List LTRIM List RPOP List RPUSH List RPUSHX List EVAL Scripting EVALSHA Scripting SADD Set SCARD Set SISMEMBER Set SMEMBERS Set SPOP Set SRANDMEMBER Set SREM Set SSCAN Set ZADD Sorted Set ZCARD Sorted Set ZCOUNT Sorted Set ZINCRBY Sorted Set ZLEXCOUNT Sorted Set ZRANGE Sorted Set ZRANGEBYLEX Sorted Set ZRANGEBYSCORE Sorted Set ZRANK Sorted Set ZREM Sorted Set ZREMRANGEBYLEX Sorted Set ZREMRANGEBYRANK Sorted Set ZREMRANGEBYSCORE Sorted Set ZREVRANGE Sorted Set ZREVRANGEBYLEX Sorted Set ZREVRANGEBYSCORE Sorted Set ZREVRANK Sorted Set ZSCAN Sorted Set ZSCORE Sorted Set APPEND String BITCOUNT String BITFIELD String BITPOS String DECR String DECRBY String GET String GETBIT String GETRANGE String GETSET String INCR String INCRBY String INCRBYFLOAT String MGET String MSET String PSETEX String SET String SETBIT String SETEX String SETNX String SETRANGE String STRLEN String 失败模式 If Redis throws an error, we pass that error along as the response to the command. Envoy treats a response from Redis with the error datatype as a normal response and passes it through to the caller. Envoy can also generate its own errors in response to the client. Error Meaning no upstream host The ring hash load balancer did not have a healthy host available at the ring position chosen for the key. upstream failure The backend did not respond within the timeout period or closed the connection. invalid request Command was rejected by the first stage of the command splitter due to datatype or length. unsupported command The command was not recognized by Envoy and therefore cannot be serviced because it cannot be hashed to a backend server. finished with n errors Fragmented commands which sum the response (e.g. DEL) will return the total number of errors received if any were received. upstream protocol error A fragmented command received an unexpected datatype or a backend responded with a response that not conform to the Redis protocol. wrong number of arguments for command Certain commands check in Envoy that the number of arguments is correct. In the case of MGET, each individual key that cannot be fetched will generate an error response. For example, if we fetch five keys and two of the keys’ backends time out, we would get an error response for each in place of the value. $ redis-cli MGET a b c d e 1) \"alpha\" 2) \"bravo\" 3) (error) upstream failure 4) (error) upstream failure 5) \"echo\" "},"intro/arch_overview/hot_restart.html":{"url":"intro/arch_overview/hot_restart.html","title":"热重启","keywords":"","body":"热重启 Ease of operation is one of the primary goals of Envoy. In addition to robust statistics and a local administration interface, Envoy has the ability to “hot” or “live” restart itself. This means that Envoy can fully reload itself (both code and configuration) without dropping any connections. The hot restart functionality has the following general architecture: Statistics and some locks are kept in a shared memory region. This means that gauges will be consistent across both processes as restart is taking place. The two active processes communicate with each other over unix domain sockets using a basic RPC protocol. The new process fully initializes itself (loads the configuration, does an initial service discovery and health checking phase, etc.) before it asks for copies of the listen sockets from the old process. The new process starts listening and then tells the old process to start draining. During the draining phase, the old process attempts to gracefully close existing connections. How this is done depends on the configured filters. The drain time is configurable via the--drain-time-s option and as more time passes draining becomes more aggressive. After drain sequence, the new Envoy process tells the old Envoy process to shut itself down. This time is configurable via the --parent-shutdown-time-s option. Envoy’s hot restart support was designed so that it will work correctly even if the new Envoy process and the old Envoy process are running inside different containers. Communication between the processes takes place only using unix domain sockets. An example restarter/parent process written in Python is included in the source distribution. This parent process is usable with standard process control utilities such as monit/runit/etc. "},"intro/arch_overview/dynamic_configuration.html":{"url":"intro/arch_overview/dynamic_configuration.html","title":"动态配置","keywords":"","body":"动态配置 Envoy is architected such that different types of configuration management approaches are possible. The approach taken in a deployment will be dependent on the needs of the implementor. Simple deployments are possible with a fully static configuration. More complicated deployments can incrementally add more complex dynamic configuration, the downside being that the implementor must provide one or more external REST based configuration provider APIs. This document gives an overview of the options currently available. Top level configuration reference. Reference configurations. Envoy v2 API overview. 全静态 In a fully static configuration, the implementor provides a set of listeners (and filter chains), clusters, and optionally HTTP route configurations. Dynamic host discovery is only possible via DNS basedservice discovery. Configuration reloads must take place via the built in hot restart mechanism. Though simplistic, fairly complicated deployments can be created using static configurations and graceful hot restarts. 仅 SDS/EDS The service discovery service (SDS) API provides a more advanced mechanism by which Envoy can discover members of an upstream cluster. SDS has been renamed to Endpoint Discovery Service (EDS) in the v2 API. Layered on top of a static configuration, SDS allows an Envoy deployment to circumvent the limitations of DNS (maximum records in a response, etc.) as well as consume more information used in load balancing and routing (e.g., canary status, zone, etc.). SDS/EDS 和 CDS The cluster discovery service (CDS) API layers on a mechanism by which Envoy can discover upstream clusters used during routing. Envoy will gracefully add, update, and remove clusters as specified by the API. This API allows implementors to build a topology in which Envoy does not need to be aware of all upstream clusters at initial configuration time. Typically, when doing HTTP routing along with CDS (but without route discovery service), implementors will make use of the router’s ability to forward requests to a cluster specified in an HTTP request header. Although it is possible to use CDS without SDS/EDS by specifying fully static clusters, we recommend still using the SDS/EDS API for clusters specified via CDS. Internally, when a cluster definition is updated, the operation is graceful. However, all existing connection pools will be drained and reconnected. SDS/EDS does not suffer from this limitation. When hosts are added and removed via SDS/EDS, the existing hosts in the cluster are unaffected. SDS/EDS、CDS 和 RDS The route discovery service (RDS) API layers on a mechanism by which Envoy can discover the entire route configuration for an HTTP connection manager filter at runtime. The route configuration will be gracefully swapped in without affecting existing requests. This API, when used alongside SDS/EDS and CDS, allows implementors to build a complex routing topology (traffic shifting, blue/green deployment, etc.) that will not require any Envoy restarts other than to obtain a new Envoy binary. SDS/EDS、CDS、RDS 和 LDS The listener discovery service (LDS) layers on a mechanism by which Envoy can discover entire listeners at runtime. This includes all filter stacks, up to and including HTTP filters with embedded references to RDS. Adding LDS into the mix allows almost every aspect of Envoy to be dynamically configured. Hot restart should only be required for very rare configuration changes (admin, tracing driver, etc.) or binary updates. "},"intro/arch_overview/init.html":{"url":"intro/arch_overview/init.html","title":"初始化","keywords":"","body":"初始化 How Envoy initializes itself when it starts up is complex. This section explains at a high level how the process works. All of the following happens before any listeners start listening and accepting new connections. During startup, the cluster manager goes through a multi-phase initialization where it first initializes static/DNS clusters, then predefined SDS clusters. Then it initializes CDS if applicable, waits for one response (or failure), and does the same primary/secondary initialization of CDS provided clusters. If clusters use active health checking, Envoy also does a single active HC round. Once cluster manager initialization is done, RDS and LDS initialize (if applicable). The server doesn’t start accepting connections until there has been at least one response (or failure) for LDS/RDS requests. If LDS itself returns a listener that needs an RDS response, Envoy further waits until an RDS response (or failure) is received. Note that this process takes place on every future listener addition via LDS and is known as listener warming. After all of the previous steps have taken place, the listeners start accepting new connections. This flow ensures that during hot restart the new process is fully capable of accepting and processing new connections before the draining of the old process begins. "},"intro/arch_overview/draining.html":{"url":"intro/arch_overview/draining.html","title":"排除","keywords":"","body":"排除 Draining is the process by which Envoy attempts to gracefully shed connections in response to various events. Draining occurs at the following times: The server has been manually health check failed via the healthcheck/fail admin endpoint. See the health check filter architecture overview for more information. The server is being hot restarted. Individual listeners are being modified or removed via LDS. Each configured listener has a drain_type setting which controls when draining takes place. The currently supported values are: default Envoy will drain listeners in response to all three cases above (admin drain, hot restart, and LDS update/remove). This is the default setting. modify_only Envoy will drain listeners only in response to the 2nd and 3rd cases above (hot restart and LDS update/remove). This setting is useful if Envoy is hosting both ingress and egress listeners. It may be desirable to set modify_only on egress listeners so they only drain during modifications while relying on ingress listener draining to perform full server draining when attempting to do a controlled shutdown. Note that although draining is a per-listener concept, it must be supported at the network filter level. Currently the only filters that support graceful draining are HTTP connection manager, Redis, and Mongo. "},"intro/arch_overview/scripting.html":{"url":"intro/arch_overview/scripting.html","title":"脚本","keywords":"","body":"脚本 Envoy supports experimental Lua scripting as part of a dedicated HTTP filter. "},"intro/deployment_types/deployment_types.html":{"url":"intro/deployment_types/deployment_types.html","title":"部署类型","keywords":"","body":"部署类型 Envoy is usable in a variety of different scenarios, however it’s most useful when deployed as amesh across all hosts in an infrastructure. This section describes three recommended deployment types in increasing order of complexity. Service to service only Service to service egress listener Service to service ingress listener Optional external service egress listeners Discovery service integration Configuration template Service to service plus front proxy Configuration template Service to service, front proxy, and double proxy Configuration template "},"intro/deployment_types/service_to_service.html":{"url":"intro/deployment_types/service_to_service.html","title":"仅服务之间","keywords":"","body":"仅服务之间 The above diagram shows the simplest Envoy deployment which uses Envoy as a communication bus for all traffic internal to a service oriented architecture (SOA). In this scenario, Envoy exposes several listeners that are used for local origin traffic as well as service to service traffic. 服务间 egress listener This is the port used by applications to talk to other services in the infrastructure. For example,http://localhost:9001. HTTP and gRPC requests use the HTTP/1.1 host header or the HTTP/2:authority header to indicate which remote cluster the request is destined for. Envoy handles service discovery, load balancing, rate limiting, etc. depending on the details in the configuration. Services only need to know about the local Envoy and do not need to concern themselves with network topology, whether they are running in development or production, etc. This listener supports both HTTP/1.1 or HTTP/2 depending on the capabilities of the application. 服务间 ingress listener This is the port used by remote Envoys when they want to talk to the local Envoy. For example,http://localhost:9211. Incoming requests are routed to the local service on the configured port(s). Multiple application ports may be involved depending on application or load balancing needs (for example if the service needs both an HTTP port and a gRPC port). The local Envoy performs buffering, circuit breaking, etc. as needed. Our default configurations use HTTP/2 for all Envoy to Envoy communication, regardless of whether the application uses HTTP/1.1 or HTTP/2 when egressing out of a local Envoy. HTTP/2 provides better performance via long lived connections and explicit reset notifications. 可选外部服务 egress listener Generally, an explicit egress port is used for each external service that a local service wants to talk to. This is done because some external service SDKs do not easily support overriding the hostheader to allow for standard HTTP reverse proxy behavior. For example, http://localhost:9250might be allocated for connections destined for DynamoDB. Instead of using host routing for some external services and dedicated local port routing for others, we recommend being consistent and using local port routing for all external services. 服务发现集成 The recommended service to service configuration uses an external discovery service for all cluster lookups. This provides Envoy with the most detailed information possible for use when performing load balancing, statistics gathering, etc. 配置模板 The source distribution includes an example service to service configuration that is very similar to the version that Lyft runs in production. See here for more information. "},"intro/deployment_types/front_proxy.html":{"url":"intro/deployment_types/front_proxy.html","title":"服务之间外加前端代理","keywords":"","body":"服务间外加前端代理 The above diagram shows the service to service configuration sitting behind an Envoy cluster used as an HTTP L7 edge reverse proxy. The reverse proxy provides the following features: Terminates TLS. Supports both HTTP/1.1 and HTTP/2. Full HTTP L7 routing support. Talks to the service to service Envoy clusters via the standard ingress port and using the discovery service for host lookup. Thus, the front Envoy hosts work identically to any other Envoy host, other than the fact that they do not run collocated with another service. This means that are operated in the same way and emit the same statistics. 配置模板 The source distribution includes an example front proxy configuration that is very similar to the version that Lyft runs in production. See here for more information. "},"intro/deployment_types/double_proxy.html":{"url":"intro/deployment_types/double_proxy.html","title":"服务间、前端代理、双向代理","keywords":"","body":"服务间、前端代理、双向代理 The above diagram shows the front proxy configuration alongside another Envoy cluster running as a double proxy. The idea behind the double proxy is that it is more efficient to terminate TLS and client connections as close as possible to the user (shorter round trip times for the TLS handshake, faster TCP CWND expansion, less chance for packet loss, etc.). Connections that terminate in the double proxy are then multiplexed onto long lived HTTP/2 connections running in the main data center. In the above diagram, the front Envoy proxy running in region 1 authenticates itself with the front Envoy proxy running in region 2 via TLS mutual authentication and pinned certificates. This allows the front Envoy instances running in region 2 to trust elements of the incoming requests that ordinarily would not be trustable (such as the x-forwarded-for HTTP header). Configuration template The source distribution includes an example double proxy configuration that is very similar to the version that Lyft runs in production. See here for more information. "},"intro/comparison.html":{"url":"intro/comparison.html","title":"与类似系统比较","keywords":"","body":"与类似系统比较 Overall, we believe that Envoy has a unique and compelling feature set for modern service oriented architectures. Below we compare Envoy to other related systems. Though in any particular area (edge proxy, software load balancer, service message passing layer) Envoy may not be as feature rich as some of the solutions below, in aggregate no other solution supplies the same set of overall features into a single self contained and high performance package. NOTE: Most of the projects below are under active development. Thus some of the information may become out of date. If that is the case please let us know and we will fix it. nginx nginx is the canonical modern web server. It supports serving static content, HTTP L7 reverse proxy load balancing, HTTP/2, and many other features. nginx has far more overall features than Envoy as an edge reverse proxy, though we think that most modern service oriented architectures don’t typically make use of them. Envoy provides the following main advantages over nginx as an edge proxy: Full HTTP/2 transparent proxy. Envoy supports HTTP/2 for both downstream and upstream communication. nginx only supports HTTP/2 for downstream connections. Freely available advanced load balancing. Only nginx plus (the paid server) supports similar advanced load balancing capabilities as Envoy. Ability to run the same software at the edge as well as on each service node. Many infrastructures run a mix of nginx and haproxy. A single proxy solution at every hop is substantially simpler from an operations perspective. haproxy haproxy is the canonical modern software load balancer. It also supports basic HTTP reverse proxy features. Envoy provides the following main advantages over haproxy as a load balancer: HTTP/2 support. Pluggable architecture. Integration with a remote service discovery service. Integration with a remote global rate limiting service. Substantially more detailed statistics. AWS ELB Amazon’s ELB is the standard solution for service discovery and load balancing for applications in EC2. Envoy provides the following main advantages of ELB as a load balancer and service discovery system: Statistics and logging (CloudWatch statistics are delayed and extremely lacking in detail, logs must be retrieved from S3 and have a fixed format). Stability (it is common to see sporadic instability when using ELBs which ends up being impossible to debug). Advanced load balancing and direct connection between nodes. An Envoy mesh avoids an additional network hop via variably performing elastic hardware. The load balancer can make better decisions and gather more interesting statistics based on zone, canary status, etc. The load balancer also supports advanced features such as retry. AWS recently released the application load balancer product. This product adds HTTP/2 support as well as basic HTTP L7 request routing to multiple backend clusters. The feature set is still small compared to Envoy and performance and stability are unknown, but it’s clear that AWS will continue to invest in this area in the future. SmartStack SmartStack is an interesting solution which provides additional service discovery and health checking support on top of haproxy. At a high level, SmartStack has most of the same goals as Envoy (out of process architecture, application platform agnostic, etc.). Envoy provides the following main advantages over SmartStack as a load balancer and service discovery package: All of the previously mentioned advantages over haproxy. Integrated service discovery and active health checking. Envoy provides everything in a single high performance package. Finagle Finagle is Twitter’s Scala/JVM service to service communication library. It is used by Twitter and many other companies that have a primarily JVM based architecture. It has many of the same features as Envoy such as service discovery, load balancing, filters, etc. Envoy provides the following main advantages over Finagle as a load balancer and service discovery package: Eventually consistent service discovery via distributed active health checking. Order of magnitude better performance across all metrics (memory consumption, CPU usage, and P99 latency properties). Out of process and application agnostic architecture. Envoy works with any application stack. proxygen 和 wangle proxygen is Facebook’s high performance C++11 HTTP proxy library, written on top of a Finagle like C++ library called wangle. From a code perspective, Envoy uses most of the same techniques as proxygen to obtain high performance as an HTTP library/proxy. Beyond that however the two projects are not really comparable as Envoy is a complete self contained server with a large feature set versus a library that must be built into something by each project individually. gRPC gRPC is a new multi-platform message passing system out of Google. It uses an IDL to describe an RPC library and then implements application specific runtimes for a variety of different languages. The underlying transport is HTTP/2. Although gRPC likely has the goal of implementing many Envoy like features in the future (load balancing, etc.), as of this writing the various runtimes are somewhat immature and are primarily focused on serialization/de-serialization. We consider gRPC to be a companion to Envoy versus a competitor. How Envoy integrates with gRPC is described here. linkerd linkerd is a standalone, open source RPC routing proxy built on Netty and Finagle (Scala/JVM). linkerd offers many of Finagle’s features, including latency-aware load balancing, connection pooling, circuit-breaking, retry budgets, deadlines, tracing, fine-grained instrumentation, and a traffic routing layer for request-level routing. linkerd provides a pluggable service discovery interface (with standard support for Consul and ZooKeeper, as well as the Marathon and Kubernetes APIs). linkerd’s memory and CPU requirements are significantly higher than Envoy’s. In contrast to Envoy, linkerd provides a minimalist configuration language, and explicitly does not support hot reloads, relying instead on dynamic provisioning and service abstractions. linkerd supports HTTP/1.1, Thrift, ThriftMux, HTTP/2 (experimental) and gRPC (experimental). nghttp2 nghttp2 is a project that contains a few different things. Primarily, it contains a library (nghttp2) that implements the HTTP/2 protocol. Envoy uses this library (with a very thin wrapper on top) for its HTTP/2 support. The project also contains a very useful load testing tool (h2load) as well as a reverse proxy (nghttpx). From a comparison perspective, Envoy is most similar to nghttpx. nghttpx is a transparent HTTP/1 HTTP/2 reverse proxy, supports TLS termination, correctly supports gRPC proxying, among a variety of other features. With that said, we consider nghttpx to be an excellent example of a variety of proxy features, rather than a robust production ready solution. Envoy’s focus is much more targeted towards observability, general operational agility, and advanced load balancing features. "},"intro/getting_help.html":{"url":"intro/getting_help.html","title":"获取帮助","keywords":"","body":"获取帮助 We are very interested in building a community around Envoy. Please reach out to us if you are interested in using it and need help or want to contribute. Please see contact info. 报告安全性缺陷 Please see security contact info. "},"intro/version_history.html":{"url":"intro/version_history.html","title":"历史版本","keywords":"","body":"历史版本 1.7.0 (Pending) access log: ability to log response trailers access log: ability to format START_TIME access log: added DYNAMIC_METADATA access log formatter. access log: added HeaderFilter to filter logs based on request headers admin: added GET /config_dump for dumping the current configuration and associated xDS version information (if applicable). admin: added GET /stats/prometheus as an alternative endpoint for getting stats in prometheus format. admin: added /runtime_modify endpoint to add or change runtime values admin: mutations must be sent as POSTs, rather than GETs. Mutations include:POST /cpuprofiler, POST /healthcheck/fail, POST /healthcheck/ok, POST /logging, POST /quitquitquit, POST /reset_counters,POST /runtime_modify?key1=value1&key2=value2&keyN=valueN, admin: removed /routes endpoint; route configs can now be found at the /config_dump endpoint. buffer filter: the buffer filter can be optionally disabled or overridden with route-local configuration. cli: added –config-yaml flag to the Envoy binary. When set its value is interpreted as a yaml representation of the bootstrap config and overrides –config-path. cluster: Add option to close tcp_proxy upstream connections when health checks fail. cluster: Add option to drain connections from hosts after they are removed from service discovery, regardless of health status. cluster: fixed bug preventing the deletion of all endpoints in a priority health check: added ability to set additional HTTP headers for HTTP health check. health check: added support for EDS delivered endpoint health status. health check: added interval overrides for health state transitions from healthy to unhealthy, unhealthy to healthy and for subsequent checks on unhealthy hosts. health check http filter: added generic header matching to trigger health check response. Deprecated the endpoint option. health check: added support for custom health check. http: filters can now optionally support virtual host, route, and weighted cluster local configuration. http: added the ability to pass DNS type Subject Alternative Names of the client certificate in the x-forwarded-client-cert header. listeners: added tcp_fast_open_queue_length option. load balancing: added weighted round robin support. The round robin scheduler now respects endpoint weights and also has improved fidelity across picks. load balancer: Locality weighted load balancing is now supported. load balancer: ability to configure zone aware load balancer settings through the API logger: added the ability to optionally set the log format via the --log-format option. logger: all logging levels can be configured at run-time: trace debug info warning error critical. sockets: added capture transport socket extension to support recording plain text traffic and PCAP generation. sockets: added IP_FREEBIND socket option support for listeners and upstream connections viacluster manager wide and cluster specific options. sockets: added IP_TRANSPARENT socket option support for listeners. sockets: added SO_KEEPALIVE socket option for upstream connections per cluster. stats: added support for histograms. stats: added option to configure the statsd prefix tls: removed support for legacy SHA-2 CBC cipher suites. tracing: the sampling decision is now delegated to the tracers, allowing the tracer to decide when and if to use it. For example, if the x-b3-sampled header is supplied with the client request, its value will override any sampling decision made by the Envoy proxy. websocket: support configuring idle_timeout and max_connect_attempts. 1.6.0 (March 20, 2018) access log: added DOWNSTREAM_REMOTE_ADDRESS, DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT, and DOWNSTREAM_LOCAL_ADDRESS access log formatters. DOWNSTREAM_ADDRESS access log formatter has been deprecated. access log: added less than or equal (LE) comparison filter. access log: added configuration to runtime filter to set default sampling rate, divisor, and whether to use independent randomness or not. admin: added /runtime admin endpoint to read the current runtime values. build: added support for building Envoy with exported symbols. This change allows scripts loaded with the Lua filter to load shared object libraries such as those installed via LuaRocks. config: added support for sending error details as grpc.rpc.Status in DiscoveryRequest. config: added support for inline delivery of TLS certificates and private keys. config: added restrictions for the backing config sources of xDS resources. For filesystem based xDS the file must exist at configuration time. For cluster based xDS the backing cluster must be statically defined and be of non-EDS type. grpc: the Google gRPC C++ library client is now supported as specified in the gRPC services overview and GrpcService. grpc-json: Added support for inline descriptors. health check: added gRPC health check based on grpc.health.v1.Health service. health check: added ability to set host header value for http health check. health check: extended the health check filter to support computation of the health check response based on the percentage of healthy servers in upstream clusters. health check: added setting for no-traffic interval. http : added idle timeout for upstream http connections. http: added support for proxying 100-Continue responses. http: added the ability to pass a URL encoded PEM encoded peer certificate in the x-forwarded-client-cert header. http: added support for trusting additional hops in the x-forwarded-for request header. http: added support for incoming HTTP/1.0. hot restart: added SIGTERM propagation to children to hot-restarter.py, which enables using it as a parent of containers. ip tagging: added HTTP IP Tagging filter. listeners: added support for listening for both IPv4 and IPv6 when binding to ::. listeners: added support for listening on UNIX domain sockets. listeners: added support for abstract unix domain sockets on Linux. The abstract namespace can be used by prepending ‘@’ to a socket path. load balancer: added cluster configuration for healthy panic threshold percentage. load balancer: added Maglev consistent hash load balancer. load balancer: added support for LocalityLbEndpoints priorities. lua: added headers replace() API. lua: extended to support metadata object API. redis: added local PING support to the Redis filter. redis: added GEORADIUS_RO and GEORADIUSBYMEMBER_RO to the Redis command splitter whitelist. router: added DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT, DOWNSTREAM_LOCAL_ADDRESS, DOWNSTREAM_LOCAL_ADDRESS_WITHOUT_PORT, PROTOCOL, and UPSTREAM_METADATA header formatters. The CLIENT_IP header formatter has been deprecated. router: added gateway-error retry-on policy. router: added support for route matching based on URL query string parameters. router: added support for more granular weighted cluster routing by allowing the total_weightto be specified in configuration. router: added support for custom request/response headers with mixed static and dynamic values. router: added support for direct responses. I.e., sending a preconfigured HTTP response without proxying anywhere. router: added support for HTTPS redirects on specific routes. router: added support for prefix_rewrite for redirects. router: added support for stripping the query string for redirects. router: added support for downstream request/upstream response header manipulation in weighted cluster. router: added support for range based header matching for request routing. squash: added support for the Squash microservices debugger. Allows debugging an incoming request to a microservice in the mesh. stats: added metrics service API implementation. stats: added native DogStatsd support. stats: added support for fixed stats tag values which will be added to all metrics. tcp proxy: added support for specifying a metadata matcher for upstream clusters in the tcp filter. tcp proxy: improved TCP proxy to correctly proxy TCP half-close. tcp proxy: added idle timeout. tcp proxy: access logs now bring an IP address without a port when using DOWNSTREAM_ADDRESS. Use DOWNSTREAM_REMOTE_ADDRESS instead. tracing: added support for dynamically loading an OpenTracing tracer. tracing: when using the Zipkin tracer, it is now possible for clients to specify the sampling decision (using the x-b3-sampled header) and have the decision propagated through to subsequently invoked services. tracing: when using the Zipkin tracer, it is no longer necessary to propagate the x-ot-span-context header. See more on trace context propagation here. transport sockets: added transport socket interface to allow custom implementations of transport sockets. A transport socket provides read and write logic with buffer encryption and decryption (if applicable). The existing TLS implementation has been refactored with the interface. upstream: added support for specifying an alternate stats name while emitting stats for clusters. Many small bug fixes and performance improvements not listed. 1.5.0 (December 4, 2017) access log: added fields for UPSTREAM_LOCAL_ADDRESS and DOWNSTREAM_ADDRESS. admin: added JSON output for stats admin endpoint. admin: added basic Prometheus output for stats admin endpoint. Histograms are not currently output. admin: added version_info to the /clusters admin endpoint. config: the v2 API is now considered production ready. config: added --v2-config-only CLI flag. cors: added CORS filter. health check: added x-envoy-immediate-health-check-fail header support. health check: added reuse_connection option. http: added per-listener stats. http: end-to-end HTTP flow control is now complete across both connections, streams, and filters. load balancer: added subset load balancer. load balancer: added ring size and hash configuration options. This used to be configurable via runtime. The runtime configuration was deleted without deprecation as we are fairly certain no one is using it. log: added the ability to optionally log to a file instead of stderr via the --log-path option. listeners: added drain_type option. lua: added experimental Lua filter. mongo filter: added fault injection. mongo filter: added “drain close” support. outlier detection: added HTTP gateway failure type. See DEPRECATED.md for outlier detection stats deprecations in this release. redis: the redis proxy filter is now considered production ready. redis: added “drain close” functionality. router: added x-envoy-overloaded support. router: added regex route matching. router: added custom request headers for upstream requests. router: added downstream IP hashing for HTTP ketama routing. router: added cookie hashing. router: added start_child_span option to create child span for egress calls. router: added optional upstream logs. router: added complete custom append/override/remove support of request/response headers. router: added support to specify response code during redirect. router: added configuration to return either a 404 or 503 if the upstream cluster does not exist. runtime: added comment capability. server: change default log level (-l) to info. stats: maximum stat/name sizes and maximum number of stats are now variable via the--max-obj-name-len and --max-stats options. tcp proxy: added access logging. tcp proxy: added configurable connect retries. tcp proxy: enable use of outlier detector. tls: added SNI support. tls: added support for specifying TLS session ticket keys. tls: allow configuration of the min and max TLS protocol versions. tracing: added custom trace span decorators. Many small bug fixes and performance improvements not listed. 1.4.0 (August 24, 2017) macOS is now supported. (A few features are missing such as hot restart and original destination routing). YAML is now directly supported for config files. Added /routes admin endpoint. End-to-end flow control is now supported for TCP proxy, HTTP/1, and HTTP/2. HTTP flow control that includes filter buffering is incomplete and will be implemented in 1.5.0. Log verbosity compile time flag added. Hot restart compile time flag added. Original destination cluster and load balancer added. WebSocket is now supported. Virtual cluster priorities have been hard removed without deprecation as we are reasonably sure no one is using this feature. Route validate_clusters option added. x-envoy-downstream-service-node header added. x-forwarded-client-cert header added. Initial HTTP/1 forward proxy support for absolute URLs has been added. HTTP/2 codec settings are now configurable. gRPC/JSON transcoder filter added. gRPC web filter added. Configurable timeout for the rate limit service call in the network and HTTP rate limit filters. x-envoy-retry-grpc-on header added. LDS API added. TLS require_client_certificate option added. Configuration check tool added. JSON schema check tool added. Config validation mode added via the --mode option. --local-address-ip-version option added. IPv6 support is now complete. UDP statsd_ip_address option added. Per-cluster DNS resolvers added. Fault filter enhancements and fixes. Several features are deprecated as of the 1.4.0 release. They will be removed at the beginning of the 1.5.0 release cycle. We explicitly call out that the HttpFilterConfigFactory filter API has been deprecated in favor of NamedHttpFilterConfigFactory. Many small bug fixes and performance improvements not listed. 1.3.0 (May 17, 2017) As of this release, we now have an official breaking change policy. Note that there are numerous breaking configuration changes in this release. They are not listed here. Future releases will adhere to the policy and have clear documentation on deprecations and changes. Bazel is now the canonical build system (replacing CMake). There have been a huge number of changes to the development/build/test flow. See /bazel/README.md and /ci/README.md for more information. Outlier detection has been expanded to include success rate variance, and all parameters are now configurable in both runtime and in the JSON configuration. TCP level listener and cluster connections now have configurable receive buffer limits at which point connection level back pressure is applied. Full end to end flow control will be available in a future release. Redis health checking has been added as an active health check type. Full Redis support will be documented/supported in 1.4.0. TCP health checking now supports a “connect only” mode that only checks if the remote server can be connected to without writing/reading any data. BoringSSL is now the only supported TLS provider. The default cipher suites and ECDH curves have been updated with more modern defaults for both listener and cluster connections. The header value match rate limit action has been expanded to include an expect matchparameter. Route level HTTP rate limit configurations now do not inherit the virtual host level configurations by default. The include_vh_rate_limits to inherit the virtual host level options if desired. HTTP routes can now add request headers on a per route and per virtual host basis via therequest_headers_to_add option. The example configurations have been refreshed to demonstrate the latest features. per_try_timeout_ms can now be configured in a route’s retry policy in addition to via the x-envoy-upstream-rq-per-try-timeout-ms HTTP header. HTTP virtual host matching now includes support for prefix wildcard domains (e.g., *.lyft.com). The default for tracing random sampling has been changed to 100% and is still configurable inruntime. HTTP tracing configuration has been extended to allow tags to be populated from arbitrary HTTP headers. The HTTP rate limit filter can now be applied to internal, external, or all requests via the request_type option. Listener binding now requires specifying an address field. This can be used to bind a listener to both a specific address as well as a port. The MongoDB filter now emits a stat for queries that do not have $maxTimeMS set. The MongoDB filter now emits logs that are fully valid JSON. The CPU profiler output path is now configurable. A watchdog system has been added that can kill the server if a deadlock is detected. A route table checking tool has been added that can be used to test route tables before use. We have added an example repo that shows how to compile/link a custom filter. Added additional cluster wide information related to outlier detection to the /clusters admin endpoint. Multiple SANs can now be verified via the verify_subject_alt_name setting. Additionally, URI type SANs can be verified. HTTP filters can now be passed opaque configuration specified on a per route basis. By default Envoy now has a built in crash handler that will print a back trace. This behavior can be disabled if desired via the --define=signal_trace=disabled Bazel option. Zipkin has been added as a supported tracing provider. Numerous small changes and fixes not listed here. 1.2.0 (March 7, 2017) Cluster discovery service (CDS) API. Outlier detection (passive health checking). Envoy configuration is now checked against a JSON schema. Ring hash consistent load balancer, as well as HTTP consistent hash routing based on a policy. Vastly enhanced global rate limit configuration via the HTTP rate limiting filter. HTTP routing to a cluster retrieved from a header. Weighted cluster HTTP routing. Auto host rewrite during HTTP routing. Regex header matching during HTTP routing. HTTP access log runtime filter. LightStep tracer parent/child span association. Route discovery service (RDS) API. HTTP router x-envoy-upstream-rq-timeout-alt-response header support. use_original_dst and bind_to_port listener options (useful for iptables based transparent proxy support). TCP proxy filter route table support. Configurable stats flush interval. Various third party library upgrades, including using BoringSSL as the default SSL provider. No longer maintain closed HTTP/2 streams for priority calculations. Leads to substantial memory savings for large meshes. Numerous small changes and fixes not listed here. 1.1.0 (November 30, 2016) Switch from Jannson to RapidJSON for our JSON library (allowing for a configuration schema in 1.2.0). Upgrade recommended version of various other libraries. Configurable DNS refresh rate for DNS service discovery types. Upstream circuit breaker configuration can be overridden via runtime. Zone aware routing support. Generic header matching routing rule. HTTP/2 graceful connection draining (double GOAWAY). DynamoDB filter per shard statistics (pre-release AWS feature). Initial release of the fault injection HTTP filter. HTTP rate limit filter enhancements (note that the configuration for HTTP rate limiting is going to be overhauled in 1.2.0). Added refused-stream retry policy. Multiple priority queues for upstream clusters (configurable on a per route basis, with separate connection pools, circuit breakers, etc.). Added max connection circuit breaking to the TCP proxy filter. Added CLI options for setting the logging file flush interval as well as the drain/shutdown time during hot restart. A very large number of performance enhancements for core HTTP/TCP proxy flows as well as a few new configuration flags to allow disabling expensive features if they are not needed (specifically request ID generation and dynamic response code stats). Support Mongo 3.2 in the Mongo sniffing filter. Lots of other small fixes and enhancements not listed. 1.0.0 (September 12, 2016) Initial open source release. "},"start/start.html":{"url":"start/start.html","title":"入门指南","keywords":"","body":"入门指南 This section gets you started with a very simple configuration and provides some example configurations. Envoy does not currently provide separate pre-built binaries, but does provide Docker images. This is the fastest way to get started using Envoy. Should you wish to use Envoy outside of a Docker container, you will need to build it. These examples use the v2 Envoy API, but use only the static configuration feature of the API, which is most useful for simple requirements. For more complex requirements Dynamic Configuration is supported. 快速开始运行简单示例 These instructions run from files in the Envoy repo. The sections below give a more detailed explanation of the configuration file and execution steps for the same configuration. A very minimal Envoy configuration that can be used to validate basic plain HTTP proxying is available in configs/google_com_proxy.v2.yaml. This is not intended to represent a realistic Envoy deployment: $ docker pull envoyproxy/envoy:latest $ docker run --rm -d -p 10000:10000 envoyproxy/envoy:latest $ curl -v localhost:10000 The Docker image used will contain the latest version of Envoy and a basic Envoy configuration. This basic configuration tells Envoy to route incoming requests to *.google.com. 简单的配置 Envoy can be configured using a single YAML file passed in as an argument on the command line. The admin message is required to configure the administration server. The address key specifies the listening address which in this case is simply 0.0.0.0:9901. admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9901 } The static_resources contains everything that is configured statically when Envoy starts, as opposed to the means of configuring resources dynamically when Envoy is running. The v2 API Overview describes this. static_resources: The specification of the listeners. listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { host_rewrite: www.google.com, cluster: service_google } http_filters: - name: envoy.router The specification of the clusters. clusters: - name: service_google connect_timeout: 0.25s type: LOGICAL_DNS # Comment out the following line to test on v6 networks dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: google.com, port_value: 443 }}] tls_context: { sni: www.google.com } 使用 Envoy Docker 镜像 Create a simple Dockerfile to execute Envoy, which assumes that envoy.yaml (described above) is in your local directory. You can refer to the Command line options. FROM envoyproxy/envoy:latest COPY envoy.yaml /etc/envoy/envoy.yaml Build the Docker image that runs your configuration using: $ docker build -t envoy:v1 And now you can execute it with: $ docker run -d --name envoy -p 9901:9901 -p 10000:10000 envoy:v1 And finally test is using: $ curl -v localhost:10000 If you would like to use envoy with docker-compose you can overwrite the provided configuration file by using a volume. Sandbox We’ve created a number of sandboxes using Docker Compose that set up different environments to test out Envoy’s features and show sample configurations. As we gauge peoples’ interests we will add more sandboxes demonstrating different features. The following sandboxes are available: Front Proxy Zipkin Tracing Jaeger Tracing Jaeger Native Tracing gRPC Bridge 其他用例 In addition to the proxy itself, Envoy is also bundled as part of several open source distributions that target specific use cases. Envoy as an API Gateway in Kubernetes "},"start/sandboxes/front_proxy.html":{"url":"start/sandboxes/front_proxy.html","title":"前端代理","keywords":"","body":"前端代理 To get a flavor of what Envoy has to offer as a front proxy, we are releasing a docker compose sandbox that deploys a front envoy and a couple of services (simple flask apps) colocated with a running service envoy. The three containers will be deployed inside a virtual network called envoymesh. Below you can see a graphic showing the docker compose deployment: All incoming requests are routed via the front envoy, which is acting as a reverse proxy sitting on the edge of the envoymesh network. Port 80 is mapped to port 8000 by docker compose (see /examples/front-proxy/docker-compose.yml). Moreover, notice that all traffic routed by the front envoy to the service containers is actually routed to the service envoys (routes setup in /examples/front-proxy/front-envoy.yaml). In turn the service envoys route the request to the flask app via the loopback address (routes setup in /examples/front-proxy/service-envoy.yaml). This setup illustrates the advantage of running service envoys collocated with your services: all requests are handled by the service envoy, and efficiently routed to your services. 运行 Sandbox The following documentation runs through the setup of an envoy cluster organized as is described in the image above. Step 1: Install Docker Ensure that you have a recent versions of docker, docker-compose and docker-machine installed. A simple way to achieve this is via the Docker Toolbox. Step 2: Docker Machine setup First let’s create a new machine which will hold the containers: $ docker-machine create --driver virtualbox default $ eval $(docker-machine env default) Step 4: Clone the Envoy repo, and start all of our containers If you have not cloned the envoy repo, clone it with git clone git@github.com:envoyproxy/envoy or git clone https://github.com/envoyproxy/envoy.git: $ pwd envoy/examples/front-proxy $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- example_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp Step 5: Test Envoy’s routing capabilities You can now send a request to both services via the front-envoy. For service1: $ curl -v $(docker-machine ip default):8000/service/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > For service2: $ curl -v $(docker-machine ip default):8000/service/2 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/2 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > Notice that each request, while sent to the front envoy, was correctly routed to the respective application. Step 6: Test Envoy’s load balancing capabilities Now let’s scale up our service1 nodes to demonstrate the clustering abilities of envoy.: $ docker-compose scale service1=3 Creating and starting example_service1_2 ... done Creating and starting example_service1_3 ... done Now if we send a request to service1 multiple times, the front envoy will load balance the requests by doing a round robin of the three service1 machines: $ curl -v $(docker-machine ip default):8000/service/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > Step 7: enter containers and curl services In addition of using curl from your host machine, you can also enter the containers themselves and curl from inside them. To enter a container you can use docker-compose exec /bin/bash. For example we can enter the front-envoy container, and curl for services locally: $ docker-compose exec front-envoy /bin/bash root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: 85ac151715c6 resolvedhostname: 172.19.0.3 root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: 20da22cfc955 resolvedhostname: 172.19.0.5 root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: f26027f1ce28 resolvedhostname: 172.19.0.6 root@81288499f9d7:/# curl localhost:80/service/2 Hello from behind Envoy (service 2)! hostname: 92f4a3737bbc resolvedhostname: 172.19.0.2 Step 8: enter containers and curl admin When envoy runs it also attaches an admin to your desired port. In the example configs the admin is bound to port 8001. We can curl it to gain useful information. For example you can curl``/server_info to get information about the envoy version you are running. Additionally you can curl /stats to get statistics. For example inside frontenvoy we can get: $ docker-compose exec front-envoy /bin/bash root@e654c2c83277:/# curl localhost:8001/server_info envoy 10e00b/RELEASE live 142 142 0 root@e654c2c83277:/# curl localhost:8001/stats cluster.service1.external.upstream_rq_200: 7 ... cluster.service1.membership_change: 2 cluster.service1.membership_total: 3 ... cluster.service1.upstream_cx_http2_total: 3 ... cluster.service1.upstream_rq_total: 7 ... cluster.service2.external.upstream_rq_200: 2 ... cluster.service2.membership_change: 1 cluster.service2.membership_total: 1 ... cluster.service2.upstream_cx_http2_total: 1 ... cluster.service2.upstream_rq_total: 2 ... Notice that we can get the number of members of upstream clusters, number of requests fulfilled by them, information about http ingress, and a plethora of other useful stats. "},"start/sandboxes/zipkin_tracing.html":{"url":"start/sandboxes/zipkin_tracing.html","title":"Zipkin 追踪","keywords":"","body":"Zipkin 追踪 The Zipkin tracing sandbox demonstrates Envoy’s request tracing capabilities using Zipkin as the tracing provider. This sandbox is very similar to the front proxy architecture described above, with one difference: service1 makes an API call to service2 before returning a response. The three containers will be deployed inside a virtual network called envoymesh. All incoming requests are routed via the front envoy, which is acting as a reverse proxy sitting on the edge of the envoymesh network. Port 80 is mapped to port 8000 by docker compose (see /examples/zipkin-tracing/docker-compose.yml). Notice that all envoys are configured to collect request traces (e.g., http_connection_manager/config/tracing setup in /examples/zipkin-tracing/front-envoy-zipkin.yaml) and setup to propagate the spans generated by the Zipkin tracer to a Zipkin cluster (trace driver setup in /examples/zipkin-tracing/front-envoy-zipkin.yaml). Before routing a request to the appropriate service envoy or the application, Envoy will take care of generating the appropriate spans for tracing (parent/child/shared context spans). At a high-level, each span records the latency of upstream API calls as well as information needed to correlate the span with other related spans (e.g., the trace ID). One of the most important benefits of tracing from Envoy is that it will take care of propagating the traces to the Zipkin service cluster. However, in order to fully take advantage of tracing, the application has to propagate trace headers that Envoy generates, while making calls to other services. In the sandbox we have provided, the simple flask app (see trace function in /examples/front-proxy/service.py) acting as service1 propagates the trace headers while making an outbound call to service2. 运行 Sandbox The following documentation runs through the setup of an envoy cluster organized as is described in the image above. Step 1: Build the sandbox To build this sandbox example, and start the example apps run the following commands: $ pwd envoy/examples/zipkin-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- zipkintracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp zipkintracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp zipkintracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp Step 2: Generate some load You can now send a request to service1 via the front-envoy as follows: $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > Step 3: View the traces in Zipkin UI Point your browser to http://localhost:9411 . You should see the Zipkin dashboard. If this ip address is incorrect, you can find the correct one by running: $ docker-machine ip default. Set the service to “front-proxy” and set the start time to a few minutes before the start of the test (step 2) and hit enter. You should see traces from the front-proxy. Click on a trace to explore the path taken by the request from front-proxy to service1 to service2, as well as the latency incurred at each hop. "},"start/sandboxes/jaeger_tracing.html":{"url":"start/sandboxes/jaeger_tracing.html","title":"Jaeger 追踪","keywords":"","body":"Jaeger Tracing The Jaeger tracing sandbox demonstrates Envoy’s request tracing capabilities using Jaeger as the tracing provider. This sandbox is very similar to the front proxy architecture described above, with one difference: service1 makes an API call to service2 before returning a response. The three containers will be deployed inside a virtual network called envoymesh. All incoming requests are routed via the front envoy, which is acting as a reverse proxy sitting on the edge of the envoymesh network. Port 80 is mapped to port 8000 by docker compose (see /examples/jaeger-tracing/docker-compose.yml). Notice that all envoys are configured to collect request traces (e.g., http_connection_manager/config/tracing setup in /examples/jaeger-tracing/front-envoy-jaeger.yaml) and setup to propagate the spans generated by the Jaeger tracer to a Jaeger cluster (trace driver setup in /examples/jaeger-tracing/front-envoy-jaeger.yaml). Before routing a request to the appropriate service envoy or the application, Envoy will take care of generating the appropriate spans for tracing (parent/child context spans). At a high-level, each span records the latency of upstream API calls as well as information needed to correlate the span with other related spans (e.g., the trace ID). One of the most important benefits of tracing from Envoy is that it will take care of propagating the traces to the Jaeger service cluster. However, in order to fully take advantage of tracing, the application has to propagate trace headers that Envoy generates, while making calls to other services. In the sandbox we have provided, the simple flask app (see trace function in /examples/front-proxy/service.py) acting as service1 propagates the trace headers while making an outbound call to service2. Running the Sandbox The following documentation runs through the setup of an envoy cluster organized as is described in the image above. Step 1: Build the sandbox To build this sandbox example, and start the example apps run the following commands: $ pwd envoy/examples/jaeger-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- jaegertracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp Step 2: Generate some load You can now send a request to service1 via the front-envoy as follows: $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > Step 3: View the traces in Jaeger UI Point your browser to http://localhost:16686 . You should see the Jaeger dashboard. Set the service to “front-proxy” and hit ‘Find Traces’. You should see traces from the front-proxy. Click on a trace to explore the path taken by the request from front-proxy to service1 to service2, as well as the latency incurred at each hop. "},"start/sandboxes/jaeger_native_tracing.html":{"url":"start/sandboxes/jaeger_native_tracing.html","title":"Jaeger 原生追踪","keywords":"","body":"Jaeger 原生追踪 The Jaeger tracing sandbox demonstrates Envoy’s request tracing capabilities using Jaeger as the tracing provider and Jaeger’s native C++ client as a plugin. Using Jaeger with its native client instead of with Envoy’s builtin Zipkin client has the following advantages: Trace propagation will work with other other services using Jaeger without needing to make configuration changes. A variety of different sampling strategies can be used, including probabilistic or remote where sampling can be centrally controlled from Jaeger’s backend. Spans are sent to the collector in a more efficient binary encoding. This sandbox is very similar to the front proxy architecture described above, with one difference: service1 makes an API call to service2 before returning a response. The three containers will be deployed inside a virtual network called envoymesh. (Note: the sandbox only works on x86-64). All incoming requests are routed via the front envoy, which is acting as a reverse proxy sitting on the edge of the envoymesh network. Port 80 is mapped to port 8000 by docker compose (see /examples/jaeger-native-tracing/docker-compose.yml). Notice that all envoys are configured to collect request traces (e.g., http_connection_manager/config/tracing setup in /examples/jaeger-native-tracing/front-envoy-jaeger.yaml) and setup to propagate the spans generated by the Jaeger tracer to a Jaeger cluster (trace driver setup in /examples/jaeger-native-tracing/front-envoy-jaeger.yaml). Before routing a request to the appropriate service envoy or the application, Envoy will take care of generating the appropriate spans for tracing (parent/child context spans). At a high-level, each span records the latency of upstream API calls as well as information needed to correlate the span with other related spans (e.g., the trace ID). One of the most important benefits of tracing from Envoy is that it will take care of propagating the traces to the Jaeger service cluster. However, in order to fully take advantage of tracing, the application has to propagate trace headers that Envoy generates, while making calls to other services. In the sandbox we have provided, the simple flask app (see trace function in /examples/front-proxy/service.py) acting as service1 propagates the trace headers while making an outbound call to service2. Running the Sandbox The following documentation runs through the setup of an envoy cluster organized as is described in the image above. Step 1: Build the sandbox To build this sandbox example, and start the example apps run the following commands: $ pwd envoy/examples/jaeger-native-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- jaegertracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp Step 2: Generate some load You can now send a request to service1 via the front-envoy as follows: $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > Step 3: View the traces in Jaeger UI Point your browser to http://localhost:16686 . You should see the Jaeger dashboard. Set the service to “front-proxy” and hit ‘Find Traces’. You should see traces from the front-proxy. Click on a trace to explore the path taken by the request from front-proxy to service1 to service2, as well as the latency incurred at each hop. "},"start/distro/ambassador.html":{"url":"start/distro/ambassador.html","title":"Envoy 作为 Kubernetes 的 API 网关","keywords":"","body":"Envoy 作为 Kubernetes 的 API 网关 A common scenario for using Envoy is deploying it as an edge service (API Gateway) in Kubernetes. Ambassador is an open source distribution of Envoy designed for Kubernetes. Ambassador uses Envoy for all L4/L7 management and Kubernetes for reliability, availability, and scalability. Ambassador operates as a specialized control plane to expose Envoy’s functionality as Kubernetes annotations. This example will walk through how you can deploy Envoy on Kubernetes via Ambassador. 部署 Ambassador Ambassador is configured via Kubernetes deployments. To install Ambassador/Envoy on Kubernetes, run the following if you’re using a cluster with RBAC enabled: kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-rbac.yaml or this if you are not using RBAC: kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-no-rbac.yaml The above YAML will create a Kubernetes deployment for Ambassador that includes readiness and liveness checks. By default, it will also create 3 instances of Ambassador. Each Ambassador instance consists of an Envoy proxy along with the Ambassador control plane. We’ll now need to create a Kubernetes service to point to the Ambassador deployment. In this example, we’ll use a LoadBalancer service. If your cluster doesn’t support LoadBalancer services, you’ll need to change to a NodePort or ClusterIP. --- apiVersion: v1 kind: Service metadata: labels: service: ambassador name: ambassador spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: service: ambassador Save this YAML to a file ambassador-svc.yaml. Then, deploy this service to Kubernetes: kubectl apply -f ambassador-svc.yaml At this point, Envoy is now running on your cluster, along with the Ambassador control plane. 配置 Ambassador Ambassador uses Kubernetes annotations to add or remove configuration. This sample YAML will add a route to Google, similar to the basic configuration example in the Getting Started guide. --- apiVersion: v1 kind: Service metadata: name: google annotations: getambassador.io/config: | --- apiVersion: ambassador/v0 kind: Mapping name: google_mapping prefix: /google/ service: https://google.com:443 host_rewrite: www.google.com spec: type: ClusterIP clusterIP: None Save the above into a file called google.yaml. Then run: kubectl apply -f google.yaml Ambassador will detect the change to your Kubernetes annotation and add the route to Envoy. Note that we used a dummy service in this example; typically, you would associate the annotation with your real Kubernetes service. Testing the mapping You can test this mapping by getting the external IP address for the Ambassador service, and then sending a request via curl. $ kubectl get svc ambassador NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE ambassador 10.19.241.98 35.225.154.81 80:32491/TCP 15m $ curl -v 35.225.154.81/google/ 更多 Ambassador exposes multiple Envoy features on mappings, such as CORS, weighted round robin, gRPC, TLS, and timeouts. For more information, read the configuration documentation. "},"install/building.html":{"url":"install/building.html","title":"构建","keywords":"","body":"构建 The Envoy build system uses Bazel. In order to ease initial building and for a quick start, we provide an Ubuntu 16 based docker container that has everything needed inside of it to build and statically link envoy, see ci/README.md. In order to build manually, follow the instructions at bazel/README.md. 要求 Envoy was initially developed and deployed on Ubuntu 14 LTS. It should work on any reasonably recent Linux including Ubuntu 16 LTS. Building Envoy has the following requirements: GCC 5+ (for C++14 support). These pre-built third party dependencies. These Bazel native dependencies. Please see the linked CI and Bazel documentation for more information on performing manual builds. 预构建的二进制文件 On every master commit we create a set of lightweight Docker images that contain the Envoy binary. We also tag the docker images with release versions when we do official releases. envoyproxy/envoy: Release binary with symbols stripped on top of an Ubuntu Xenial base. envoyproxy/envoy-alpine: Release binary with symbols stripped on top of a glibc alpine base. envoyproxy/envoy-alpine-debug: Release binary with debug symbols on top of a glibc alpine base. We will consider producing additional binary types depending on community interest in helping with CI, packaging, etc. Please open an issue in GitHub if desired. 修改 Envoy If you’re interested in modifying Envoy and testing your changes, one approach is to use Docker. This guide will walk through the process of building your own Envoy binary, and putting the binary in an Ubuntu container. Building an Envoy Docker image "},"install/ref_configs.html":{"url":"install/ref_configs.html","title":"参考配置","keywords":"","body":"参考配置 The source distribution includes a set of example configuration templates for each of the three major Envoy deployment types: Service to service Front proxy Double proxy The goal of this set of example configurations is to demonstrate the full capabilities of Envoy in a complex deployment. All features will not be applicable to all use cases. For full documentation see the configuration reference. Configuration generator Envoy configurations can become relatively complicated. At Lyft we use jinja templating to make the configurations easier to create and manage. The source distribution includes a version of the configuration generator that loosely approximates what we use at Lyft. We have also included three example configuration templates for each of the above three scenarios. Generator script: configs/configgen.py Service to service template: configs/envoy_service_to_service.template.json Front proxy template: configs/envoy_front_proxy.template.json Double proxy template: configs/envoy_double_proxy.template.json To generate the example configurations run the following from the root of the repo: mkdir -p generated/configs bazel build //configs:example_configs tar xvf $PWD/bazel-genfiles/configs/example_configs.tar -C generated/configs The previous command will produce three fully expanded configurations using some variables defined inside of configgen.py. See the comments inside of configgen.py for detailed information on how the different expansions work. A few notes about the example configurations: An instance of service discovery service is assumed to be running at discovery.yourcompany.net. DNS for yourcompany.net is assumed to be setup for various things. Search the configuration templates for different instances of this. Tracing is configured for LightStep. To disable this or enable Zipkin http://zipkin.io tracing, delete or change the tracing configuration accordingly. The configuration demonstrates the use of a global rate limiting service. To disable this delete the rate limit configuration. Route discovery service is configured for the service to service reference configuration and it is assumed to be running at rds.yourcompany.net. Cluster discovery service is configured for the service to service reference configuration and it is assumed that be running at cds.yourcompany.net. "},"install/tools/config_load_check_tool.html":{"url":"install/tools/config_load_check_tool.html","title":"配置负载检查工具","keywords":"","body":"配置负载检查工具 The config load check tool checks that a configuration file in JSON format is written using valid JSON and conforms to the Envoy JSON schema. This tool leverages the configuration test intest/config_test/config_test.cc. The test loads the JSON configuration file and runs server configuration initialization with it. Input The tool expects a PATH to the root of a directory that holds JSON Envoy configuration files. The tool will recursively go through the file system tree and run a configuration test for each file found. Keep in mind that the tool will try to load all files found in the path. Output The tool will output Envoy logs as it initializes the server configuration with the config it is currently testing. If there are configuration files where the JSON file is malformed or is does not conform to the Envoy JSON schema, the tool will exit with status EXIT_FAILURE. If the tool successfully loads all configuration files found it will exit with status EXIT_SUCCESS. Building The tool can be built locally using Bazel.bazel build //test/tools/config_load_check:config_load_check_tool Running The tool takes a path as described above.bazel-bin/test/tools/config_load_check/config_load_check_tool PATH "},"install/tools/route_table_check_tool.html":{"url":"install/tools/route_table_check_tool.html","title":"路由表检查工具","keywords":"","body":"路由表检查工具 The route table check tool checks whether the route parameters returned by a router match what is expected. The tool can also be used to check whether a path redirect, path rewrite, or host rewrite match what is expected. Input The tool expects two input JSON files:A router config JSON file. The router config JSON file schema is found in config. A tool config JSON file. The tool config JSON file schema is found in config. The tool config input file specifies urls (composed of authorities and paths) and expected route parameter values. Additional parameters such as additional headers are optional. Output The program exits with status EXIT_FAILURE if any test case does not match the expected route parameter value.The --details option prints out details for each test. The first line indicates the test name.If a test fails, details of the failed test cases are printed. The first field is the expected route parameter value. The second field is the actual route parameter value. The third field indicates the parameter that is compared. In the following example, Test_2 and Test_5 failed while the other tests passed. In the failed test cases, conflict details are printed.Test_1 Test_2 default other virtual_host_name Test_3 Test_4 Test_5 locations ats cluster_name Test_6Testing with valid runtime values is not currently supported, this may be added in future work. Building The tool can be built locally using Bazel.bazel build //test/tools/router_check:router_check_tool Running The tool takes two input json files and an optional command line parameter --details. The expected order of command line arguments is: 1. The router configuration json file. 2. The tool configuration json file. 3. The optional details flag.bazel-bin/test/tools/router_check/router_check_tool router_config.json tool_config.json bazel-bin/test/tools/router_check/router_check_tool router_config.json tool_config.json --details Testing A bash shell script test can be run with bazel. The test compares routes using different router and tool configuration json files. The configuration json files can be found in test/tools/router_check/test/config/… .bazel test //test/tools/router_check/... "},"install/tools/schema_validator_check_tool.html":{"url":"install/tools/schema_validator_check_tool.html","title":"Schema 验证器检查工具","keywords":"","body":"Schema 验证器检查工具 The schema validator tool validates that the passed in JSON conforms to a schema in the configuration. To validate the entire config, please refer to the config load check tool. Currently, only route config schema validation is supported. Input The tool expects two inputs:The schema type to check the passed in JSON against. The supported type is:route - for route configuration validation.The path to the JSON. Output If the JSON conforms to the schema, the tool will exit with status EXIT_SUCCESS. If the JSON does not conform to the schema, an error message is outputted detailing what doesn’t conform to the schema. The tool will exit with status EXIT_FAILURE. Building The tool can be built locally using Bazel.bazel build //test/tools/schema_validator:schema_validator_tool Running The tool takes a path as described above.bazel-bin/test/tools/schema_validator/schema_validator_tool --schema-type SCHEMA_TYPE --json-path PATH "},"configuration/overview/v1_overview.html":{"url":"configuration/overview/v1_overview.html","title":"v1 API 概览","keywords":"","body":"v1 API 概览 Attention The v1 configuration/API is now considered legacy and the deprecation schedule has been announced. Please upgrade and use the v2 configuration/API. The Envoy configuration format is written in JSON and is validated against a JSON schema. The schema can be found in source/common/json/config_schemas.cc. The main configuration for the server is contained within the listeners and cluster manager sections. The other top level elements specify miscellaneous configuration. YAML support is also provided as a syntactic convenience for hand-written configurations. Envoy will internally convert YAML to JSON if a file path ends with .yaml. In the rest of the configuration documentation, we refer exclusively to JSON. Envoy expects unambiguous YAML scalars, so if a cluster name (which should be a string) is called true, it should be written in the configuration YAML as “true”. The same applies to integer and floating point values (e.g. 1 vs. 1.0 vs. “1.0”). { \"listeners\": [], \"lds\": \"{...}\", \"admin\": \"{...}\", \"cluster_manager\": \"{...}\", \"flags_path\": \"...\", \"statsd_udp_ip_address\": \"...\", \"statsd_tcp_cluster_name\": \"...\", \"stats_flush_interval_ms\": \"...\", \"watchdog_miss_timeout_ms\": \"...\", \"watchdog_megamiss_timeout_ms\": \"...\", \"watchdog_kill_timeout_ms\": \"...\", \"watchdog_multikill_timeout_ms\": \"...\", \"tracing\": \"{...}\", \"rate_limit_service\": \"{...}\", \"runtime\": \"{...}\", } listeners (required, array) An array of listeners that will be instantiated by the server. A single Envoy process can contain any number of listeners. lds (optional, object) Configuration for the Listener Discovery Service (LDS). If not specified only static listeners are loaded. admin (required, object) Configuration for the local administration HTTP server. cluster_manager (required, object) Configuration for the cluster manager which owns all upstream clusters within the server. flags_path (optional, string) The file system path to search for startup flag files. statsd_udp_ip_address (optional, string) The UDP address of a running statsd compliant listener. If specified, statisticswill be flushed to this address. IPv4 addresses should have format host:port (ex: 127.0.0.1:855). IPv6 addresses should have URL format [host]:port (ex: [::1]:855). statsd_tcp_cluster_name (optional, string) The name of a cluster manager cluster that is running a TCP statsd compliant listener. If specified, Envoy will connect to this cluster to flush statistics. stats_flush_interval_ms (optional, integer) The time in milliseconds between flushes to configured stats sinks. For performance reasons Envoy latches counters and only flushes counters and gauges at a periodic interval. If not specified the default is 5000ms (5 seconds). watchdog_miss_timeout_ms (optional, integer) The time in milliseconds after which Envoy counts a nonresponsive thread in the “server.watchdog_miss” statistic. If not specified the default is 200ms. watchdog_megamiss_timeout_ms (optional, integer) The time in milliseconds after which Envoy counts a nonresponsive thread in the “server.watchdog_mega_miss” statistic. If not specified the default is 1000ms. watchdog_kill_timeout_ms (optional, integer) If a watched thread has been nonresponsive for this many milliseconds assume a programming error and kill the entire Envoy process. Set to 0 to disable kill behavior. If not specified the default is 0 (disabled). watchdog_multikill_timeout_ms (optional, integer) If at least two watched threads have been nonresponsive for at least this many milliseconds assume a true deadlock and kill the entire Envoy process. Set to 0 to disable this behavior. If not specified the default is 0 (disabled). tracing (optional, object) Configuration for an external tracing provider. If not specified, no tracing will be performed. rate_limit_service (optional, object) Configuration for an external rate limit service provider. If not specified, any calls to the rate limit service will immediately return success. runtime (optional, object) Configuration for the runtime configuration provider. If not specified, a “null” provider will be used which will result in all defaults being used. "},"configuration/overview/v2_overview.html":{"url":"configuration/overview/v2_overview.html","title":"v2 API 概览","keywords":"","body":"v2 API 概览 The Envoy v2 APIs are defined as proto3 Protocol Buffers in the data plane API repository. They evolve the existing v1 APIs and concepts to support: Streaming delivery of xDS API updates via gRPC. This reduces resource requirements and can lower the update latency. A new REST-JSON API in which the JSON/YAML formats are derived mechanically via the proto3 canonical JSON mapping. Delivery of updates via the filesystem, REST-JSON or gRPC endpoints. Advanced load balancing through an extended endpoint assignment API and load and resource utilization reporting to management servers. Stronger consistency and ordering properties when needed. The v2 APIs still maintain a baseline eventual consistency model. See the xDS protocol description for further details on aspects of v2 message exchange between Envoy and the management server. Bootstrap 配置 To use the v2 API, it’s necessary to supply a bootstrap configuration file. This provides static server configuration and configures Envoy to access dynamic configuration if needed. As with the v1 JSON/YAML configuration, this is supplied on the command-line via the -c flag, i.e.: ./envoy -c .{json,yaml,pb,pb_text} --v2-config-only where the extension reflects the underlying v2 config representation. The --v2-config-only flag is not strictly required as Envoy will attempt to autodetect the config file version, but this option provides an enhanced debug experience when configuration parsing fails. The Bootstrap message is the root of the configuration. A key concept in the Bootstrap message is the distinction between static and dynamic resouces. Resources such as a Listener or Cluster may be supplied either statically in static_resources or have an xDS service such as LDS or CDSconfigured in dynamic_resources. 示例 Below we will use YAML representation of the config protos and a running example of a service proxying HTTP from 127.0.0.1:10000 to 127.0.0.2:1234. 静态 A minimal fully static bootstrap config is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 127.0.0.1, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } http_filters: - name: envoy.router clusters: - name: some_service connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: 127.0.0.2, port_value: 1234 }}] 除了动态 EDS 大部分静态 A bootstrap config that continues from the above example with dynamic endpoint discovery via anEDS gRPC management server listening on 127.0.0.3:5678 is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 127.0.0.1, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } http_filters: - name: envoy.router clusters: - name: some_service connect_timeout: 0.25s lb_policy: ROUND_ROBIN type: EDS eds_cluster_config: eds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] - name: xds_cluster connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN http2_protocol_options: {} hosts: [{ socket_address: { address: 127.0.0.3, port_value: 5678 }}] Notice above that xds_cluster is defined to point Envoy at the management server. Even in an otherwise completely dynamic configurations, some static resources need to be defined to point Envoy at its xDS management server(s). In the above example, the EDS management server could then return a proto encoding of a DiscoveryResponse: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.ClusterLoadAssignment cluster_name: some_service endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 127.0.0.2 port_value: 1234 The versioning and type URL scheme that appear above are explained in more detail in the streaming gRPC subscription protocol documentation. 动态 A fully dynamic bootstrap configuration, in which all resources other than those belonging to the management server are discovered via xDS is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } dynamic_resources: lds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] cds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] static_resources: clusters: - name: xds_cluster connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN http2_protocol_options: {} hosts: [{ socket_address: { address: 127.0.0.3, port_value: 5678 }}] The management server could respond to LDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.Listener name: listener_0 address: socket_address: address: 127.0.0.1 port_value: 10000 filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO rds: route_config_name: local_route config_source: api_config_source: api_type: GRPC cluster_names: [xds_cluster] http_filters: - name: envoy.router The management server could respond to RDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.RouteConfiguration name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } The management server could respond to CDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.Cluster name: some_service connect_timeout: 0.25s lb_policy: ROUND_ROBIN type: EDS eds_cluster_config: eds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] The management server could respond to EDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.ClusterLoadAssignment cluster_name: some_service endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 127.0.0.2 port_value: 1234 管理服务器 A v2 xDS management server will implement the below endpoints as required for gRPC and/or REST serving. In both streaming gRPC and REST-JSON cases, a DiscoveryRequest is sent and aDiscoveryResponse received following the xDS protocol. gRPC streaming 端点 POST /envoy.api.v2.ClusterDiscoveryService/StreamClusters See cds.proto for the service definition. This is used by Envoy as a client when cds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /envoy.api.v2.EndpointDiscoveryService/StreamEndpoints See eds.proto for the service definition. This is used by Envoy as a client when eds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the eds_cluster_config field of the Cluster config. POST ``/envoy.api.v2.ListenerDiscoveryService/StreamListeners See lds.proto for the service definition. This is used by Envoy as a client when lds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /envoy.api.v2.RouteDiscoveryService/StreamRoutes See rds.proto for the service definition. This is used by Envoy as a client when route_config_name: some_route_name config_source: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the rds field of the HttpConnectionManager config. REST 端点 POST /v2/discovery:clusters See cds.proto for the service definition. This is used by Envoy as a client when cds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /v2/discovery:endpoints See eds.proto for the service definition. This is used by Envoy as a client when eds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the eds_cluster_config field of the Cluster config. POST /v2/discovery:listeners See lds.proto for the service definition. This is used by Envoy as a client when lds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /v2/discovery:routes See rds.proto for the service definition. This is used by Envoy as a client when route_config_name: some_route_name config_source: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the rds field of the HttpConnectionManager config. 聚合发现服务 While Envoy fundamentally employs an eventual consistency model, ADS provides an opportunity to sequence API update pushes and ensure affinity of a single management server for an Envoy node for API updates. ADS allows one or more APIs and their resources to be delivered on a single, bidirectional gRPC stream by the management server. Without this, some APIs such as RDS and EDS may require the management of multiple streams and connections to distinct management servers. ADS will allow for hitless updates of configuration by appropriate sequencing. For example, suppose foo.com was mappped to cluster X. We wish to change the mapping in the route table to point foo.com at cluster Y. In order to do this, a CDS/EDS update must first be delivered containing both clusters X and Y. Without ADS, the CDS/EDS/RDS streams may point at distinct management servers, or when on the same management server at distinct gRPC streams/connections that require coordination. The EDS resource requests may be split across two distinct streams, one for X and one for Y. ADS allows these to be coalesced to a single stream to a single management server, avoiding the need for distributed synchronization to correctly sequence the update. With ADS, the management server would deliver the CDS, EDS and then RDS updates on a single stream. ADS is only available for gRPC streaming (not REST) and is described more fully in this document. The gRPC endpoint is: POST /envoy.api.v2.AggregatedDiscoveryService/StreamAggregatedResources See discovery.proto for the service definition. This is used by Envoy as a client when ads_config: api_type: GRPC cluster_names: [some_ads_cluster] is set in the dynamic_resources of the Bootstrap config. When this is set, any of the configuration sources above can be set to use the ADS channel. For example, a LDS config could be changed from lds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] to lds_config: {ads: {}} with the effect that the LDS stream will be directed to some_ads_cluster over the shared ADS channel. 管理服务器不可达 When Envoy instance looses connectivity with the management server, Envoy will latch on to the previous configuration while actively retrying in the background to reestablish the connection with the management server. Envoy debug logs the fact that it is not able to establish a connection with the management server every time it attempts a connection. upstream_cx_connect_fail a cluster level statistic of the cluster pointing to management server provides a signal for monitoring this behavior. 状态 All features described in the v2 API reference are implemented unless otherwise noted. In the v2 API reference and the v2 API repository, all protos are frozen unless they are tagged as draft or experimental. Here, frozen means that we will not break wire format compatibility. Frozen protos may be further extended, e.g. by adding new fields, in a manner that does not break backwards compatibility. Fields in the above protos may be later deprecated, subject to thebreaking change policy, when their related functionality is no longer required. While frozen APIs have their wire format compatibility preserved, we reserve the right to change proto namespaces, file locations and nesting relationships, which may cause breaking code changes. We will aim to minimize the churn here. Protos tagged draft, meaning that they are near finalized, are likely to be at least partially implemented in Envoy but may have wire format breaking changes made prior to freezing. Protos tagged experimental, have the same caveats as draft protos and may have have major changes made prior to Envoy implementation and freezing. The current open v2 API issues are tracked here. "},"configuration/listeners/stats.html":{"url":"configuration/listeners/stats.html","title":"统计","keywords":"","body":"统计 Listener Every listener has a statistics tree rooted at listener.. with the following statistics: Name Type Description downstream_cx_total Counter Total connections downstream_cx_destroy Counter Total destroyed connections downstream_cx_active Gauge Total active connections downstream_cx_length_ms Histogram Connection length milliseconds ssl.connection_error Counter Total TLS connection errors not including failed certificate verifications ssl.handshake Counter Total successful TLS connection handshakes ssl.session_reused Counter Total successful TLS session resumptions ssl.no_certificate Counter Total successul TLS connections with no client certificate ssl.fail_no_sni_match Counter Total TLS connections that were rejected because of missing SNI match ssl.fail_verify_no_cert Counter Total TLS connections that failed because of missing client certificate ssl.fail_verify_error Counter Total TLS connections that failed CA verification ssl.fail_verify_san Counter Total TLS connections that failed SAN verification ssl.fail_verify_cert_hash Counter Total TLS connections that failed certificate pinning verification ssl.cipher. Counter Total TLS connections that used Listener manager The listener manager has a statistics tree rooted at listener_manager. with the following statistics. Any : character in the stats name is replaced with _. Name Type Description listener_added Counter Total listeners added (either via static config or LDS) listener_modified Counter Total listeners modified (via LDS) listener_removed Counter Total listeners removed (via LDS) listener_create_success Counter Total listener objects successfully added to workers listener_create_failure Counter Total failed listener object additions to workers total_listeners_warming Gauge Number of currently warming listeners total_listeners_active Gauge Number of currently active listeners total_listeners_draining Gauge Number of currently draining listeners "},"configuration/listeners/runtime.html":{"url":"configuration/listeners/runtime.html","title":"运行时","keywords":"","body":"运行时 Listeners support the following runtime settings: ssl.alt_alpn What % of requests use the configured alt_alpn protocol string. Defaults to 0. "},"configuration/listeners/lds.html":{"url":"configuration/listeners/lds.html","title":"Listener 发现服务（LDS）","keywords":"","body":"Listener 发现服务（LDS） The listener discovery service (LDS) is an optional API that Envoy will call to dynamically fetch listeners. Envoy will reconcile the API response and add, modify, or remove known listeners depending on what is required. The semantics of listener updates are as follows: Every listener must have a unique name. If a name is not provided, Envoy will create a UUID. Listeners that are to be dynamically updated should have a unique name supplied by the management server. When a listener is added, it will be “warmed” before taking traffic. For example, if the listener references an RDS configuration, that configuration will be resolved and fetched before the listener is moved to “active.” Listeners are effectively constant once created. Thus, when a listener is updated, an entirely new listener is created (with the same listen socket). This listener goes through the same warming process described above for a newly added listener. When a listener is updated or removed, the old listener will be placed into a “draining” state much like when the entire server is drained for restart. Connections owned by the listener will be gracefully closed (if possible) for some period of time before the listener is removed and any remaining connections are closed. The drain time is set via the --drain-time-s option. Note Any listeners that are statically defined within the Envoy configuration cannot be modified or removed via the LDS API. 配置 v1 LDS API v2 LDS API 统计 LDS has a statistics tree rooted at listener_manager.lds. with the following statistics: Name Type Description config_reload Counter Total API fetches that resulted in a config reload due to a different config update_attempt Counter Total API fetches attempted update_success Counter Total API fetches completed successfully update_failure Counter Total API fetches that failed (either network or schema errors) version Gauge Hash of the contents from the last successful API fetch "},"configuration/listener_filters/original_dst_filter.html":{"url":"configuration/listener_filters/original_dst_filter.html","title":"原始目的地","keywords":"","body":"原始目的地 Original destination listener filter reads the SO_ORIGINAL_DST socket option set when a connection has been redirected by an iptables REDIRECT target, or by an iptables TPROXY target in combination with setting the listener’s transparent option. Later processing in Envoy sees the restored destination address as the connection’s local address, rather than the address at which the listener is listening at. Furthermore, an original destination cluster may be used to forward HTTP requests or TCP connections to the restored destination address. v2 API reference "},"configuration/listener_filters/tls_inspector.html":{"url":"configuration/listener_filters/tls_inspector.html","title":"TLS 检查器","keywords":"","body":"TLS 检查器 TLS inspector listener filter allows detecting whether the transport appears to be TLS or plaintext, and if it is TLS, it detects the server name indication from the client. This can be used to select aFilterChain via the sni_domains of a FilterChainMatch. SNI v2 API reference "},"configuration/network_filters/client_ssl_auth_filter.html":{"url":"configuration/network_filters/client_ssl_auth_filter.html","title":"客户端 TLS 身份验证","keywords":"","body":"客户端 TLS 身份验证 Client TLS authentication filter architecture overview v1 API reference v2 API reference Statistics Every configured client TLS authentication filter has statistics rooted at auth.clientssl..with the following statistics: Name Type Description update_success Counter Total principal update successes update_failure Counter Total principal update failures auth_no_ssl Counter Total connections ignored due to no TLS auth_ip_white_list Counter Total connections allowed due to the IP white list auth_digest_match Counter Total connections allowed due to certificate match auth_digest_no_match Counter Total connections denied due to no certificate match total_principals Gauge Total loaded principals REST API GET /v1/certs/list/approved The authentication filter will call this API every refresh interval to fetch the current list of approved certificates/principals. The expected JSON response looks like:{ \"certificates\": [] }certificates(required, array) list of approved certificates/principals.Each certificate object is defined as:{ \"fingerprint_sha256\": \"...\", }fingerprint_sha256(required, string) The SHA256 hash of the approved client certificate. Envoy will match this hash to the presented client certificate to determine whether there is a digest match. "},"configuration/network_filters/echo_filter.html":{"url":"configuration/network_filters/echo_filter.html","title":"Echo","keywords":"","body":"Echo The echo is a trivial network filter mainly meant to demonstrate the network filter API. If installed it will echo (write) all received data back to the connected downstream client. v1 API reference v2 API reference "},"configuration/network_filters/mongo_proxy_filter.html":{"url":"configuration/network_filters/mongo_proxy_filter.html","title":"Mongo 代理","keywords":"","body":"Mongo 代理 MongoDB architecture overview v1 API reference v2 API reference Fault injection The Mongo proxy filter supports fault injection. See the v1 and v2 API reference for how to configure. Statistics Every configured MongoDB proxy filter has statistics rooted at mongo.. with the following statistics: Name Type Description decoding_error Counter Number of MongoDB protocol decoding errors delay_injected Counter Number of times the delay is injected op_get_more Counter Number of OP_GET_MORE messages op_insert Counter Number of OP_INSERT messages op_kill_cursors Counter Number of OP_KILL_CURSORS messages op_query Counter Number of OP_QUERY messages op_query_tailable_cursor Counter Number of OP_QUERY with tailable cursor flag set op_query_no_cursor_timeout Counter Number of OP_QUERY with no cursor timeout flag set op_query_await_data Counter Number of OP_QUERY with await data flag set op_query_exhaust Counter Number of OP_QUERY with exhaust flag set op_query_no_max_time Counter Number of queries without maxTimeMS set op_query_scatter_get Counter Number of scatter get queries op_query_multi_get Counter Number of multi get queries op_query_active Gauge Number of active queries op_reply Counter Number of OP_REPLY messages op_reply_cursor_not_found Counter Number of OP_REPLY with cursor not found flag set op_reply_query_failure Counter Number of OP_REPLY with query failure flag set op_reply_valid_cursor Counter Number of OP_REPLY with a valid cursor cx_destroy_local_with_active_rq Counter Connections destroyed locally with an active query cx_destroy_remote_with_active_rq Counter Connections destroyed remotely with an active query cx_drain_close Counter Connections gracefully closed on reply boundaries during server drain Scatter gets Envoy defines a scatter get as any query that does not use an _id field as a query parameter. Envoy looks in both the top level document as well as within a $query field for _id. Multi gets Envoy defines a multi get as any query that does use an _id field as a query parameter, but where _id is not a scalar value (i.e., a document or an array). Envoy looks in both the top level document as well as within a $query field for _id. $comment parsing If a query has a top level $comment field (typically in addition to a $query field), Envoy will parse it as JSON and look for the following structure: { \"callingFunction\": \"...\" } callingFunction (required, string) the function that made the query. If available, the function will be used in callsite query statistics. Per command statistics The MongoDB filter will gather statistics for commands in the mongo..cmd..namespace. Name Type Description total Counter Number of commands reply_num_docs Histogram Number of documents in reply reply_size Histogram Size of the reply in bytes reply_time_ms Histogram Command time in milliseconds Per collection query statistics The MongoDB filter will gather statistics for queries in the mongo..collection..query. namespace. Name Type Description total Counter Number of queries scatter_get Counter Number of scatter gets multi_get Counter Number of multi gets reply_num_docs Histogram Number of documents in reply reply_size Histogram Size of the reply in bytes reply_time_ms Histogram Query time in milliseconds Per collection and callsite query statistics If the application provides the calling function in the $comment field, Envoy will generate per callsite statistics. These statistics match the per collection statistics but are found in the mongo..collection..callsite..query. namespace. Runtime The Mongo proxy filter supports the following runtime settings: mongo.connection_logging_enabled % of connections that will have logging enabled. Defaults to 100. This allows only a % of connections to have logging, but for all messages on those connections to be logged. mongo.proxy_enabled % of connections that will have the proxy enabled at all. Defaults to 100. mongo.logging_enabled % of messages that will be logged. Defaults to 100. If less than 100, queries may be logged without replies, etc. mongo.mongo.drain_close_enabled % of connections that will be drain closed if the server is draining and would otherwise attempt a drain close. Defaults to 100. mongo.fault.fixed_delay.percent Probability of an eligible MongoDB operation to be affected by the injected fault when there is no active fault. Defaults to the percent specified in the config. mongo.fault.fixed_delay.duration_ms The delay duration in milliseconds. Defaults to the duration_ms specified in the config. Access log format The access log format is not customizable and has the following layout: {\"time\": \"...\", \"message\": \"...\", \"upstream_host\": \"...\"} time System time that complete message was parsed, including milliseconds. message Textual expansion of the message. Whether the message is fully expanded depends on the context. Sometimes summary data is presented to avoid extremely large log sizes. upstream_host The upstream host that the connection is proxying to, if available. This is populated if the filter is used along with the TCP proxy filter. "},"configuration/network_filters/rate_limit_filter.html":{"url":"configuration/network_filters/rate_limit_filter.html","title":"速率限制","keywords":"","body":"速率限制 Global rate limiting architecture overview v1 API reference v2 API reference 统计 Every configured rate limit filter has statistics rooted at ratelimit.. with the following statistics: Name Type Description total Counter Total requests to the rate limit service error Counter Total errors contacting the rate limit service over_limit Counter Total over limit responses from the rate limit service ok Counter Total under limit responses from the rate limit service cx_closed Counter Total connections closed due to an over limit response from the rate limit service active Gauge Total active requests to the rate limit service 运行时 The network rate limit filter supports the following runtime settings: ratelimit.tcp_filter_enabled % of connections that will call the rate limit service. Defaults to 100. ratelimit.tcp_filter_enforcing % of connections that will call the rate limit service and enforce the decision. Defaults to 100. This can be used to test what would happen before fully enforcing the outcome. "},"configuration/network_filters/redis_proxy_filter.html":{"url":"configuration/network_filters/redis_proxy_filter.html","title":"Redis 代理","keywords":"","body":"Redis 代理 Redis architecture overview v1 API reference v2 API reference Statistics Every configured Redis proxy filter has statistics rooted at redis.. with the following statistics: Name Type Description downstream_cx_active Gauge Total active connections downstream_cx_protocol_error Counter Total protocol errors downstream_cx_rx_bytes_buffered Gauge Total received bytes currently buffered downstream_cx_rx_bytes_total Counter Total bytes received downstream_cx_total Counter Total connections downstream_cx_tx_bytes_buffered Gauge Total sent bytes currently buffered downstream_cx_tx_bytes_total Counter Total bytes sent downstream_cx_drain_close Counter Number of connections closed due to draining downstream_rq_active Gauge Total active requests downstream_rq_total Counter Total requests Splitter statistics The Redis filter will gather statistics for the command splitter in the redis..splitter. with the following statistics: Name Type Description invalid_request Counter Number of requests with an incorrect number of arguments unsupported_command Counter Number of commands issued which are not recognized by the command splitter Per command statistics The Redis filter will gather statistics for commands in the redis..command.. namespace. Name Type Description total Counter Number of commands Runtime The Redis proxy filter supports the following runtime settings: redis.drain_close_enabled % of connections that will be drain closed if the server is draining and would otherwise attempt a drain close. Defaults to 100. "},"configuration/network_filters/tcp_proxy_filter.html":{"url":"configuration/network_filters/tcp_proxy_filter.html","title":"TCP 代理","keywords":"","body":"TCP 代理 TCP proxy architecture overview v1 API reference v2 API reference Statistics The TCP proxy filter emits both its own downstream statistics as well as many of the cluster upstream statistics where applicable. The downstream statistics are rooted at tcp..with the following statistics: Name Type Description downstream_cx_total Counter Total number of connections handled by the filter downstream_cx_no_route Counter Number of connections for which no matching route was found or the cluster for the route was not found downstream_cx_tx_bytes_total Counter Total bytes written to the downstream connection downstream_cx_tx_bytes_buffered Gauge Total bytes currently buffered to the downstream connection downstream_cx_rx_bytes_total Counter Total bytes read from the downstream connection downstream_cx_rx_bytes_buffered Gauge Total bytes currently buffered from the downstream connection downstream_flow_control_paused_reading_total Counter Total number of times flow control paused reading from downstream downstream_flow_control_resumed_reading_total Counter Total number of times flow control resumed reading from downstream idle_timeout Counter Total number of connections closed due to idle timeout upstream_flush_total Counter Total number of connections that continued to flush upstream data after the downstream connection was closed upstream_flush_active Gauge Total connections currently continuing to flush upstream data after the downstream connection was closed "},"configuration/http_conn_man/route_matching.html":{"url":"configuration/http_conn_man/route_matching.html","title":"路由匹配","keywords":"","body":"路由匹配 Attention This section is written for the v1 API but the concepts also apply to the v2 API. It will be rewritten to target the v2 API in a future release. When Envoy matches a route, it uses the following procedure: The HTTP request’s host or :authority header is matched to a virtual host. Each route entry in the virtual host is checked, in order. If there is a match, the route is used and no further route checks are made. Independently, each virtual cluster in the virtual host is checked, in order. If there is a match, the virtual cluster is used and no further virtual cluster checks are made. "},"configuration/http_conn_man/traffic_splitting.html":{"url":"configuration/http_conn_man/traffic_splitting.html","title":"流量转换/切分","keywords":"","body":"流量转换切分 Attention This section is written for the v1 API but the concepts also apply to the v2 API. It will be rewritten to target the v2 API in a future release. Traffic shifting between two upstreams Traffic splitting across multiple upstreams Envoy’s router can split traffic to a route in a virtual host across two or more upstream clusters. There are two common use cases. Version upgrades: traffic to a route is shifted gradually from one cluster to another. The traffic shifting section describes this scenario in more detail. A/B testing or multivariate testing: two or more versions of the same service are tested simultaneously. The traffic to the route has to be split between clusters running different versions of the same service. The traffic splitting section describes this scenario in more detail. Traffic shifting between two upstreams The runtime object in the route configuration determines the probability of selecting a particular route (and hence its cluster). By using the runtime configuration, traffic to a particular route in a virtual host can be gradually shifted from one cluster to another. Consider the following example configuration, where two versions helloworld_v1 and helloworld_v2 of a service named helloworldare declared in the envoy configuration file. { \"route_config\": { \"virtual_hosts\": [ { \"name\": \"helloworld\", \"domains\": [\"*\"], \"routes\": [ { \"prefix\": \"/\", \"cluster\": \"helloworld_v1\", \"runtime\": { \"key\": \"routing.traffic_shift.helloworld\", \"default\": 50 } }, { \"prefix\": \"/\", \"cluster\": \"helloworld_v2\", } ] } ] } } Envoy matches routes with a first match policy. If the route has a runtime object, the request will be additionally matched based on the runtime value (or the default, if no value is specified). Thus, by placing routes back-to-back in the above example and specifying a runtime object in the first route, traffic shifting can be accomplished by changing the runtime value. The following are the approximate sequence of actions required to accomplish the task. In the beginning, set routing.traffic_shift.helloworld to 100, so that all requests to the helloworld virtual host would match with the v1 route and be served by the helloworld_v1cluster. To start shifting traffic to helloworld_v2 cluster, set routing.traffic_shift.helloworld to values 0 . For instance at 90, 1 out of every 10 requests to the helloworld virtual host will not match the v1 route and will fall through to the v2 route. Gradually decrease the value set in routing.traffic_shift.helloworld so that a larger percentage of requests match the v2 route. When routing.traffic_shift.helloworld is set to 0, no requests to the helloworld virtual host will match to the v1 route. All traffic would now fall through to the v2 route and be served by the helloworld_v2 cluster. Traffic splitting across multiple upstreams Consider the helloworld example again, now with three versions (v1, v2 and v3) instead of two. To split traffic evenly across the three versions (i.e., 33%, 33%, 34%), the weighted_clusters option can be used to specify the weight for each upstream cluster. Unlike the previous example, a single route entry is sufficient. The weighted_clusters configuration block in a route can be used to specify multiple upstream clusters along with weights that indicate the percentage of traffic to be sent to each upstream cluster. { \"route_config\": { \"virtual_hosts\": [ { \"name\": \"helloworld\", \"domains\": [\"*\"], \"routes\": [ { \"prefix\": \"/\", \"weighted_clusters\": { \"runtime_key_prefix\" : \"routing.traffic_split.helloworld\", \"clusters\" : [ { \"name\" : \"helloworld_v1\", \"weight\" : 33 }, { \"name\" : \"helloworld_v2\", \"weight\" : 33 }, { \"name\" : \"helloworld_v3\", \"weight\" : 34 } ] } } ] } ] } } By default, the weights must sum to exactly 100. In the V2 API, the total weight defaults to 100, but can be modified to allow finer granularity. The weights assigned to each cluster can be dynamically adjusted using the following runtime variables: routing.traffic_split.helloworld.helloworld_v1,routing.traffic_split.helloworld.helloworld_v2 and routing.traffic_split.helloworld.helloworld_v3. "},"configuration/http_conn_man/headers.html":{"url":"configuration/http_conn_man/headers.html","title":"HTTP header 操作","keywords":"","body":"HTTP header manipulation The HTTP connection manager manipulates several HTTP headers both during decoding (when the request is being received) as well as during encoding (when the response is being sent). user-agent server x-client-trace-id x-envoy-downstream-service-cluster x-envoy-downstream-service-node x-envoy-external-address x-envoy-force-trace x-envoy-internal x-forwarded-client-cert x-forwarded-for x-forwarded-proto x-request-id x-ot-span-context x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags Custom request/response headers See https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers "},"configuration/http_conn_man/header_sanitizing.html":{"url":"configuration/http_conn_man/header_sanitizing.html","title":"HTTP header sanitizing","keywords":"","body":"HTTP header sanitizing For security reasons, Envoy will “sanitize” various incoming HTTP headers depending on whether the request is an internal or external request. The sanitizing action depends on the header and may result in addition, removal, or modification. Ultimately, whether the request is considered internal or external is governed by the x-forwarded-for header (please read the linked section carefully as how Envoy populates the header is complex and depends on the use_remote_address setting). Envoy will potentially sanitize the following headers: x-envoy-decorator-operation x-envoy-downstream-service-cluster x-envoy-downstream-service-node x-envoy-expected-rq-timeout-ms x-envoy-external-address x-envoy-force-trace x-envoy-internal x-envoy-ip-tags x-envoy-max-retries x-envoy-retry-grpc-on x-envoy-retry-on x-envoy-upstream-alt-stat-name x-envoy-upstream-rq-per-try-timeout-ms x-envoy-upstream-rq-timeout-alt-response x-envoy-upstream-rq-timeout-ms x-forwarded-client-cert x-forwarded-for x-forwarded-proto x-request-id "},"configuration/http_conn_man/stats.html":{"url":"configuration/http_conn_man/stats.html","title":"统计","keywords":"","body":"统计 Every connection manager has a statistics tree rooted at http.. with the following statistics: Name Type Description downstream_cx_total Counter Total connections downstream_cx_ssl_total Counter Total TLS connections downstream_cx_http1_total Counter Total HTTP/1.1 connections downstream_cx_websocket_total Counter Total WebSocket connections downstream_cx_http2_total Counter Total HTTP/2 connections downstream_cx_destroy Counter Total connections destroyed downstream_cx_destroy_remote Counter Total connections destroyed due to remote close downstream_cx_destroy_local Counter Total connections destroyed due to local close downstream_cx_destroy_active_rq Counter Total connections destroyed with 1+ active request downstream_cx_destroy_local_active_rq Counter Total connections destroyed locally with 1+ active request downstream_cx_destroy_remote_active_rq Counter Total connections destroyed remotely with 1+ active request downstream_cx_active Gauge Total active connections downstream_cx_ssl_active Gauge Total active TLS connections downstream_cx_http1_active Gauge Total active HTTP/1.1 connections downstream_cx_websocket_active Gauge Total active WebSocket connections downstream_cx_http2_active Gauge Total active HTTP/2 connections downstream_cx_protocol_error Counter Total protocol errors downstream_cx_length_ms Histogram Connection length milliseconds downstream_cx_rx_bytes_total Counter Total bytes received downstream_cx_rx_bytes_buffered Gauge Total received bytes currently buffered downstream_cx_tx_bytes_total Counter Total bytes sent downstream_cx_tx_bytes_buffered Gauge Total sent bytes currently buffered downstream_cx_drain_close Counter Total connections closed due to draining downstream_cx_idle_timeout Counter Total connections closed due to idle timeout downstream_flow_control_paused_reading_total Counter Total number of times reads were disabled due to flow control downstream_flow_control_resumed_reading_total Counter Total number of times reads were enabled on the connection due to flow control downstream_rq_total Counter Total requests downstream_rq_http1_total Counter Total HTTP/1.1 requests downstream_rq_http2_total Counter Total HTTP/2 requests downstream_rq_active Gauge Total active requests downstream_rq_response_before_rq_complete Counter Total responses sent before the request was complete downstream_rq_rx_reset Counter Total request resets received downstream_rq_tx_reset Counter Total request resets sent downstream_rq_non_relative_path Counter Total requests with a non-relative HTTP path downstream_rq_too_large Counter Total requests resulting in a 413 due to buffering an overly large body downstream_rq_1xx Counter Total 1xx responses downstream_rq_2xx Counter Total 2xx responses downstream_rq_3xx Counter Total 3xx responses downstream_rq_4xx Counter Total 4xx responses downstream_rq_5xx Counter Total 5xx responses downstream_rq_ws_on_non_ws_route Counter Total WebSocket upgrade requests rejected by non WebSocket routes downstream_rq_time Histogram Request time milliseconds rs_too_large Counter Total response errors due to buffering an overly large body Per user agent statistics Additional per user agent statistics are rooted at http..user_agent..Currently Envoy matches user agent for both iOS (ios) and Android (android) and produces the following statistics: Name Type Description downstream_cx_total Counter Total connections downstream_cx_destroy_remote_active_rq Counter Total connections destroyed remotely with 1+ active requests downstream_rq_total Counter Total requests Per listener statistics Additional per listener statistics are rooted at listener..http.. with the following statistics: Name Type Description downstream_rq_1xx Counter Total 1xx responses downstream_rq_2xx Counter Total 2xx responses downstream_rq_3xx Counter Total 3xx responses downstream_rq_4xx Counter Total 4xx responses downstream_rq_5xx Counter Total 5xx responses Per codec statistics Each codec has the option of adding per-codec statistics. Currently only http2 has codec stats. Http2 codec statistics All http2 statistics are rooted at http2. Name Type Description header_overflow Counter Total number of connections reset due to the headers being larger than Envoy::Http::Http2::ConnectionImpl::StreamImpl::MAX_HEADER_SIZE (63k) headers_cb_no_stream Counter Total number of errors where a header callback is called without an associated stream. This tracks an unexpected occurrence due to an as yet undiagnosed bug rx_messaging_error Counter Total number of invalid received frames that violated section 8 of the HTTP/2 spec. This will result in a tx_reset rx_reset Counter Total number of reset stream frames received by Envoy too_many_header_frames Counter Total number of times an HTTP2 connection is reset due to receiving too many headers frames. Envoy currently supports proxying at most one header frame for 100-Continue one non-100 response code header frame and one frame with trailers trailers Counter Total number of trailers seen on requests coming from downstream tx_reset Counter Total number of reset stream frames transmitted by Envoy Tracing statistics Tracing statistics are emitted when tracing decisions are made. All tracing statistics are rooted at http..tracing. with the following statistics: Name Type Description random_sampling Counter Total number of traceable decisions by random sampling service_forced Counter Total number of traceable decisions by server runtime flag tracing.global_enabled client_enabled Counter Total number of traceable decisions by request header x-envoy-force-trace not_traceable Counter Total number of non-traceable decisions by request id health_check Counter Total number of non-traceable decisions by health check "},"configuration/http_conn_man/runtime.html":{"url":"configuration/http_conn_man/runtime.html","title":"运行时","keywords":"","body":"运行时 The HTTP connection manager supports the following runtime settings: http_connection_manager.represent_ipv4_remote_address_as_ipv4_mapped_ipv6 % of requests with a remote address that will have their IPv4 address mapped to IPv6. Defaults to 0. use_remote_address must also be enabled. Seerepresent_ipv4_remote_address_as_ipv4_mapped_ipv6 for more details. tracing.client_enabled % of requests that will be force traced if the x-client-trace-id header is set. Defaults to 100. tracing.global_enabled % of requests that will be traced after all other checks have been applied (force tracing, sampling, etc.). Defaults to 100. tracing.random_sampling % of requests that will be randomly traced. See here for more information. This runtime control is specified in the range 0-10000 and defaults to 10000. Thus, trace sampling can be specified in 0.01% increments. "},"configuration/http_conn_man/rds.html":{"url":"configuration/http_conn_man/rds.html","title":"路由发现服务（RDS）","keywords":"","body":"路由发现服务（RDS） The route discovery service (RDS) API is an optional API that Envoy will call to dynamically fetchroute configurations. A route configuration includes both HTTP header modifications, virtual hosts, and the individual route entries contained within each virtual host. Each HTTP connection manager filter can independently fetch its own route configuration via the API. v1 API reference v2 API reference Statistics RDS has a statistics tree rooted at http..rds... Any : character in the route_config_name name gets replaced with _ in the stats tree. The stats tree contains the following statistics: Name Type Description config_reload Counter Total API fetches that resulted in a config reload due to a different config update_attempt Counter Total API fetches attempted update_success Counter Total API fetches completed successfully update_failure Counter Total API fetches that failed (either network or schema errors) version Gauge Hash of the contents from the last successful API fetch "},"configuration/http_filters/buffer_filter.html":{"url":"configuration/http_filters/buffer_filter.html","title":"Buffer","keywords":"","body":"Buffer The buffer filter is used to stop filter iteration and wait for a fully buffered complete request. This is useful in different situations including protecting some applications from having to deal with partial requests and high network latency. v1 API reference v2 API reference Per-Route Configuration The buffer filter configuration can be overridden or disabled on a per-route basis by providing aBufferPerRoute configuration on the virtual host, route, or weighted cluster. Statistics The buffer filter outputs statistics in the http..buffer. namespace. The stat prefixcomes from the owning HTTP connection manager. Name Type Description rq_timeout Counter Total requests that timed out waiting for a full request "},"configuration/http_filters/cors_filter.html":{"url":"configuration/http_filters/cors_filter.html","title":"CORS","keywords":"","body":"CORS This is a filter which handles Cross-Origin Resource Sharing requests based on route or virtual host settings. For the meaning of the headers please refer to the pages below. https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS https://www.w3.org/TR/cors/ v1 API reference v2 API reference "},"configuration/http_filters/dynamodb_filter.html":{"url":"configuration/http_filters/dynamodb_filter.html","title":"DynamoDB","keywords":"","body":"DynamoDB DynamoDB architecture overview v1 API reference v2 API reference Statistics The DynamoDB filter outputs statistics in the http..dynamodb. namespace. The stat prefix comes from the owning HTTP connection manager. Per operation stats can be found in the http..dynamodb.operation.. namespace. Name Type Description upstream_rq_total Counter Total number of requests with upstream_rq_time Histogram Time spent on upstream_rq_total_xxx Counter Total number of requests with per response code (503/2xx/etc) upstream_rq_time_xxx Histogram Time spent on per response code (400/3xx/etc) Per table stats can be found in the http..dynamodb.table.. namespace. Most of the operations to DynamoDB involve a single table, but BatchGetItem and BatchWriteItem can include several tables, Envoy tracks per table stats in this case only if it is the same table used in all operations from the batch. Name Type Description upstream_rq_total Counter Total number of requests on table upstream_rq_time Histogram Time spent on table upstream_rq_total_xxx Counter Total number of requests on table per response code (503/2xx/etc) upstream_rq_time_xxx Histogram Time spent on table per response code (400/3xx/etc) Disclaimer: Please note that this is a pre-release Amazon DynamoDB feature that is not yet widely available. Per partition and operation stats can be found in the http..dynamodb.table.. namespace. For batch operations, Envoy tracks per partition and operation stats only if it is the same table used in all operations. Name Type Description capacity..__partition_id= Counter Total number of capacity for on table for a given Additional detailed stats: For 4xx responses and partial batch operation failures, the total number of failures for a given table and failure are tracked in the http..dynamodb.error..namespace. Name Type Description Counter Total number of specific for a given BatchFailureUnprocessedKeys Counter Total number of partial batch failures for a given Runtime The DynamoDB filter supports the following runtime settings: dynamodb.filter_enabled The % of requests for which the filter is enabled. Default is 100%. "},"configuration/http_filters/fault_filter.html":{"url":"configuration/http_filters/fault_filter.html","title":"故障注入","keywords":"","body":"故障注入 The fault injection filter can be used to test the resiliency of microservices to different forms of failures. The filter can be used to inject delays and abort requests with user-specified error codes, thereby providing the ability to stage different failure scenarios such as service failures, service overloads, high network latency, network partitions, etc. Faults injection can be limited to a specific set of requests based on the (destination) upstream cluster of a request and/or a set of pre-defined request headers. The scope of failures is restricted to those that are observable by an application communicating over the network. CPU and disk failures on the local host cannot be emulated. Currently, the fault injection filter has the following limitations: Abort codes are restricted to HTTP status codes only Delays are restricted to fixed duration. Future versions will include support for restricting faults to specific routes, injecting gRPC and HTTP/2 specific error codes and delay durations based on distributions. Configuration Note The fault injection filter must be inserted before any other filter, including the router filter. v1 API reference v2 API reference Runtime The HTTP fault injection filter supports the following global runtime settings: fault.http.abort.abort_percent % of requests that will be aborted if the headers match. Defaults to the abort_percent specified in config. If the config does not contain an abort block, then abort_percent defaults to 0. fault.http.abort.http_status HTTP status code that will be used as the of requests that will be aborted if the headers match. Defaults to the HTTP status code specified in the config. If the config does not contain an abortblock, then http_status defaults to 0. fault.http.delay.fixed_delay_percent % of requests that will be delayed if the headers match. Defaults to the delay_percent specified in the config or 0 otherwise. fault.http.delay.fixed_duration_ms The delay duration in milliseconds. If not specified, the fixed_duration_ms specified in the config will be used. If this field is missing from both the runtime and the config, no delays will be injected. Note, fault filter runtime settings for the specific downstream cluster override the default ones if present. The following are downstream specific runtime keys: fault.http..abort.abort_percent fault.http..abort.http_status fault.http..delay.fixed_delay_percent fault.http..delay.fixed_duration_ms Downstream cluster name is taken from the HTTP x-envoy-downstream-service-cluster header. If the following settings are not found in the runtime it defaults to the global runtime settings which defaults to the config settings. Statistics The fault filter outputs statistics in the http..fault. namespace. The stat prefix comes from the owning HTTP connection manager. Name Type Description delays_injected Counter Total requests that were delayed aborts_injected Counter Total requests that were aborted .delays_injected Counter Total delayed requests for the given downstream cluster .aborts_injected Counter Total aborted requests for the given downstream cluster "},"configuration/http_filters/grpc_http1_bridge_filter.html":{"url":"configuration/http_filters/grpc_http1_bridge_filter.html","title":"gRPC HTTP/1.1 bridge","keywords":"","body":"gRPC HTTP/1.1 bridge gRPC architecture overview v1 API reference v2 API reference This is a simple filter which enables the bridging of an HTTP/1.1 client which does not support response trailers to a compliant gRPC server. It works by doing the following: When a request is sent, the filter sees if the connection is HTTP/1.1 and the request content type is application/grpc. If so, when the response is received, the filter buffers it and waits for trailers and then checks the grpc-status code. If it is not zero, the filter switches the HTTP response code to 503. It also copies the grpc-status and grpc-message trailers into the response headers so that the client can look at them if it wishes. The client should send HTTP/1.1 requests that translate to the following pseudo headers: :method: POST :path: content-type: application/grpc The body should be the serialized grpc body which is: 1 byte of zero (not compressed). network order 4 bytes of proto message length. serialized proto message. Because this scheme must buffer the response to look for the grpc-status trailer it will only work with unary gRPC APIs. This filter also collects stats for all gRPC requests that transit, even if those requests are normal gRPC requests over HTTP/2. More info: wire format in gRPC over HTTP/2. Statistics The filter emits statistics in the cluster..grpc. namespace. Name Type Description ..success Counter Total successful service/method calls ..failure Counter Total failed service/method calls ..total Counter Total service/method calls "},"configuration/http_filters/grpc_json_transcoder_filter.html":{"url":"configuration/http_filters/grpc_json_transcoder_filter.html","title":"gRPC-JSON 转码","keywords":"","body":"gRPC-JSON 转码 gRPC architecture overview v1 API reference v2 API reference This is a filter which allows a RESTful JSON API client to send requests to Envoy over HTTP and get proxied to a gRPC service. The HTTP mapping for the gRPC service has to be defined by custom options. How to generate proto descriptor set Envoy has to know the proto descriptor of your gRPC service in order to do the transcoding. To generate a protobuf descriptor set for the gRPC service, you’ll also need to clone the googleapis repository from GitHub before running protoc, as you’ll need annotations.proto in your include path, to define the HTTP mapping. git clone https://github.com/googleapis/googleapis GOOGLEAPIS_DIR= Then run protoc to generate the descriptor set from bookstore.proto: protoc -I$(GOOGLEAPIS_DIR) -I. --include_imports --include_source_info \\ --descriptor_set_out=proto.pb test/proto/bookstore.proto If you have more than one proto source files, you can pass all of them in one command. "},"configuration/http_filters/grpc_web_filter.html":{"url":"configuration/http_filters/grpc_web_filter.html","title":"gRPC-Web","keywords":"","body":"gRPC-Web gRPC architecture overview v1 API reference v2 API reference This is a filter which enables the bridging of a gRPC-Web client to a compliant gRPC server by following https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-WEB.md. "},"configuration/http_filters/gzip_filter.html":{"url":"configuration/http_filters/gzip_filter.html","title":"Gzip","keywords":"","body":"Gzip Gzip is an HTTP filter which enables Envoy to compress dispatched data from an upstream service upon client request. Compression is useful in situations where large payloads need to be transmitted without compromising the response time. Configuration v2 API reference Attention The window bits is a number that tells the compressor how far ahead in the text the algorithm should be looking for repeated sequence of characters. Due to a known bug in the underlying zlib library, window bits with value eight does not work as expected. Therefore any number below that will be automatically set to 9. This issue might be solved in future releases of the library. How it works When gzip filter is enabled, request and response headers are inspected to determine whether or not the content should be compressed. The content is compressed and then sent to the client with the appropriate headers if either response and request allow. By default compression will be skipped when: A request does NOT contain accept-encoding header. A request includes accept-encoding header, but it does not contain “gzip”. A response contains a content-encoding header. A Response contains a cache-control header whose value includes “no-transform”. A response contains a transfer-encoding header whose value includes “gzip”. A response does not contain a content-type value that matches one of the selected mime-types, which default to application/javascript, application/json, application/xhtml+xml, image/svg+xml, text/css, text/html, text/plain, text/xml. Neither content-length nor transfer-encoding headers are present in the response. Response size is smaller than 30 bytes (only applicable when transfer-encoding is not chuncked). When compression is applied: The content-length is removed from response headers. Response headers contain “transfer-encoding: chunked” and “content-encoding: gzip”. The “vary: accept-encoding” header is inserted on every response. "},"configuration/http_filters/health_check_filter.html":{"url":"configuration/http_filters/health_check_filter.html","title":"健康检查","keywords":"","body":"健康检查 Health check filter architecture overview v1 API reference v2 API reference Note Note that the filter will automatically fail health checks and set the x-envoy-immediate-health-check-fail header if the /healthcheck/fail admin endpoint has been called. (The /healthcheck/okadmin endpoint reverses this behavior). "},"configuration/http_filters/ip_tagging_filter.html":{"url":"configuration/http_filters/ip_tagging_filter.html","title":"IP Tagging","keywords":"","body":"IP Tagging The HTTP IP Tagging filter sets the header x-envoy-ip-tags with the string tags for the trusted address from x-forwarded-for. If there are no tags for an address, the header is not set. The implementation for IP Tagging provides a scalable way to compare an IP address to a large list of CIDR ranges efficiently. The underlying algorithm for storing tags and IP address subnets is a Level-Compressed trie described in the paper IP-address lookup using LC-tries by S. Nilsson and G. Karlsson. Configuration v2 API reference Statistics The IP Tagging filter outputs statistics in the http..ip_tagging. namespace. The stat prefix comes from the owning HTTP connection manager. Name Type Description .hit Counter Total number of requests that have the applied to it no_hit Counter Total number of requests with no applicable IP tags total Counter Total number of requests the IP Tagging Filter operated on Runtime The IP Tagging filter supports the following runtime settings: ip_tagging.http_filter_enabled The % of requests for which the filter is enabled. Default is 100. "},"configuration/http_filters/lua_filter.html":{"url":"configuration/http_filters/lua_filter.html","title":"Lua","keywords":"","body":"Lua Attention By default Envoy is built without exporting symbols that you may need when interacting with Lua modules installed as shared objects. Envoy may need to be built with support for exported symbols. Please see the Bazel docs for more information. Overview The HTTP Lua filter allows Lua scripts to be run during both the request and response flows. LuaJITis used as the runtime. Because of this, the supported Lua version is mostly 5.1 with some 5.2 features. See the LuaJIT documentation for more details. The filter only supports loading Lua code in-line in the configuration. If local filesystem code is desired, a trivial in-line script can be used to load the rest of the code from the local environment. The design of the filter and Lua support at a high level is as follows: All Lua environments are per worker thread. This means that there is no truly global data. Any globals create and populated at load time will be visible from each worker thread in isolation. True global support may be added via an API in the future. All scripts are run as coroutines. This means that they are written in a synchronous style even though they may perform complex asynchronous tasks. This makes the scripts substantially easier to write. All network/async processing is performed by Envoy via a set of APIs. Envoy will yield the script as appropriate and resume it when async tasks are complete. Do not perform blocking operations from scripts. It is critical for performance that Envoy APIs are used for all IO. Currently supported high level features NOTE: It is expected that this list will expand over time as the filter is used in production. The API surface has been kept small on purpose. The goal is to make scripts extremely simple and safe to write. Very complex or high performance use cases are assumed to use the native C++ filter API. Inspection of headers, body, and trailers while streaming in either the request flow, response flow, or both. Modification of headers and trailers. Blocking and buffering the full request/response body for inspection. Performing an outbound async HTTP call to an upstream host. Such a call can be performed while buffering body data so that when the call completes upstream headers can be modified. Performing a direct response and skipping further filter iteration. For example, a script could make an upstream HTTP call for authentication, and then directly respond with a 403 response code. Configuration v1 API reference v2 API reference Script examples This section provides some concrete examples of Lua scripts as a more gentle introduction and quick start. Please refer to the stream handle API for more details on the supported API. -- Called on the request path. function envoy_on_request(request_handle) -- Wait for the entire request body and add a request header with the body size. request_handle:headers():add(\"request_body_size\", request_handle:body():length()) end -- Called on the response path. function envoy_on_response(response_handle) -- Wait for the entire response body and a response header with the the body size. response_handle:headers():add(\"response_body_size\", response_handle:body():length()) -- Remove a response header named 'foo' response_handle:headers():remove(\"foo\") end function envoy_on_request(request_handle) -- Make an HTTP call to an upstream host with the following headers, body, and timeout. local headers, body = request_handle:httpCall( \"lua_cluster\", { [\":method\"] = \"POST\", [\":path\"] = \"/\", [\":authority\"] = \"lua_cluster\" }, \"hello world\", 5000) -- Add information from the HTTP call into the headers that are about to be sent to the next -- filter in the filter chain. request_handle:headers():add(\"upstream_foo\", headers[\"foo\"]) request_handle:headers():add(\"upstream_body_size\", #body) end function envoy_on_request(request_handle) -- Make an HTTP call. local headers, body = request_handle:httpCall( \"lua_cluster\", { [\":method\"] = \"POST\", [\":path\"] = \"/\", [\":authority\"] = \"lua_cluster\" }, \"hello world\", 5000) -- Response directly and set a header from the HTTP call. No further filter iteration -- occurs. request_handle:respond( {[\":status\"] = \"403\", [\"upstream_foo\"] = headers[\"foo\"]}, \"nope\") end Stream handle API When Envoy loads the script in the configuration, it looks for two global functions that the script defines: function envoy_on_request(request_handle) end function envoy_on_response(response_handle) end A script can define either or both of these functions. During the request path, Envoy will run envoy_on_request as a coroutine, passing an API handle. During the response path, Envoy will run envoy_on_response as a coroutine, passing an API handle. Attention It is critical that all interaction with Envoy occur through the passed stream handle. The stream handle should not be assigned to any global variable and should not be used outside of the coroutine. Envoy will fail your script if the handle is used incorrectly. The following methods on the stream handle are supported: headers() headers = handle:headers() Returns the stream’s headers. The headers can be modified as long as they have not been sent to the next filter in the header chain. For example, they can be modified after an httpCall() or after a body() call returns. The script will fail if the headers are modified in any other situation. Returns a header object. body() body = handle:body() Returns the stream’s body. This call will cause Envoy to yield the script until the entire body has been buffered. Note that all buffering must adhere to the flow control policies in place. Envoy will not buffer more data than is allowed by the connection manager. Returns a buffer object. bodyChunks() iterator = handle:bodyChunks() Returns an iterator that can be used to iterate through all received body chunks as they arrive. Envoy will yield the script in between chunks, but will not buffer them. This can be used by a script to inspect data as it is streaming by. for chunk in request_handle:bodyChunks() do request_handle:log(0, chunk:length()) end Each chunk the iterator returns is a buffer object. trailers() trailers = handle:trailers() Returns the stream’s trailers. May return nil if there are no trailers. The trailers may be modified before they are sent to the next filter. Returns a header object. log*() handle:logTrace(message) handle:logDebug(message) handle:logInfo(message) handle:logWarn(message) handle:logErr(message) handle:logCritical(message) Logs a message using Envoy’s application logging. message is a string to log. httpCall() headers, body = handle:httpCall(cluster, headers, body, timeout) Makes an HTTP call to an upstream host. Envoy will yield the script until the call completes or has an error. cluster is a string which maps to a configured cluster manager cluster. headers is a table of key/value pairs to send. Note that the :method, :path, and :authority headers must be set. body is an optional string of body data to send. timeout is an integer that specifies the call timeout in milliseconds. Returns headers which is a table of response headers. Returns body which is the string response body. May be nil if there is no body. respond() handle:respond(headers, body) Respond immediately and do not continue further filter iteration. This call is only valid in the request flow. Additionally, a response is only possible if request headers have not yet been passed to subsequent filters. Meaning, the following Lua code is invalid: function envoy_on_request(request_handle) for chunk in request_handle:bodyChunks() do request_handle:respond( {[\":status\"] = \"100\"}, \"nope\") end end headers is a table of key/value pairs to send. Note that the :status header must be set. body is a string and supplies the optional response body. May be nil. metadata() metadata = handle:metadata() Returns the current route entry metadata. Note that the metadata should be specified under the filter name i.e. envoy.lua. Below is an example of a metadata in a route entry. metadata: filter_metadata: envoy.lua: foo: bar baz: - bad - baz Returns a metadata object. Header object API add() headers:add(key, value) Adds a header. key is a string that supplies the header key. value is a string that supplies the header value. Attention Envoy treats certain headers specially. These are known as the O(1) or inline headers. The list of inline headers can be found here. If an inline header is already present in the header map, add()will have no effect. If attempting to add() a non-inline header, the additional header will be added so that the resultant headers contains multiple header entries with the same name. Consider using the replace function if want to replace a header with another value. Note also that we understand this behavior is confusing and we may change it in a future release. get() headers:get(key) Gets a header. key is a string that supplies the header key. Returns a string that is the header value or nil if there is no such header. __pairs() for key, value in pairs(headers) do end Iterates through every header. key is a string that supplies the header key. value is a string that supplies the header value. Attention In the currently implementation, headers cannot be modified during iteration. Additionally, if it is desired to modify headers after iteration, the iteration must be completed. Meaning, do not use break or any other mechanism to exit the loop early. This may be relaxed in the future. remove() headers:remove(key) Removes a header. key supplies the header key to remove. replace() headers:replace(key, value) Replaces a header. key is a string that supplies the header key. value is a string that supplies the header value. If the header does not exist, it is added as per the add() function. Buffer API length() size = buffer:length() Gets the size of the buffer in bytes. Returns an integer. getBytes() buffer:getBytes(index, length) Get bytes from the buffer. By default Envoy will not copy all buffer bytes to Lua. This will cause a buffer segment to be copied. index is an integer and supplies the buffer start index to copy. lengthis an integer and supplies the buffer length to copy. index + length must be less than the buffer length. Metadata object API get() metadata:get(key) Gets a metadata. key is a string that supplies the metadata key. Returns the corresponding value of the given metadata key. The type of the value can be: null, boolean, number, string and table. __pairs() for key, value in pairs(metadata) do end Iterates through every metadata entry. key is a string that supplies a metadata key. value is metadata entry value. "},"configuration/http_filters/rate_limit_filter.html":{"url":"configuration/http_filters/rate_limit_filter.html","title":"速率限制","keywords":"","body":"速率限制 Global rate limiting architecture overview v1 API reference v2 API reference The HTTP rate limit filter will call the rate limit service when the request’s route or virtual host has one or more rate limit configurations that match the filter stage setting. The route can optionally include the virtual host rate limit configurations. More than one configuration can apply to a request. Each configuration results in a descriptor being sent to the rate limit service. If the rate limit service is called, and the response for any of the descriptors is over limit, a 429 response is returned. Composing Actions Attention This section is written for the v1 API but the concepts also apply to the v2 API. It will be rewritten to target the v2 API in a future release. Each rate limit action on the route or virtual host populates a descriptor entry. A vector of descriptor entries compose a descriptor. To create more complex rate limit descriptors, actions can be composed in any order. The descriptor will be populated in the order the actions are specified in the configuration. Example 1 For example, to generate the following descriptor: (\"generic_key\", \"some_value\") (\"source_cluster\", \"from_cluster\") The configuration would be: { \"actions\" : [ { \"type\" : \"generic_key\", \"descriptor_value\" : \"some_value\" }, { \"type\" : \"source_cluster\" } ] } Example 2 If an action doesn’t append a descriptor entry, no descriptor is generated for the configuration. For the following configuration: { \"actions\" : [ { \"type\" : \"generic_key\", \"descriptor_value\" : \"some_value\" }, { \"type\" : \"remote_address\" }, { \"type\" : \"souce_cluster\" } ] } If a request did not set x-forwarded-for, no descriptor is generated. If a request sets x-forwarded-for, the the following descriptor is generated: (\"generic_key\", \"some_value\") (\"remote_address\", \"\") (\"source_cluster\", \"from_cluster\") Statistics The buffer filter outputs statistics in the cluster..ratelimit. namespace. 429 responses are emitted to the normal cluster dynamic HTTP statistics. Name Type Description ok Counter Total under limit responses from the rate limit service error Counter Total errors contacting the rate limit service over_limit Counter total over limit responses from the rate limit service Runtime The HTTP rate limit filter supports the following runtime settings: ratelimit.http_filter_enabled % of requests that will call the rate limit service. Defaults to 100. ratelimit.http_filter_enforcing % of requests that will call the rate limit service and enforce the decision. Defaults to 100. This can be used to test what would happen before fully enforcing the outcome. ratelimit..http_filter_enabled % of requests that will call the rate limit service for a given route_key specified in the rate limit configuration. Defaults to 100. "},"configuration/http_filters/router_filter.html":{"url":"configuration/http_filters/router_filter.html","title":"路由","keywords":"","body":"路由 The router filter implements HTTP forwarding. It will be used in almost all HTTP proxy scenarios that Envoy is deployed for. The filter’s main job is to follow the instructions specified in the configured route table. In addition to forwarding and redirection, the filter also handles retry, statistics, etc. v1 API reference v2 API reference HTTP headers The router consumes and sets various HTTP headers both on the egress/request path as well as on the ingress/response path. They are documented in this section. x-envoy-expected-rq-timeout-ms x-envoy-max-retries x-envoy-retry-on x-envoy-retry-grpc-on x-envoy-upstream-alt-stat-name x-envoy-upstream-canary x-envoy-upstream-rq-timeout-alt-response x-envoy-upstream-rq-timeout-ms x-envoy-upstream-rq-per-try-timeout-ms x-envoy-upstream-service-time x-envoy-original-path x-envoy-immediate-health-check-fail x-envoy-overloaded x-envoy-decorator-operation x-envoy-expected-rq-timeout-ms This is the time in milliseconds the router expects the request to be completed. Envoy sets this header so that the upstream host receiving the request can make decisions based on the request timeout, e.g., early exit. This is set on internal requests and is either taken from the x-envoy-upstream-rq-timeout-ms header or the route timeout, in that order. x-envoy-max-retries If a retry policy is in place, Envoy will default to retrying one time unless explicitly specified. The number of retries can be explicitly set in the route retry config or by using this header. If a retry policy is not configured and x-envoy-retry-on or x-envoy-retry-grpc-on headers are not specified, Envoy will not retry a failed request. A few notes on how Envoy does retries: The route timeout (set via x-envoy-upstream-rq-timeout-ms or the route configuration) includesall retries. Thus if the request timeout is set to 3s, and the first request attempt takes 2.7s, the retry (including backoff) has .3s to complete. This is by design to avoid an exponential retry/timeout explosion. Envoy uses a fully jittered exponential backoff algorithm for retries with a base time of 25ms. The first retry will be delayed randomly between 0-24ms, the 2nd between 0-74ms, the 3rd between 0-174ms and so on. If max retries is set both by header as well as in the route configuration, the maximum value is taken when determining the max retries to use for the request. x-envoy-retry-on Setting this header on egress requests will cause Envoy to attempt to retry failed requests (number of retries defaults to 1 and can be controlled by x-envoy-max-retries header or the route config retry policy). The value to which the x-envoy-retry-on header is set indicates the retry policy. One or more policies can be specified using a ‘,’ delimited list. The supported policies are: 5xx Envoy will attempt a retry if the upstream server responds with any 5xx response code, or does not respond at all (disconnect/reset/read timeout). (Includes connect-failure and refused-stream)NOTE: Envoy will not retry when a request exceeds x-envoy-upstream-rq-timeout-ms(resulting in a 504 error code). Use x-envoy-upstream-rq-per-try-timeout-ms if you want to retry when individual attempts take too long. x-envoy-upstream-rq-timeout-ms is an outer time limit for a request, including any retries that take place. gateway-error This policy is similar to the 5xx policy but will only retry requests that result in a 502, 503, or 504. connect-failure Envoy will attempt a retry if a request is failed because of a connection failure to the upstream server (connect timeout, etc.). (Included in 5xx)NOTE: A connection failure/timeout is a the TCP level, not the request level. This does not include upstream request timeouts specified via x-envoy-upstream-rq-timeout-ms or via route configuration. retriable-4xx Envoy will attempt a retry if the upstream server responds with a retriable 4xx response code. Currently, the only response code in this category is 409.NOTE: Be careful turning on this retry type. There are certain cases where a 409 can indicate that an optimistic locking revision needs to be updated. Thus, the caller should not retry and needs to read then attempt another write. If a retry happens in this type of case it will always fail with another 409. refused-stream Envoy will attempt a retry if the upstream server resets the stream with a REFUSED_STREAM error code. This reset type indicates that a request is safe to retry. (Included in 5xx) The number of retries can be controlled via the x-envoy-max-retries header or via the route configuration. Note that retry policies can also be applied at the route level. By default, Envoy will not perform retries unless you’ve configured them per above. x-envoy-retry-grpc-on Setting this header on egress requests will cause Envoy to attempt to retry failed requests (number of retries defaults to 1, and can be controlled by x-envoy-max-retries header or the route config retry policy). gRPC retries are currently only supported for gRPC status codes in response headers. gRPC status codes in trailers will not trigger retry logic. One or more policies can be specified using a ‘,’ delimited list. The supported policies are: cancelled Envoy will attempt a retry if the gRPC status code in the response headers is “cancelled” (1) deadline-exceeded Envoy will attempt a retry if the gRPC status code in the response headers is “deadline-exceeded” (4) resource-exhausted Envoy will attempt a retry if the gRPC status code in the response headers is “resource-exhausted” (8) As with the x-envoy-retry-grpc-on header, the number of retries can be controlled via the x-envoy-max-retries header Note that retry policies can also be applied at the route level. By default, Envoy will not perform retries unless you’ve configured them per above. x-envoy-upstream-alt-stat-name Setting this header on egress requests will cause Envoy to emit upstream response code/timing statistics to a dual stat tree. This can be useful for application level categories that Envoy doesn’t know about. The output tree is documented here. This should not be confused with alt_stat_name which is specified while defining the cluster and when provided specifies an alternative name for the cluster at the root of the statistic tree. x-envoy-upstream-canary If an upstream host sets this header, the router will use it to generate canary specific statistics. The output tree is documented here. x-envoy-upstream-rq-timeout-alt-response Setting this header on egress requests will cause Envoy to set a 204 response code (instead of 504) in the event of a request timeout. The actual value of the header is ignored; only its presence is considered. See also x-envoy-upstream-rq-timeout-ms. x-envoy-upstream-rq-timeout-ms Setting this header on egress requests will cause Envoy to override the route configuration. The timeout must be specified in millisecond units. See also x-envoy-upstream-rq-per-try-timeout-ms. x-envoy-upstream-rq-per-try-timeout-ms Setting this header on egress requests will cause Envoy to set a per try timeout on routed requests. This timeout must be x-envoy-upstream-rq-timeout-ms) or it is ignored. This allows a caller to set a tight per try timeout to allow for retries while maintaining a reasonable overall timeout. x-envoy-upstream-service-time Contains the time in milliseconds spent by the upstream host processing the request. This is useful if the client wants to determine service time compared to network latency. This header is set on responses. x-envoy-original-path If the route utilizes prefix_rewrite, Envoy will put the original path header in this header. This can be useful for logging and debugging. x-envoy-immediate-health-check-fail If the upstream host returns this header (set to any value), Envoy will immediately assume the upstream host has failed active health checking (if the cluster has been configured for active health checking). This can be used to fast fail an upstream host via standard data plane processing without waiting for the next health check interval. The host can become healthy again via standard active health checks. See the health checking overview for more information. x-envoy-overloaded If this header is set by upstream, Envoy will not retry. Currently the value of the header is not looked at, only its presence. Additionally, Envoy will set this header on the downstream response if a request was dropped due to either maintenance mode or upstream circuit breaking. x-envoy-decorator-operation If this header is present on ingress requests, its value will override any locally defined operation (span) name on the server span generated by the tracing mechanism. Similarly, if this header is present on an egress response, its value will override any locally defined operation (span) name on the client span. Statistics The router outputs many statistics in the cluster namespace (depending on the cluster specified in the chosen route). See here for more information. The router filter outputs statistics in the http.. namespace. The stat prefix comes from the owning HTTP connection manager. Name Type Description no_route Counter Total requests that had no route and resulted in a 404 no_cluster Counter Total requests in which the target cluster did not exist and resulted in a 404 rq_redirect Counter Total requests that resulted in a redirect response rq_direct_response Counter Total requests that resulted in a direct response rq_total Counter Total routed requests Virtual cluster statistics are output in the vhost..vcluster.. namespace and include the following statistics: Name Type Description upstreamrq Counter Aggregate HTTP response codes (e.g., 2xx, 3xx, etc.) upstreamrq Counter Specific HTTP response codes (e.g., 201, 302, etc.) upstream_rq_time Histogram Request time milliseconds Runtime The router filter supports the following runtime settings: upstream.base_retry_backoff_ms Base exponential retry back off time. See here for more information. Defaults to 25ms. upstream.maintenance_mode. % of requests that will result in an immediate 503 response. This overrides any routing behavior for requests that would have been destined for . This can be used for load shedding, failure injection, etc. Defaults to disabled. upstream.use_retry % of requests that are eligible for retry. This configuration is checked before any other retry configuration and can be used to fully disable retries across all Envoys if needed. "},"configuration/http_filters/squash_filter.html":{"url":"configuration/http_filters/squash_filter.html","title":"Squash","keywords":"","body":"Squash Squash is an HTTP filter which enables Envoy to integrate with Squash microservices debugger. Code: https://github.com/solo-io/squash, API Docs: https://squash.solo.io/ Overview The main use case for this filter is in a service mesh, where Envoy is deployed as a sidecar. Once a request marked for debugging enters the mesh, the Squash Envoy filter reports its ‘location’ in the cluster to the Squash server - as there is a 1-1 mapping between Envoy sidecars and application containers, the Squash server can find and attach a debugger to the application container. The Squash filter also holds the request until a debugger is attached (or a timeout occurs). This enables developers (via Squash) to attach a native debugger to the container that will handle the request, before the request arrive to the application code, without any changes to the cluster. Configuration v1 API reference v2 API reference How it works When the Squash filter encounters a request containing the header ‘x-squash-debug’ it will: Delay the incoming request. Contact the Squash server and request the creation of a DebugAttachment On the Squash server side, Squash will attempt to attach a debugger to the application Envoy proxies to. On success, it changes the state of the DebugAttachment to attached. Wait until the Squash server updates the DebugAttachment object’s state to attached (or error state) Resume the incoming request "},"configuration/cluster_manager/cluster_stats.html":{"url":"configuration/cluster_manager/cluster_stats.html","title":"统计","keywords":"","body":"统计 General Health check statistics Outlier detection statistics Dynamic HTTP statistics Alternate tree dynamic HTTP statistics Per service zone dynamic HTTP statistics Load balancer statistics Load balancer subset statistics General The cluster manager has a statistics tree rooted at cluster_manager. with the following statistics. Any : character in the stats name is replaced with _. Name Type Description cluster_added Counter Total clusters added (either via static config or CDS) cluster_modified Counter Total clusters modified (via CDS) cluster_removed Counter Total clusters removed (via CDS) active_clusters Gauge Number of currently active (warmed) clusters warming_clusters Gauge Number of currently warming (not active) clusters Every cluster has a statistics tree rooted at cluster.. with the following statistics: Name Type Description upstream_cx_total Counter Total connections upstream_cx_active Gauge Total active connections upstream_cx_http1_total Counter Total HTTP/1.1 connections upstream_cx_http2_total Counter Total HTTP/2 connections upstream_cx_connect_fail Counter Total connection failures upstream_cx_connect_timeout Counter Total connection connect timeouts upstream_cx_idle_timeout Counter Total connection idle timeouts upstream_cx_connect_attempts_exceeded Counter Total consecutive connection failures exceeding configured connection attempts upstream_cx_overflow Counter Total times that the cluster’s connection circuit breaker overflowed upstream_cx_connect_ms Histogram Connection establishment milliseconds upstream_cx_length_ms Histogram Connection length milliseconds upstream_cx_destroy Counter Total destroyed connections upstream_cx_destroy_local Counter Total connections destroyed locally upstream_cx_destroy_remote Counter Total connections destroyed remotely upstream_cx_destroy_with_active_rq Counter Total connections destroyed with 1+ active request upstream_cx_destroy_local_with_active_rq Counter Total connections destroyed locally with 1+ active request upstream_cx_destroy_remote_with_active_rq Counter Total connections destroyed remotely with 1+ active request upstream_cx_close_notify Counter Total connections closed via HTTP/1.1 connection close header or HTTP/2 GOAWAY upstream_cx_rx_bytes_total Counter Total received connection bytes upstream_cx_rx_bytes_buffered Gauge Received connection bytes currently buffered upstream_cx_tx_bytes_total Counter Total sent connection bytes upstream_cx_tx_bytes_buffered Gauge Send connection bytes currently buffered upstream_cx_protocol_error Counter Total connection protocol errors upstream_cx_max_requests Counter Total connections closed due to maximum requests upstream_cx_none_healthy Counter Total times connection not established due to no healthy hosts upstream_rq_total Counter Total requests upstream_rq_active Gauge Total active requests upstream_rq_pending_total Counter Total requests pending a connection pool connection upstream_rq_pending_overflow Counter Total requests that overflowed connection pool circuit breaking and were failed upstream_rq_pending_failure_eject Counter Total requests that were failed due to a connection pool connection failure upstream_rq_pending_active Gauge Total active requests pending a connection pool connection upstream_rq_cancelled Counter Total requests cancelled before obtaining a connection pool connection upstream_rq_maintenance_mode Counter Total requests that resulted in an immediate 503 due to maintenance mode upstream_rq_timeout Counter Total requests that timed out waiting for a response upstream_rq_per_try_timeout Counter Total requests that hit the per try timeout upstream_rq_rx_reset Counter Total requests that were reset remotely upstream_rq_tx_reset Counter Total requests that were reset locally upstream_rq_retry Counter Total request retries upstream_rq_retry_success Counter Total request retry successes upstream_rq_retry_overflow Counter Total requests not retried due to circuit breaking upstream_flow_control_paused_reading_total Counter Total number of times flow control paused reading from upstream upstream_flow_control_resumed_reading_total Counter Total number of times flow control resumed reading from upstream upstream_flow_control_backed_up_total Counter Total number of times the upstream connection backed up and paused reads from downstream upstream_flow_control_drained_total Counter Total number of times the upstream connection drained and resumed reads from downstream membership_change Counter Total cluster membership changes membership_healthy Gauge Current cluster healthy total (inclusive of both health checking and outlier detection) membership_total Gauge Current cluster membership total retry_or_shadow_abandoned Counter Total number of times shadowing or retry buffering was canceled due to buffer limits config_reload Counter Total API fetches that resulted in a config reload due to a different config update_attempt Counter Total cluster membership update attempts update_success Counter Total cluster membership update successes update_failure Counter Total cluster membership update failures update_empty Counter Total cluster membership updates ending with empty cluster load assignment and continuing with previous config update_no_rebuild Counter Total successful cluster membership updates that didn’t result in any cluster load balancing structure rebuilds version Gauge Hash of the contents from the last successful API fetch max_host_weight Gauge Maximum weight of any host in the cluster bind_errors Counter Total errors binding the socket to the configured source address Health check statistics If health check is configured, the cluster has an additional statistics tree rooted at cluster..health_check. with the following statistics: Name Type Description attempt Counter Number of health checks success Counter Number of successful health checks failure Counter Number of immediately failed health checks (e.g. HTTP 503) as well as network failures passive_failure Counter Number of health check failures due to passive events (e.g. x-envoy-immediate-health-check-fail) network_failure Counter Number of health check failures due to network error verify_cluster Counter Number of health checks that attempted cluster name verification healthy Gauge Number of healthy members Outlier detection statistics If outlier detection is configured for a cluster, statistics will be rooted at cluster..outlier_detection. and contain the following: Name Type Description ejections_enforced_total Counter Number of enforced ejections due to any outlier type ejections_active Gauge Number of currently ejected hosts ejections_overflow Counter Number of ejections aborted due to the max ejection % ejections_enforced_consecutive_5xx Counter Number of enforced consecutive 5xx ejections ejections_detected_consecutive_5xx Counter Number of detected consecutive 5xx ejections (even if unenforced) ejections_enforced_success_rate Counter Number of enforced success rate outlier ejections ejections_detected_success_rate Counter Number of detected success rate outlier ejections (even if unenforced) ejections_enforced_consecutive_gateway_failure Counter Number of enforced consecutive gateway failure ejections ejections_detected_consecutive_gateway_failure Counter Number of detected consecutive gateway failure ejections (even if unenforced) ejections_total Counter Deprecated. Number of ejections due to any outlier type (even if unenforced) ejections_consecutive_5xx Counter Deprecated. Number of consecutive 5xx ejections (even if unenforced) Dynamic HTTP statistics If HTTP is used, dynamic HTTP response code statistics are also available. These are emitted by various internal systems as well as some filters such as the router filter and rate limit filter. They are rooted at cluster.. and contain the following statistics: Name Type Description upstreamrq Counter Aggregate HTTP response codes (e.g., 2xx, 3xx, etc.) upstreamrq Counter Specific HTTP response codes (e.g., 201, 302, etc.) upstream_rq_time Histogram Request time milliseconds canary.upstreamrq Counter Upstream canary aggregate HTTP response codes canary.upstreamrq Counter Upstream canary specific HTTP response codes canary.upstream_rq_time Histogram Upstream canary request time milliseconds internal.upstreamrq Counter Internal origin aggregate HTTP response codes internal.upstreamrq Counter Internal origin specific HTTP response codes internal.upstream_rq_time Histogram Internal origin request time milliseconds external.upstreamrq Counter External origin aggregate HTTP response codes external.upstreamrq Counter External origin specific HTTP response codes external.upstream_rq_time Histogram External origin request time milliseconds Alternate tree dynamic HTTP statistics If alternate tree statistics are configured, they will be present in the cluster...namespace. The statistics produced are the same as documented in the dynamic HTTP statistics section above. Per service zone dynamic HTTP statistics If the service zone is available for the local service (via --service-zone) and the upstream cluster, Envoy will track the following statistics in cluster..zone...namespace. Name Type Description upstreamrq Counter Aggregate HTTP response codes (e.g., 2xx, 3xx, etc.) upstreamrq Counter Specific HTTP response codes (e.g., 201, 302, etc.) upstream_rq_time Histogram Request time milliseconds Load balancer statistics Statistics for monitoring load balancer decisions. Stats are rooted at cluster.. and contain the following statistics: Name Type Description lb_recalculate_zone_structures Counter The number of times locality aware routing structures are regenerated for fast decisions on upstream locality selection lb_healthy_panic Counter Total requests load balanced with the load balancer in panic mode lb_zone_cluster_too_small Counter No zone aware routing because of small upstream cluster size lb_zone_routing_all_directly Counter Sending all requests directly to the same zone lb_zone_routing_sampled Counter Sending some requests to the same zone lb_zone_routing_cross_zone Counter Zone aware routing mode but have to send cross zone lb_local_cluster_not_ok Counter Local host set is not set or it is panic mode for local cluster lb_zone_number_differs Counter Number of zones in local and upstream cluster different lb_zone_no_capacity_left Counter Total number of times ended with random zone selection due to rounding error Load balancer subset statistics Statistics for monitoring load balancer subset decisions. Stats are rooted at cluster.. and contain the following statistics: Name Type Description lb_subsets_active Gauge Number of currently available subsets lb_subsets_created Counter Number of subsets created lb_subsets_removed Counter Number of subsets removed due to no hosts lb_subsets_selected Counter Number of times any subset was selected for load balancing lb_subsets_fallback Counter Number of times the fallback policy was invoked "},"configuration/cluster_manager/cluster_runtime.html":{"url":"configuration/cluster_manager/cluster_runtime.html","title":"运行时","keywords":"","body":"运行时 Upstream clusters support the following runtime settings: Active health checking health_check.min_interval Min value for the health checking interval. Default value is 0. The health checking interval will be between min_interval and max_interval. health_check.max_interval Max value for the health checking interval. Default value is MAX_INT. The health checking interval will be between min_interval and max_interval. health_check.verify_cluster What % of health check requests will be verified against the expected upstream service as the health check filter will write the remote service cluster into the response. Outlier detection See the outlier detection architecture overview for more information on outlier detection. The runtime parameters supported by outlier detection are the same as the static configuration parameters, namely: outlier_detection.consecutive_5xx consecutive_5XX setting in outlier detection outlier_detection.consecutive_gateway_failure consecutive_gateway_failure setting in outlier detection outlier_detection.interval_ms interval_ms setting in outlier detection outlier_detection.base_ejection_time_ms base_ejection_time_ms setting in outlier detection outlier_detection.max_ejection_percent max_ejection_percent setting in outlier detection outlier_detection.enforcing_consecutive_5xx enforcing_consecutive_5xx setting in outlier detection outlier_detection.enforcing_consecutive_gateway_failure enforcing_consecutive_gateway_failure setting in outlier detection outlier_detection.enforcing_success_rate enforcing_success_rate setting in outlier detection outlier_detection.success_rate_minimum_hosts success_rate_minimum_hosts setting in outlier detection outlier_detection.success_rate_request_volume success_rate_request_volume setting in outlier detection outlier_detection.success_rate_stdev_factor success_rate_stdev_factor setting in outlier detection Core upstream.healthy_panic_threshold Sets the panic threshold percentage. Defaults to 50%. upstream.use_http2 Whether the cluster utilizes the http2 feature if configured. Set to 0 to disable HTTP/2 even if the feature is configured. Defaults to enabled. upstream.weight_enabled Binary switch to turn on or off weighted load balancing. If set to non 0, weighted load balancing is enabled. Defaults to enabled. Zone aware load balancing upstream.zone_routing.enabled % of requests that will be routed to the same upstream zone. Defaults to 100% of requests. upstream.zone_routing.min_cluster_size Minimal size of the upstream cluster for which zone aware routing can be attempted. Default value is 6. If the upstream cluster size is smaller than min_cluster_size zone aware routing will not be performed. Circuit breaking circuit_breakers...max_connections Max connections circuit breaker setting circuit_breakers...max_pending_requests Max pending requests circuit breaker setting circuit_breakers...max_requests Max requests circuit breaker setting circuit_breakers...max_retries Max retries circuit breaker setting "},"configuration/cluster_manager/cds.html":{"url":"configuration/cluster_manager/cds.html","title":"集群发现服务（CDS）","keywords":"","body":"集群发现服务（CDS） The cluster discovery service (CDS) is an optional API that Envoy will call to dynamically fetch cluster manager members. Envoy will reconcile the API response and add, modify, or remove known clusters depending on what is required. Note Any clusters that are statically defined within the Envoy configuration cannot be modified or removed via the CDS API. v1 CDS API v2 CDS API Statistics CDS has a statistics tree rooted at cluster_manager.cds. with the following statistics: Name Type Description config_reload Counter Total API fetches that resulted in a config reload due to a different config update_attempt Counter Total API fetches attempted update_success Counter Total API fetches completed successfully update_failure Counter Total API fetches that failed because of schema errors version Gauge Hash of the contents from the last successful API fetch "},"configuration/cluster_manager/cluster_hc.html":{"url":"configuration/cluster_manager/cluster_hc.html","title":"健康检查","keywords":"","body":"健康检查 Health checking architecture overview. If health checking is configured for a cluster, additional statistics are emitted. They are documented here. v1 API documentation. v2 API documentation. TCP health checking Attention This section is written for the v1 API but the concepts also apply to the v2 API. It will be rewritten to target the v2 API in a future release. The type of matching performed is the following (this is the MongoDB health check request and response): { \"send\": [ {\"binary\": \"39000000\"}, {\"binary\": \"EEEEEEEE\"}, {\"binary\": \"00000000\"}, {\"binary\": \"d4070000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"746573742e\"}, {\"binary\": \"24636d6400\"}, {\"binary\": \"00000000\"}, {\"binary\": \"FFFFFFFF\"}, {\"binary\": \"13000000\"}, {\"binary\": \"01\"}, {\"binary\": \"70696e6700\"}, {\"binary\": \"000000000000f03f\"}, {\"binary\": \"00\"} ], \"receive\": [ {\"binary\": \"EEEEEEEE\"}, {\"binary\": \"01000000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"0000000000000000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"11000000\"}, {\"binary\": \"01\"}, {\"binary\": \"6f6b\"}, {\"binary\": \"00000000000000f03f\"}, {\"binary\": \"00\"} ] } During each health check cycle, all of the “send” bytes are sent to the target server. Each binary block can be of arbitrary length and is just concatenated together when sent. (Separating into multiple blocks can be useful for readability). When checking the response, “fuzzy” matching is performed such that each binary block must be found, and in the order specified, but not necessarily contiguous. Thus, in the example above, “FFFFFFFF” could be inserted in the response between “EEEEEEEE” and “01000000” and the check would still pass. This is done to support protocols that insert non-deterministic data, such as time, into the response. Health checks that require a more complex pattern such as send/receive/send/receive are not currently possible. If “receive” is an empty array, Envoy will perform “connect only” TCP health checking. During each cycle, Envoy will attempt to connect to the upstream host, and consider it a success if the connection succeeds. A new connection is created for each health check cycle. "},"configuration/cluster_manager/cluster_circuit_breakers.html":{"url":"configuration/cluster_manager/cluster_circuit_breakers.html","title":"断路","keywords":"","body":"断路 Circuit Breaking architecture overview. v1 API documentation. v2 API documentation. Runtime All circuit breaking settings are runtime configurable for all defined priorities based on cluster name. They follow the following naming scheme circuit_breakers.... cluster_name is the name field in each cluster’s configuration, which is set in the envoy config file. Available runtime settings will override settings set in the envoy config file. "},"configuration/health_checkers/redis.html":{"url":"configuration/health_checkers/redis.html","title":"Redis","keywords":"","body":"Redis The Redis health checker is a custom health checker which checks Redis upstream hosts. It sends a Redis PING command and expect a PONG response. The upstream Redis server can respond with anything other than PONG to cause an immediate active health check failure. Optionally, Envoy can perform EXISTS on a user-specified key. If the key does not exist it is considered a passing healthcheck. This allows the user to mark a Redis instance for maintenance by setting the specified key to any value and waiting for traffic to drain. v2 API reference "},"configuration/access_log.html":{"url":"configuration/access_log.html","title":"访问记录","keywords":"","body":"访问记录 Configuration Access logs are configured as part of the HTTP connection manager config or TCP Proxy. v1 API reference v2 API reference Format rules The access log format string contains either command operators or other characters interpreted as a plain string. The access log formatter does not make any assumptions about a new line separator, so one has to specified as part of the format string. See the default format for an example. Note that the access log line will contain a ‘-‘ character for every not set/empty value. The same format strings are used by different types of access logs (such as HTTP and TCP). Some fields may have slightly different meanings, depending on what type of log it is. Differences are noted. The following command operators are supported: %START_TIME% HTTPRequest start time including milliseconds.TCPDownstream connection start time including milliseconds.START_TIME can be customized using a format string, for example: %START_TIME(%Y/%m/%dT%H:%M:%S%z %s)% %BYTES_RECEIVED% HTTPBody bytes received.TCPDownstream bytes received on connection. %PROTOCOL% HTTPProtocol. Currently either HTTP/1.1 or HTTP/2.TCPNot implemented (“-“). %RESPONSE_CODE% HTTPHTTP response code. Note that a response code of ‘0’ means that the server never sent the beginning of a response. This generally means that the (downstream) client disconnected.TCPNot implemented (“-“). %BYTES_SENT% HTTPBody bytes sent.TCPDownstream bytes sent on connection. %DURATION% HTTPTotal duration in milliseconds of the request from the start time to the last byte out.TCPTotal duration in milliseconds of the downstream connection. %RESPONSE_FLAGS% Additional details about the response or connection, if any. For TCP connections, the response codes mentioned in the descriptions do not apply. Possible values are:HTTP and TCPUH: No healthy upstream hosts in upstream cluster in addition to 503 response code.UF: Upstream connection failure in addition to 503 response code.UO: Upstream overflow (circuit breaking) in addition to 503 response code.NR: No route configured for a given request in addition to 404 response code.HTTP onlyLH: Local service failed health check request in addition to 503 response code.UT: Upstream request timeout in addition to 504 response code.LR: Connection local reset in addition to 503 response code.UR: Upstream remote reset in addition to 503 response code.UC: Upstream connection termination in addition to 503 response code.DI: The request processing was delayed for a period specified via fault injection.FI: The request was aborted with a response code specified via fault injection.RL: The request was ratelimited locally by the HTTP rate limit filter in addition to 429 response code. %UPSTREAM_HOST% Upstream host URL (e.g., tcp://ip:port for TCP connections). %UPSTREAM_CLUSTER% Upstream cluster to which the upstream host belongs to. %UPSTREAM_LOCAL_ADDRESS% Local address of the upstream connection. If the address is an IP address it includes both address and port. %DOWNSTREAM_REMOTE_ADDRESS% Remote address of the downstream connection. If the address is an IP address it includes both address and port.NoteThis may not be the physical remote address of the peer if the address has been inferred from proxy proto or x-forwarded-for. %DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT% Remote address of the downstream connection. If the address is an IP address the output doesnot include port.NoteThis may not be the physical remote address of the peer if the address has been inferred from proxy proto or x-forwarded-for. %DOWNSTREAM_LOCAL_ADDRESS% Local address of the downstream connection. If the address is an IP address it includes both address and port. If the original connection was redirected by iptables REDIRECT, this represents the original destination address restored by the Original Destination Filter using SO_ORIGINAL_DST socket option. If the original connection was redirected by iptables TPROXY, and the listener’s transparent option was set to true, this represents the original destination address and port. %DOWNSTREAM_LOCAL_ADDRESS_WITHOUT_PORT% Same as %DOWNSTREAM_LOCAL_ADDRESS% excluding port if the address is an IP address. %REQ(X?Y):Z% HTTPAn HTTP request header where X is the main HTTP header, Y is the alternative one, and Z is an optional parameter denoting string truncation up to Z characters long. The value is taken from the HTTP request header named X first and if it’s not set, then request header Y is used. If none of the headers are present ‘-‘ symbol will be in the log.TCPNot implemented (“-“). %RESP(X?Y):Z% HTTPSame as %REQ(X?Y):Z% but taken from HTTP response headers.TCPNot implemented (“-“). %TRAILER(X?Y):Z% HTTPSame as %REQ(X?Y):Z% but taken from HTTP response trailers.TCPNot implemented (“-“). %DYNAMIC_METADATA(NAMESPACE:KEY*):Z% HTTPDynamic Metadata info, where NAMESPACE is the the filter namespace used when setting the metadata, KEY is an optional lookup up key in the namespace with the option of specifying nested keys separated by ‘:’, and Z is an optional parameter denoting string truncation up to Z characters long. Dynamic Metadata can be set by filters using the RequestInfo API: setDynamicMetadata. The data will be logged as a JSON string. For example, for the following dynamic metadata:com.test.my_filter: {\"test_key\": \"foo\", \"test_object\": {\"inner_key\": \"bar\"}}%DYNAMIC_METADATA(com.test.my_filter)% will log: {\"test_key\": \"foo\", \"test_object\": {\"inner_key\": \"bar\"}}%DYNAMIC_METADATA(com.test.my_filter:test_key)% will log: \"foo\"%DYNAMIC_METADATA(com.test.my_filter:test_object)% will log: {\"inner_key\": \"bar\"}%DYNAMIC_METADATA(com.test.my_filter:test_object:inner_key)% will log: \"bar\"%DYNAMIC_METADATA(com.unknown_filter)% will log: -%DYNAMIC_METADATA(com.test.my_filter:unknown_key)% will log: -%DYNAMIC_METADATA(com.test.my_filter):25% will log (truncation at 25 characters): {\"test_key\": \"foo\", \"testTCPNot implemented (“-“). Default format If custom format is not specified, Envoy uses the following default format: [%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\\n Example of the default Envoy access log format: [2016-04-15T20:17:00.310Z] \"POST /api/v1/locations HTTP/2\" 204 - 154 0 226 100 \"10.0.35.28\" \"nsq2http\" \"cc21d9b0-cf5c-432b-8c7e-98aeb7988cd2\" \"locations\" \"tcp://10.0.2.1:80\" "},"configuration/rate_limit.html":{"url":"configuration/rate_limit.html","title":"速率限制服务","keywords":"","body":"速率限制服务 The rate limit service configuration specifies the global rate limit service Envoy should talk to when it needs to make global rate limit decisions. If no rate limit service is configured, a “null” service will be used which will always return OK if called. v1 API reference v2 API reference gRPC service IDL Envoy expects the rate limit service to support the gRPC IDL specified in/source/common/ratelimit/ratelimit.proto. See the IDL documentation for more information on how the API works. See Lyft’s reference implementation here. "},"configuration/runtime.html":{"url":"configuration/runtime.html","title":"运行时","keywords":"","body":"运行时 The runtime configuration specifies the location of the local file system tree that contains re-loadable configuration elements. Values can be viewed at the /runtime admin endpoint. Values can be modified and added at the /runtime_modify admin endpoint. If runtime is not configured, an empty provider is used which has the effect of using all defaults built into the code, except for any values added via /runtime_modify. Attention Use the /runtime_modify endpoint with care. Changes are effectively immediately. It is criticalthat the admin interface is properly secured. v1 API reference v2 API reference File system layout Various sections of the configuration guide describe the runtime settings that are available. For example, here are the runtime settings for upstream clusters. Assume that the folder /srv/runtime/v1 points to the actual file system path where global runtime configurations are stored. The following would be a typical configuration setting for runtime: symlink_root: /srv/runtime/current subdirectory: envoy override_subdirectory: envoy_override Where /srv/runtime/current is a symbolic link to /srv/runtime/v1. Each ‘.’ in a runtime key indicates a new directory in the hierarchy, rooted at symlink_root +subdirectory. For example, the health_check.min_interval key would have the following full file system path (using the symbolic link): /srv/runtime/current/envoy/health_check/min_interval The terminal portion of a path is the file. The contents of the file constitute the runtime value. When reading numeric values from a file, spaces and new lines will be ignored. The override_subdirectory is used along with the --service-cluster CLI option. Assume that --service-cluster has been set to my-cluster. Envoy will first look for thehealth_check.min_interval key in the following full file system path: /srv/runtime/current/envoy_override/my-cluster/health_check/min_interval If found, the value will override any value found in the primary lookup path. This allows the user to customize the runtime values for individual clusters on top of global defaults. Comments Lines starting with # as the first character are treated as comments. Comments can be used to provide context on an existing value. Comments are also useful in an otherwise empty file to keep a placeholder for deployment in a time of need. Updating runtime values via symbolic link swap There are two steps to update any runtime value. First, create a hard copy of the entire runtime tree and update the desired runtime values. Second, atomically swap the symbolic link root from the old tree to the new runtime tree, using the equivalent of the following command: /srv/runtime:~$ ln -s /srv/runtime/v2 new && mv -Tf new current It’s beyond the scope of this document how the file system data is deployed, garbage collected, etc. Statistics The file system runtime provider emits some statistics in the runtime. namespace. Name Type Description load_error Counter Total number of load attempts that resulted in an error override_dir_not_exists Counter Total number of loads that did not use an override directory override_dir_exists Counter Total number of loads that did use an override directory load_success Counter Total number of load attempts that were successful num_keys Gauge Number of keys currently loaded "},"configuration/statistics.html":{"url":"configuration/statistics.html","title":"统计","keywords":"","body":"统计 A few statistics are emitted to report statistics system behavior: Name Type Description stats.overflow Counter Total number of times Envoy cannot allocate a statistic due to a shortage of shared memory Server Server related statistics are rooted at server. with following statistics: Name Type Description uptime Gauge Current server uptime in seconds memory_allocated Gauge Current amount of allocated memory in bytes memory_heap_size Gauge Current reserved heap size in bytes live Gauge 1 if the server is not currently draining, 0 otherwise parent_connections Gauge Total connections of the old Envoy process on hot restart total_connections Gauge Total connections of both new and old Envoy processes version Gauge Integer represented version number based on SCM revision days_until_first_cert_expiring Gauge Number of days until the next certificate being managed will expire File system Statistics related to file system are emitted in the filesystem. namespace. Name Type Description write_buffered Counter Total number of times file data is moved to Envoy’s internal flush buffer write_completed Counter Total number of times a file was written flushed_by_timer Counter Total number of times internal flush buffers are written to a file due to flush timeout reopen_failed Counter Total number of times a file was failed to be opened write_total_buffered Gauge Current total size of internal flush buffer in bytes "},"configuration/tools/router_check.html":{"url":"configuration/tools/router_check.html","title":"路由表检查工具","keywords":"","body":"路由表检查工具 NOTE: The following configuration is for the route table check tool only and is not part of the Envoy binary. The route table check tool is a standalone binary that can be used to verify Envoy’s routing for a given configuration file. The following specifies input to the route table check tool. The route table check tool checks if the route returned by a router matches what is expected. The tool can be used to check cluster name, virtual cluster name, virtual host name, manual path rewrite, manual host rewrite, path redirect, and header field matches. Extensions for other test cases can be added. Details about installing the tool and sample tool input/output can be found at installation. The route table check tool config is composed of an array of json test objects. Each test object is composed of three parts. Test name This field specifies the name of each test object. Input values The input value fields specify the parameters to be passed to the router. Example input fields include the :authority, :path, and :method header fields. The :authority and :path fields specify the url sent to the router and are required. All other input fields are optional. Validate The validate fields specify the expected values and test cases to check. At least one test case is required. A simple tool configuration json has one test case and is written as follows. The test expects a cluster name match of “instant-server”.: [ { \"test_name: \"Cluster_name_test\", \"input\": { \":authority\":\"api.lyft.com\", \":path\": \"/api/locations\" }, \"validate\": { \"cluster_name\": \"instant-server\" } } ] [ { \"test_name\": \"...\", \"input\": { \":authority\": \"...\", \":path\": \"...\", \":method\": \"...\", \"internal\" : \"...\", \"random_value\" : \"...\", \"ssl\" : \"...\", \"additional_headers\": [ { \"field\": \"...\", \"value\": \"...\" }, { \"...\" } ] }, \"validate\": { \"cluster_name\": \"...\", \"virtual_cluster_name\": \"...\", \"virtual_host_name\": \"...\", \"host_rewrite\": \"...\", \"path_rewrite\": \"...\", \"path_redirect\": \"...\", \"header_fields\" : [ { \"field\": \"...\", \"value\": \"...\" }, { \"...\" } ] } }, { \"...\" } ] test_name (required, string) The name of a test object. input (required, object) Input values sent to the router that determine the returned route.:authority(required, string) The url authority. This value along with the path parameter define the url to be matched. An example authority value is “api.lyft.com”.:path(required, string) The url path. An example path value is “/foo”.:method(optional, string) The request method. If not specified, the default method is GET. The options are GET, PUT, or POST.internal(optional, boolean) A flag that determines whether to set x-envoy-internal to “true”. If not specified, or if internal is equal to false, x-envoy-internal is not set.random_value(optional, integer) An integer used to identify the target for weighted cluster selection. The default value of random_value is 0.ssl(optional, boolean) A flag that determines whether to set x-forwarded-proto to https or http. By setting x-forwarded-proto to a given protocol, the tool is able to simulate the behavior of a client issuing a request via http or https. By default ssl is false which corresponds to x-forwarded-proto set to http.additional_headers(optional, array) Additional headers to be added as input for route determination. The “:authority”, “:path”, “:method”, “x-forwarded-proto”, and “x-envoy-internal” fields are specified by the other config options and should not be set here.field(required, string) The name of the header field to add.value(required, string) The value of the header field to add. validate (required, object) The validate object specifies the returned route parameters to match. At least one test parameter must be specificed. Use “” (empty string) to indicate that no return value is expected. For example, to test that no cluster match is expected use {“cluster_name”: “”}.cluster_name(optional, string) Match the cluster name.virtual_cluster_name(optional, string) Match the virtual cluster name.virtual_host_name(optional, string) Match the virtual host name.host_rewrite(optional, string) Match the host header field after rewrite.path_rewrite(optional, string) Match the path header field after rewrite.path_redirect(optional, string) Match the returned redirect path.header_fields(optional, array) Match the listed header fields. Examples header fields include the “:path”, “cookie”, and “date” fields. The header fields are checked after all other test cases. Thus, the header fields checked will be those of the redirected or rewriten routes when applicable.field(required, string) The name of the header field to match.value(required, string) The value of the header field to match. "},"operations/cli.html":{"url":"operations/cli.html","title":"命令行选项","keywords":"","body":"命令行选项 Envoy is driven both by a JSON configuration file as well as a set of command line options. The following are the command line options that Envoy supports. -c`` ``, ``--config-path`` (optional) The path to the v1 or v2 JSON/YAML/proto3 configuration file. If this flag is missing, --config-yaml is required. This will be parsed as a v2 bootstrap configuration file and on failure, subject to --v2-config-only, will be considered as a v1 JSON configuration file. For v2 configuration files, valid extensions are .json, .yaml, .pb and .pb_text, which indicate JSON, YAML, binary proto3 and text proto3 formats respectively. --config-yaml`` (optional) The YAML string for a v2 bootstrap configuration. If --config-path is also set,the values in this YAML string will override and merge with the bootstrap loaded from --config-path. Because YAML is a superset of JSON, a JSON string may also be passed to --config-yaml. --config-yaml is not compatible with bootstrap v1.Example overriding the node id on the command line:./envoy -c bootstrap.yaml –config-yaml “node: {id: ‘node1’}” `--v2-config-only``` (optional) This flag determines whether the configuration file should only be parsed as a v2 bootstrap configuration file. If false (default), when a v2 bootstrap config parse fails, a second attempt to parse the config as a v1 JSON configuration file will be made. --mode`` (optional) One of the operating modes for Envoy:serve: (default) Validate the JSON configuration and then serve traffic normally.validate: Validate the JSON configuration and then exit, printing either an “OK” message (in which case the exit code is 0) or any errors generated by the configuration file (exit code 1). No network traffic is generated, and the hot restart process is not performed, so no other Envoy process on the machine will be disturbed. --admin-address-path`` (optional) The output file path where the admin address and port will be written. --local-address-ip-version`` (optional) The IP address version that is used to populate the server local IP address. This parameter affects various headers including what is appended to the X-Forwarded-For (XFF) header. The options are v4 or v6. The default is v4. --base-id`` (optional) The base ID to use when allocating shared memory regions. Envoy uses shared memory regions during hot restart. Most users will never have to set this option. However, if Envoy needs to be run multiple times on the same machine, each running Envoy will need a unique base ID so that the shared memory regions do not conflict. --concurrency`` (optional) The number of worker threads to run. If not specified defaults to the number of hardware threads on the machine. -l`` ``, ``--log-level`` (optional) The logging level. Non developers should generally never set this option. See the help text for the available log levels and the default. --log-path`` (optional) The output file path where logs should be written. This file will be re-opened when SIGUSR1 is handled. If this is not set, log to stderr. --log-format`` (optional) The format string to use for laying out the log message metadata. If this is not set, a default format string \"[%Y-%m-%d %T.%e][%t][%l][%n] %v\" is used.The supported format flags are (with example output):%v:The actual message to log (“some user text”)%t:Thread id (“1232”)%P:Process id (“3456”)%n:Logger’s name (“filter”)%l:The log level of the message (“debug”, “info”, etc.)%L:Short log level of the message (“D”, “I”, etc.)%a:Abbreviated weekday name (“Tue”)%A:Full weekday name (“Tuesday”)%b:Abbreviated month name (“Mar”)%B:Full month name (“March”)%c:Date and time representation (“Tue Mar 27 15:25:06 2018”)%C:Year in 2 digits (“18”)%Y:Year in 4 digits (“2018”)%D, %x:Short MM/DD/YY date (“03/27/18”)%m:Month 01-12 (“03”)%d:Day of month 01-31 (“27”)%H:Hours in 24 format 00-23 (“15”)%I:Hours in 12 format 01-12 (“03”)%M:Minutes 00-59 (“25”)%S:Seconds 00-59 (“06”)%e:Millisecond part of the current second 000-999 (“008”)%f:Microsecond part of the current second 000000-999999 (“008789”)%F:Nanosecond part of the current second 000000000-999999999 (“008789123”)%p:AM/PM (“AM”)%r:12-hour clock (“03:25:06 PM”)%R:24-hour HH:MM time, equivalent to %H:%M (“15:25”)%T, %X:ISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S (“13:25:06”)%z:ISO 8601 offset from UTC in timezone ([+/-]HH:MM) (“-07:00”)%%:The % sign (“%”) --restart-epoch`` (optional) The hot restart epoch. (The number of times Envoy has been hot restarted instead of a fresh start). Defaults to 0 for the first start. This option tells Envoy whether to attempt to create the shared memory region needed for hot restart, or whether to open an existing one. It should be incremented every time a hot restart takes place. The hot restart wrapper sets the RESTART_EPOCH environment variable which should be passed to this option in most cases. `--hot-restart-version``` (optional) Outputs an opaque hot restart compatibility version for the binary. This can be matched against the output of the GET /hot_restart_version admin endpoint to determine whether the new binary and the running binary are hot restart compatible. --service-cluster`` (optional) Defines the local service cluster name where Envoy is running. The local service cluster name is first sourced from the Bootstrap node message’s cluster field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if any of the following features are used: statsd, health check cluster verification, runtime override directory, user agent addition, HTTP global rate limiting, CDS, and HTTP tracing, either via this CLI option or in the bootstrap configuration. --service-node`` (optional) Defines the local service node name where Envoy is running. The local service node name is first sourced from the Bootstrap node message’s id field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if any of the following features are used: statsd, CDS, and HTTP tracing, either via this CLI option or in the bootstrap configuration. --service-zone`` (optional) Defines the local service zone where Envoy is running. The local service zone is first sourced from the Bootstrap node message’s locality.zone field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if discovery service routing is used and the discovery service exposes zone data, either via this CLI option or in the bootstrap configuration. The meaning of zone is context dependent, e.g. Availability Zone (AZ) on AWS, Zone on GCP, etc. --file-flush-interval-msec`` (optional) The file flushing interval in milliseconds. Defaults to 10 seconds. This setting is used during file creation to determine the duration between flushes of buffers to files. The buffer will flush every time it gets full, or every time the interval has elapsed, whichever comes first. Adjusting this setting is useful when tailing access logs in order to get more (or less) immediate flushing. --drain-time-s`` (optional) The time in seconds that Envoy will drain connections during a hot restart. See thehot restart overview for more information. Defaults to 600 seconds (10 minutes). Generally the drain time should be less than the parent shutdown time set via the --parent-shutdown-time-soption. How the two settings are configured depends on the specific deployment. In edge scenarios, it might be desirable to have a very long drain time. In service to service scenarios, it might be possible to make the drain and shutdown time much shorter (e.g., 60s/90s). --parent-shutdown-time-s`` (optional) The time in seconds that Envoy will wait before shutting down the parent process during a hot restart. See the hot restart overview for more information. Defaults to 900 seconds (15 minutes). --max-obj-name-len`` (optional) The maximum name length (in bytes) of the name field in a cluster/route_config/listener. This setting is typically used in scenarios where the cluster names are auto generated, and often exceed the built-in limit of 60 characters. Defaults to 60.AttentionThis setting affects the output of --hot-restart-version. If you started envoy with this option set to a non default value, you should use the same option (and same value) for subsequent hot restarts. --max-stats`` (optional) The maximum number of stats that can be shared between hot-restarts. This setting affects the output of --hot-restart-version; the same value must be used to hot restart. Defaults to 16384. `--disable-hot-restart``` (optional) This flag disables Envoy hot restart for builds that have it enabled. By default, hot restart is enabled. "},"operations/hot_restarter.html":{"url":"operations/hot_restarter.html","title":"热重启 Python 包装器","keywords":"","body":"热重启 Python 包装器 Typically, Envoy will be hot restarted for config changes and binary updates. However, in many cases, users will wish to use a standard process manager such as monit, runit, etc. We provide /restarter/hot-restarter.py to make this straightforward. The restarter is invoked like so: hot-restarter.py start_envoy.sh start_envoy.sh might be defined like so (using salt/jinja like syntax): #!/bin/bash ulimit -n {{ pillar.get('envoy_max_open_files', '102400') }} exec /usr/sbin/envoy -c /etc/envoy/envoy.cfg --restart-epoch $RESTART_EPOCH --service-cluster {{ grains['cluster_name'] }} --service-node {{ grains['service_node'] }} --service-zone {{ grains.get('ec2_availability-zone', 'unknown') }} The RESTART_EPOCH environment variable is set by the restarter on each restart and can be passed to the --restart-epoch option. The restarter handles the following signals: SIGTERM: Will cleanly terminate all child processes and exit. SIGHUP: Will hot restart by re-invoking whatever is passed as the first argument to the hot restart script. SIGCHLD: If any of the child processes shut down unexpectedly, the restart script will shut everything down and exit to avoid being in an unexpected state. The controlling process manager should then restart the restarter script to start Envoy again. SIGUSR1: Will be forwarded to Envoy as a signal to reopen all access logs. This is used for atomic move and reopen log rotation. "},"operations/admin.html":{"url":"operations/admin.html","title":"管理接口","keywords":"","body":"管理接口 Envoy exposes a local administration interface that can be used to query and modify different aspects of the server: v1 API reference v2 API reference Attention The administration interface in its current form both allows destructive operations to be performed (e.g., shutting down the server) as well as potentially exposes private information (e.g., stats, cluster names, cert info, etc.). It is critical that access to the administration interface is only allowed via a secure network. It is also critical that hosts that access the administration interface are only attached to the secure network (i.e., to avoid CSRF attacks). This involves setting up an appropriate firewall or optimally only allowing access to the administration listener via localhost. This can be accomplished with a v2 configuration like the following: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } In the future additional security options will be added to the administration interface. This work is tracked in this issue. All mutations should be sent as HTTP POST operations. For a limited time, they will continue to work with HTTP GET, with a warning logged. GET / Render an HTML home page with a table of links to all available options. GET /help Print a textual table of all available options. GET /certs List out all loaded TLS certificates, including file name, serial number, and days until expiration. GET /clusters List out all configured cluster manager clusters. This information includes all discovered upstream hosts in each cluster along with per host statistics. This is useful for debugging service discovery issues.Cluster manager informationversion_info string – the version info string of the last loaded CDS update. If envoy does not have CDS setup, the output will read version_info::static.Cluster wide informationcircuit breakers settings for all priority settings.Information about outlier detection if a detector is installed. Currently success rate average, and ejection threshold are presented. Both of these values could be -1 if there was not enough data to calculate them in the last interval.added_via_api flag – false if the cluster was added via static configuration, true if it was added via the CDS api.Per host statisticsNameTypeDescriptioncx_totalCounterTotal connectionscx_activeGaugeTotal active connectionscx_connect_failCounterTotal connection failuresrq_totalCounterTotal requestsrq_timeoutCounterTotal timed out requestsrq_successCounterTotal requests with non-5xx responsesrq_errorCounterTotal requests with 5xx responsesrq_activeGaugeTotal active requestshealthyStringThe health status of the host. See belowweightIntegerLoad balancing weight (1-100)zoneStringService zonecanaryBooleanWhether the host is a canarysuccess_rateDoubleRequest success rate (0-100). -1 if there was not enough request volume in the interval to calculate itHost health statusA host is either healthy or unhealthy because of one or more different failing health states. If the host is healthy the healthy output will be equal to healthy.If the host is not healthy, the healthy output will be composed of one or more of the following strings:/failed_active_hc: The host has failed an active health check./failed_eds_health: The host was marked unhealthy by EDS./failed_outlier_check: The host has failed an outlier detection check. GET /config_dump Dump currently loaded configuration from various Envoy components as JSON-serialized proto messages. See the response definition for more information. POST /cpuprofiler Enable or disable the CPU profiler. Requires compiling with gperftools. POST /healthcheck/fail Fail inbound health checks. This requires the use of the HTTP health check filter. This is useful for draining a server prior to shutting it down or doing a full restart. Invoking this command will universally fail health check requests regardless of how the filter is configured (pass through, etc.). POST /healthcheck/ok Negate the effect of POST /healthcheck/fail. This requires the use of the HTTP health check filter. GET /hot_restart_version See --hot-restart-version. POST /logging Enable/disable different logging levels on different subcomponents. Generally only used during development. POST /quitquitquit Cleanly exit the server. POST /reset_counters Reset all counters to zero. This is useful along with GET /stats during debugging. Note that this does not drop any data sent to statsd. It just effects local output of the GET /stats command. GET /server_info Outputs information about the running server. Sample output looks like: envoy 267724/RELEASE live 1571 1571 0 The fields are: Process name Compiled SHA and build type Health check state (live or draining) Current hot restart epoch uptime in seconds Total uptime in seconds (across all hot restarts) Current hot restart epoch GET /stats Outputs all statistics on demand. This command is very useful for local debugging. Histograms will output the computed quantiles i.e P0,P25,P50,P75,P90,P99,P99.9 and P100. The output for each quantile will be in the form of (interval,cumulative) where interval value represents the summary since last flush interval and cumulative value represents the summary since the start of envoy instance. See here for more information.GET /stats?format=jsonOutputs /stats in JSON format. This can be used for programmatic access of stats. Counters and Gauges will be in the form of a set of (name,value) pairs. Histograms will be under the element “histograms”, that contains “supported_quantiles” which lists the quantiles supported and an array of computed_quantiles that has the computed quantile for each histogram. Only histograms with recorded values will be exported.If a histogram is not updated during an interval, the ouput will have null for all the quantiles.Example histogram output: { \"histograms\": { \"supported_quantiles\": [ 0, 25, 50, 75, 90, 95, 99, 99.9, 100 ], \"computed_quantiles\": [ { \"name\": \"cluster.external_auth_cluster.upstream_cx_length_ms\", \"values\": [ {\"interval\": 0, \"cumulative\": 0}, {\"interval\": 0, \"cumulative\": 0}, {\"interval\": 1.0435787, \"cumulative\": 1.0435787}, {\"interval\": 1.0941565, \"cumulative\": 1.0941565}, {\"interval\": 2.0860023, \"cumulative\": 2.0860023}, {\"interval\": 3.0665233, \"cumulative\": 3.0665233}, {\"interval\": 6.046609, \"cumulative\": 6.046609}, {\"interval\": 229.57333,\"cumulative\": 229.57333}, {\"interval\": 260,\"cumulative\": 260} ] }, { \"name\": \"http.admin.downstream_rq_time\", \"values\": [ {\"interval\": null, \"cumulative\": 0}, {\"interval\": null, \"cumulative\": 0}, {\"interval\": null, \"cumulative\": 1.0435787}, {\"interval\": null, \"cumulative\": 1.0941565}, {\"interval\": null, \"cumulative\": 2.0860023}, {\"interval\": null, \"cumulative\": 3.0665233}, {\"interval\": null, \"cumulative\": 6.046609}, {\"interval\": null, \"cumulative\": 229.57333}, {\"interval\": null, \"cumulative\": 260} ] } ] } } GET /stats?format=prometheus or alternatively,GET /stats/prometheusOutputs /stats in Prometheus v0.0.4 format. This can be used to integrate with a Prometheus server. Currently, only counters and gauges are output. Histograms will be output in a future update. GET /runtime Outputs all runtime values on demand in JSON format. See here for more information on how these values are configured and utilized. The output include the list of the active runtime override layers and the stack of layer values for each key. Empty strings indicate no value, and the final active value from the stack also is included in a separate key. Example output: { \"layers\": [ \"disk\", \"override\", \"admin\", ], \"entries\": { \"my_key\": { \"layer_values\": [ \"my_disk_value\", \"\", \"\" ], \"final_value\": \"my_disk_value\" }, \"my_second_key\": { \"layer_values\": [ \"my_second_disk_value\", \"my_disk_override_value\", \"my_admin_override_value\" ], \"final_value\": \"my_admin_override_value\" } } } POST /runtime_modify?key1=value1&key2=value2&keyN=valueN Adds or modifies runtime values as passed in query parameters. To delete a previously added key, use an empty string as the value. Note that deletion only applies to overrides added via this endpoint; values loaded from disk can be modified via override but not deleted. Attention Use the /runtime_modify endpoint with care. Changes are effectively immediately. It is criticalthat the admin interface is properly secured. "},"operations/stats_overview.html":{"url":"operations/stats_overview.html","title":"统计概览","keywords":"","body":"统计概览 Envoy outputs numerous statistics which depend on how the server is configured. They can be seen locally via the GET /stats command and are typically sent to a statsd cluster. The statistics that are output are documented in the relevant sections of the configuration guide. Some of the more important statistics that will almost always be used can be found in the following sections: HTTP connection manager Upstream cluster "},"operations/runtime.html":{"url":"operations/runtime.html","title":"运行时","keywords":"","body":"运行时 Runtime configuration can be used to modify various server settings without restarting Envoy. The runtime settings that are available depend on how the server is configured. They are documented in the relevant sections of the configuration guide. "},"operations/fs_flags.html":{"url":"operations/fs_flags.html","title":"文件系统标志","keywords":"","body":"文件系统标志 Envoy supports file system “flags” that alter state at startup. This is used to persist changes between restarts if necessary. The flag files should be placed in the directory specified in the flags_path configuration option. The currently supported flag files are: drain If this file exists, Envoy will start in HC failing mode, similar to after the POST /healthcheck/failcommand has been executed. "},"operations/traffic_capture.html":{"url":"operations/traffic_capture.html","title":"流量捕获","keywords":"","body":"流量捕获 Envoy currently provides an experimental transport socket extension that can intercept traffic and write to a protobuf capture file. Warning This feature is experimental and has a known limitation that it will OOM for large traces on a given socket. It can also be disabled in the build if there are security concerns, see https://github.com/envoyproxy/envoy/blob/master/bazel/README.md#disabling-extensions. 配置 Capture can be configured on Listener and Cluster transport sockets, providing the ability to interpose on downstream and upstream L4 connections respectively. To configure traffic capture, add an envoy.transport_sockets.capture transport socket configurationto the listener or cluster. For a plain text socket this might look like: transport_socket: name: envoy.transport_sockets.capture config: file_sink: path_prefix: /some/capture/path transport_socket: name: raw_buffer For a TLS socket, this will be: transport_socket: name: envoy.transport_sockets.capture config: file_sink: path_prefix: /some/capture/path transport_socket: name: ssl config: where the TLS context configuration replaces any existing downstream or upstream TLS configuration on the listener or cluster, respectively. Each unique socket instance will generate a trace file prefixed with path_prefix. E.g./some/capture/path_0.pb. PCAP 传播 The generated trace file can be converted to libpcap format, suitable for analysis with tools such as Wireshark with the capture2pcap utility, e.g.: bazel run @envoy_api//tools:capture2pcap /some/capture/path_0.pb path_0.pcap tshark -r path_0.pcap -d \"tcp.port==10000,http2\" -P 1 0.000000 127.0.0.1 → 127.0.0.1 HTTP2 157 Magic, SETTINGS, WINDOW_UPDATE, HEADERS 2 0.013713 127.0.0.1 → 127.0.0.1 HTTP2 91 SETTINGS, SETTINGS, WINDOW_UPDATE 3 0.013820 127.0.0.1 → 127.0.0.1 HTTP2 63 SETTINGS 4 0.128649 127.0.0.1 → 127.0.0.1 HTTP2 5586 HEADERS 5 0.130006 127.0.0.1 → 127.0.0.1 HTTP2 7573 DATA 6 0.131044 127.0.0.1 → 127.0.0.1 HTTP2 3152 DATA, DATA "},"extending/extending.html":{"url":"extending/extending.html","title":"为自定义用例扩展 Envoy","keywords":"","body":"为自定义用例扩展 Envoy The Envoy architecture makes it fairly easily extensible via both network filters and HTTP filters. An example of how to add a network filter and structure the repository and build dependencies can be found at envoy-filter-example. "},"faq/how_fast_is_envoy.html":{"url":"faq/how_fast_is_envoy.html","title":"Envoy 有多快？","keywords":"","body":"Envoy 有多快？ We are frequently asked how fast is Envoy? or how much latency will Envoy add to my requests? The answer is: it depends. Performance depends a great deal on which Envoy features are being used and the environment in which Envoy is run. In addition, doing accurate performance testing is an incredibly difficult task that the project does not currently have resources for. Although we have done quite a bit of performance tuning of Envoy in the critical path and we believe it performs extremely well, because of the previous points we do not currently publish any official benchmarks. We encourage users to benchmark Envoy in their own environments with a configuration similar to what they plan on using in production. "},"faq/binaries.html":{"url":"faq/binaries.html","title":"从哪里能获得二进制文件？","keywords":"","body":"从哪里能获得二进制文件？ 请参考 这里。 "},"faq/sni.html":{"url":"faq/sni.html","title":"如何设置 SNI？","keywords":"","body":"如何设置 SNI？ SNI is only supported in the v2 configuration/API. The current implementation has the requirement that the filters in every FilterChain must be identical. In a future release, this requirement will be relaxed so that SNI can be used to choose between completely different filter chains. Domain name matching can still be used within the HTTP connection manager to choose different routes. This is by far the most common use case for SNI. The following is a YAML example of the above requirement. address: socket_address: { address: 127.0.0.1, port_value: 1234 } filter_chains: - filter_chain_match: sni_domains: \"example.com\" tls_context: common_tls_context: tls_certificates: - certificate_chain: { filename: \"example_com_cert.pem\" } private_key: { filename: \"example_com_key.pem\" } filters: - name: envoy.http_connection_manager config: route_config: virtual_hosts: - routes: - match: { prefix: \"/\" } route: { cluster: service_foo } - filter_chain_match: sni_domains: \"www.example.com\" tls_context: common_tls_context: tls_certificates: - certificate_chain: { filename: \"www_example_com_cert.pem\" } private_key: { filename: \"www_example_com_key.pem\" } filters: - name: envoy.http_connection_manager config: route_config: virtual_hosts: - routes: - match: { prefix: \"/\" } route: { cluster: service_foo } "},"faq/zone_aware_routing.html":{"url":"faq/zone_aware_routing.html","title":"如何设置 zone 可感知路由？","keywords":"","body":"如何设置 zone 可感知路由？ There are several steps required for enabling zone aware routing between source service (“cluster_a”) and destination service (“cluster_b”). Envoy configuration on the source service This section describes the specific configuration for the Envoy running side by side with the source service. These are the requirements: Envoy must be launched with --service-zone option which defines the zone for the current host. Both definitions of the source and the destination clusters must have sds type. local_cluster_name must be set to the source cluster. Only essential parts are listed in the configuration below for the cluster manager. { \"sds\": \"{...}\", \"local_cluster_name\": \"cluster_a\", \"clusters\": [ { \"name\": \"cluster_a\", \"type\": \"sds\", }, { \"name\": \"cluster_b\", \"type\": \"sds\" } ] } Envoy configuration on the destination service It’s not necessary to run Envoy side by side with the destination service, but it’s important that each host in the destination cluster registers with the discovery service queried by the source service Envoy. Zone information must be available as part of that response. Only zone related data is listed in the response below. { \"tags\": { \"az\": \"us-east-1d\" } } Infrastructure setup The above configuration is necessary for zone aware routing, but there are certain conditions when zone aware routing is not performed. Verification steps Use per zone Envoy stats to monitor cross zone traffic. "},"faq/zipkin_tracing.html":{"url":"faq/zipkin_tracing.html","title":"如何设置 Zipkin 追踪？","keywords":"","body":"如何设置 Zipkin 追踪？ Refer to the zipkin sandbox setup for an example of zipkin tracing configuration. "},"faq/lb_panic_threshold.html":{"url":"faq/lb_panic_threshold.html","title":"我设置了健康检查，但是当有节点失败时，Envoy 又路由到那些节点，这是怎么回事？","keywords":"","body":"我设置了健康检查，但是当有节点失败时，Envoy 又路由到那些节点，这是怎么回事？ This feature is known as the load balancer panic threshold. It is used to prevent cascading failure when upstream hosts start failing health checks in large numbers. "},"faq/concurrency_lb.html":{"url":"faq/concurrency_lb.html","title":"为什么 Round Robin 负载均衡看起来不起作用？","keywords":"","body":"为什么 Round Robin 负载均衡看起来不起作用？ Envoy utilizes a siloed threading model. This means that worker threads and the load balancers that run on them do not coordinate with each other. When utilizing load balancing policies such as round robin, it may thus appear that load balancing is not working properly when using multiple workers. The --concurrency option can be used to adjust the number of workers if desired. The siloed execution model is also the reason why multiple HTTP/2 connections may be established to each upstream; connection pools are not shared between workers. "}}