{"./":{"url":"./","title":"介绍","keywords":"","body":"Envoy 官方文档中文版 Envoy 官方文档中文版，基于 Envoy 最新的 1.7 版本。 本项目文档地址：https://github.com/servicemesher/envoy 英文官方文档地址：https://www.envoyproxy.io/docs/envoy/latest GitHub Pages 阅读地址：https://servicemesher.github.io/envoy/ 参与翻译请先阅读翻译规范。 本文档使用 Gitbook 生成。 "},"about_docs.html":{"url":"about_docs.html","title":"关于本文档","keywords":"","body":"关于本文档 Envoy 文档由以下几个主要部分组成： 简介：本部分介绍 Envoy 的概况、体系结构概述、典型部署方式等。 入门：使用 Docker 快速开始使用 Envoy。 安装：如何使用 Docker 构建/安装 Envoy。 配置：遗留 v1 API 和新 v2 API 共同的详细配置指令。相关时，配置指南还包含有关统计信息、运行时配置和 API 的信息。 操作：有关如何操作 Envoy 的常规信息，包括命令行界面、热重启包装器、管理界面、常规统计概览等。 扩展 Envoy：有关如何为 Envoy 编写自定义过滤器的信息。 v1 API 参考：特定于遗留 v1 API 的配置详细信息。 v2 API 参考：特定于新 v2 API 的配置细节。 Envoy 常见问题：有疑问？希望我们的答案能让您满意。 "},"intro/what_is_envoy.html":{"url":"intro/what_is_envoy.html","title":"Envoy 是什么？","keywords":"","body":"Envoy 是什么？ Envoy is an L7 proxy and communication bus designed for large modern service oriented architectures. The project was born out of the belief that: The network should be transparent to applications. When network and application problems do occur it should be easy to determine the source of the problem. In practice, achieving the previously stated goal is incredibly difficult. Envoy attempts to do so by providing the following high level features: Out of process architecture: Envoy is a self contained process that is designed to run alongside every application server. All of the Envoys form a transparent communication mesh in which each application sends and receives messages to and from localhost and is unaware of the network topology. The out of process architecture has two substantial benefits over the traditional library approach to service to service communication: Envoy works with any application language. A single Envoy deployment can form a mesh between Java, C++, Go, PHP, Python, etc. It is becoming increasingly common for service oriented architectures to use multiple application frameworks and languages. Envoy transparently bridges the gap. As anyone that has worked with a large service oriented architecture knows, deploying library upgrades can be incredibly painful. Envoy can be deployed and upgraded quickly across an entire infrastructure transparently. Modern C++11 code base: Envoy is written in C++11. Native code was chosen because we believe that an architectural component such as Envoy should get out of the way as much as possible. Modern application developers already deal with tail latencies that are difficult to reason about due to deployments in shared cloud environments and the use of very productive but not particularly well performing languages such as PHP, Python, Ruby, Scala, etc. Native code provides generally excellent latency properties that don’t add additional confusion to an already confusing situation. Unlike other native code proxy solutions written in C, C++11 provides both excellent developer productivity and performance. L3/L4 filter architecture: At its core, Envoy is an L3/L4 network proxy. A pluggable filter chain mechanism allows filters to be written to perform different TCP proxy tasks and inserted into the main server. Filters have already been written to support various tasks such as raw TCP proxy,HTTP proxy, TLS client certificate authentication, etc. HTTP L7 filter architecture: HTTP is such a critical component of modern application architectures that Envoy supports an additional HTTP L7 filter layer. HTTP filters can be plugged into the HTTP connection management subsystem that perform different tasks such as buffering, rate limiting, routing/forwarding, sniffing Amazon’s DynamoDB, etc. First class HTTP/2 support: When operating in HTTP mode, Envoy supports both HTTP/1.1 and HTTP/2. Envoy can operate as a transparent HTTP/1.1 to HTTP/2 proxy in both directions. This means that any combination of HTTP/1.1 and HTTP/2 clients and target servers can be bridged. The recommended service to service configuration uses HTTP/2 between all Envoys to create a mesh of persistent connections that requests and responses can be multiplexed over. Envoy does not support SPDY as the protocol is being phased out. HTTP L7 routing: When operating in HTTP mode, Envoy supports a routing subsystem that is capable of routing and redirecting requests based on path, authority, content type, runtime values, etc. This functionality is most useful when using Envoy as a front/edge proxy but is also leveraged when building a service to service mesh. gRPC support: gRPC is an RPC framework from Google that uses HTTP/2 as the underlying multiplexed transport. Envoy supports all of the HTTP/2 features required to be used as the routing and load balancing substrate for gRPC requests and responses. The two systems are very complementary. MongoDB L7 support: MongoDB is a popular database used in modern web applications. Envoy supports L7 sniffing, statistics production, and logging for MongoDB connections. DynamoDB L7 support: DynamoDB is Amazon’s hosted key/value NOSQL datastore. Envoy supports L7 sniffing and statistics production for DynamoDB connections. Service discovery: Service discovery is a critical component of service oriented architectures. Envoy supports multiple service discovery methods including asynchronous DNS resolution and REST based lookup via a service discovery service. Health checking: The recommended way of building an Envoy mesh is to treat service discovery as an eventually consistent process. Envoy includes a health checking subsystem which can optionally perform active health checking of upstream service clusters. Envoy then uses the union of service discovery and health checking information to determine healthy load balancing targets. Envoy also supports passive health checking via an outlier detection subsystem. Advanced load balancing: Load balancing among different components in a distributed system is a complex problem. Because Envoy is a self contained proxy instead of a library, it is able to implement advanced load balancing techniques in a single place and have them be accessible to any application. Currently Envoy includes support for automatic retries, circuit breaking, global rate limiting via an external rate limiting service, request shadowing, and outlier detection. Future support is planned for request racing. Front/edge proxy support: Although Envoy is primarily designed as a service to service communication system, there is benefit in using the same software at the edge (observability, management, identical service discovery and load balancing algorithms, etc.). Envoy includes enough features to make it usable as an edge proxy for most modern web application use cases. This includes TLS termination, HTTP/1.1 and HTTP/2 support, as well as HTTP L7 routing. Best in class observability: As stated above, the primary goal of Envoy is to make the network transparent. However, problems occur both at the network level and at the application level. Envoy includes robust statistics support for all subsystems. statsd (and compatible providers) is the currently supported statistics sink, though plugging in a different one would not be difficult. Statistics are also viewable via the administration port. Envoy also supports distributed tracing via thirdparty providers. Dynamic configuration: Envoy optionally consumes a layered set of dynamic configuration APIs. Implementors can use these APIs to build complex centrally managed deployments if desired. 设计目标 A short note on the design goals of the code itself: Although Envoy is by no means slow (we have spent considerable time optimizing certain fast paths), the code has been written to be modular and easy to test versus aiming for the greatest possible absolute performance. It’s our view that this is a more efficient use of time given that typical deployments will be alongside languages and runtimes many times slower and with many times greater memory usage. "},"intro/arch_overview/terminology.html":{"url":"intro/arch_overview/terminology.html","title":"术语","keywords":"","body":"术语 在深入主架构文档之前的一些定义。部分定义在行业中略有争议，下面将展开在 Envoy 文档和代码库中如何使用它们。 Host/主机：能够进行网络通信的实体（如移动设备、服务器上的应用程序）。在此文档中，主机是逻辑网络应用程序。一块物理硬件上可能运行有多个主机，只要它们是可以独立寻址的。 Downstream/下游：下游主机连接到 Envoy，发送请求并接收响应。 Upstream/上游：上游主机接收来自 Envoy 的连接和请求，并返回响应。 Listener/监听器：监听器是命名网地址（例如，端口、unix domain socket等)，可以被下游客户端连接。Envoy 暴露一个或者多个监听器给下游主机连接。 Cluster/集群：集群是指 Envoy 连接到的逻辑上相同的一组上游主机。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到哪个集群成员。 Mesh/网格：一组主机，协调好以提供一致的网络拓扑。在本文档中，“Envoy mesh”是一组 Envoy 代理，它们构成了分布式系统的消息传递基础，这个分布式系统由很多不同服务和应用程序平台组成。 Runtime configuration/运行时配置：外置实时配置系统，和 Envoy 一起部署。可以更改配置设置，影响操作，而无需重启 Envoy 或更改主要配置。 "},"intro/arch_overview/threading_model.html":{"url":"intro/arch_overview/threading_model.html","title":"线程模型","keywords":"","body":"线程模型 Envoy 使用单进程 - 多线程的架构模型。一个 master 线程管理各种琐碎的任务，而一些 worker 线程则负责执行监听、过滤和转发。当监听器接收到一个连接请求时，该连接将其生命周期绑定到一个单独的 worker 线程。这使得 Envoy 主要使用大量单线程（ embarrassingly parallel ）处理工作，并且只有少量的复杂代码用于实现 worker 线程之间的协调工作。通常情况下，Envoy 实现了100%的非阻塞。对于大多数工作负载，我们建议将 worker 线程数配置为物理机器的线程数。 "},"intro/arch_overview/listeners.html":{"url":"intro/arch_overview/listeners.html","title":"监听器","keywords":"","body":"监听器 Envoy配置支持在单个进程中启用任意数量的监听器。通常建议每台机器运行单个Envoy，而不必介意配置的监听器数量。这样运维更简单，而且只有单个统计来源。目前Envoy只支持TCP监听器。 每个监听器都独立配置有一些（L3 / L4）网络级别的过滤器。当监听器接接收到新连接时，配置好的连接本地过滤器将被实例化，并开始处理后续事件。通用监听器架构用于执行绝大多数不同的代理任务（例如，限速，TLS客户端认证, HTTP 连接管理, MongoDB sniffing, 原始 TCP 代理等）。 监听器也可以选择性的配置某些监听器过滤器。 这些过滤器的处理在网络级别过滤器之前进行，并有机会操纵连接元数据，通常会影响后续过滤器或集群处理连接的方式。 监听器也可以通过监听器发现服务 (LDS)动态获取。 监听器 配置. "},"intro/arch_overview/listener_filters.html":{"url":"intro/arch_overview/listener_filters.html","title":"监听器过滤器","keywords":"","body":"监听器过滤器 在监听器一节中讨论到，监听器过滤器可以用于操纵连接元数据。监听器过滤器的主要目的是来更方便地添加系统集成功能，而无需更改Envoy核心功能，并且还可以让多个此类功能之间的交互更加明确。 监听器过滤器的API相对简单，因为最终这些过滤器在新接收的套接字上操作。链中的过滤器可以停止后面的过滤器并随后继续支持。这允许更复杂的场景，例如调用限速服务等。Envoy包含多个监听器过滤器，这些过滤器在此架构概述以及配置参考中都有记录。 "},"intro/arch_overview/network_filters.html":{"url":"intro/arch_overview/network_filters.html","title":"网络 (L3/L4) 过滤器","keywords":"","body":"网络 (L3/L4) 过滤器 如监听器一节所述，网络级（L3/L4）过滤器构成Envoy连接处理的核心。过滤器API允许混合不同的过滤器组合，并匹配和附加到给定的监听器。有三种不同类型的网络过滤器： 读: 当Envoy从下游连接接收数据时，调用读过滤器。 写: 当Envoy要发送数据到下游连接时，调用写过滤器。 读/写: 当Envoy从下游连接接收数据和要发送数据到下游连接时，调用读/写过滤器。 网络级过滤器的API相对简单，因为最终过滤器只操作原始字节和少量连接事件（例如，TLS握手完成，连接在本地或远程断开等）。链中的过滤器可以停止后续的过滤器，并随后继续。这可以实现更复杂的场景，例如调用限速服务等。Envoy包含多个网络级的过滤器，这些过滤器在此架构概述和配置参考中都有说明。 "},"intro/arch_overview/http_connection_management.html":{"url":"intro/arch_overview/http_connection_management.html","title":"HTTP 连接管理","keywords":"","body":"HTTP 连接管理 HTTP is such a critical component of modern service oriented architectures that Envoy implements a large amount of HTTP specific functionality. Envoy has a built in network level filter called theHTTP connection manager. This filter translates raw bytes into HTTP level messages and events (e.g., headers received, body data received, trailers received, etc.). It also handles functionality common to all HTTP connections and requests such as access logging, request ID generation and tracing, request/response header manipulation, route table management, and statistics. HTTP connection manager configuration. HTTP 协议 Envoy’s HTTP connection manager has native support for HTTP/1.1, WebSockets, and HTTP/2. It does not support SPDY. Envoy’s HTTP support was designed to first and foremost be an HTTP/2 multiplexing proxy. Internally, HTTP/2 terminology is used to describe system components. For example, an HTTP request and response take place on a stream. A codec API is used to translate from different wire protocols into a protocol agnostic form for streams, requests, responses, etc. In the case of HTTP/1.1, the codec translates the serial/pipelining capabilities of the protocol into something that looks like HTTP/2 to higher layers. This means that the majority of the code does not need to understand whether a stream originated on an HTTP/1.1 or HTTP/2 connection. HTTP header sanitizing The HTTP connection manager performs various header sanitizing actions for security reasons.路由表 路由表配置 Each HTTP connection manager filter has an associated route table. The route table can be specified in one of two ways: Statically. Dynamically via the RDS API. "},"intro/arch_overview/http_filters.html":{"url":"intro/arch_overview/http_filters.html","title":"HTTP 过滤器","keywords":"","body":"HTTP filter Much like the network level filter stack, Envoy supports an HTTP level filter stack within the connection manager. Filters can be written that operate on HTTP level messages without knowledge of the underlying physical protocol (HTTP/1.1, HTTP/2, etc.) or multiplexing capabilities. There are three types of HTTP level filters: Decoder: Decoder filters are invoked when the connection manager is decoding parts of the request stream (headers, body, and trailers). Encoder: Encoder filters are invoked when the connection manager is about to encode parts of the response stream (headers, body, and trailers). Decoder/Encoder: Decoder/Encoder filters are invoked both when the connection manager is decoding parts of the request stream and when the connection manager is about to encode parts of the response stream. The API for HTTP level filters allows the filters to operate without knowledge of the underlying protocol. Like network level filters, HTTP filters can stop and continue iteration to subsequent filters. This allows for more complex scenarios such as health check handling, calling a rate limiting service, buffering, routing, generating statistics for application traffic such as DynamoDB, etc. Envoy already includes several HTTP level filters that are documented in this architecture overview as well as the configuration reference. "},"intro/arch_overview/http_routing.html":{"url":"intro/arch_overview/http_routing.html","title":"HTTP 路由","keywords":"","body":"HTTP 路由 Envoy includes an HTTP router filter which can be installed to perform advanced routing tasks. This is useful both for handling edge traffic (traditional reverse proxy request handling) as well as for building a service to service Envoy mesh (typically via routing on the host/authority HTTP header to reach a particular upstream service cluster). Envoy also has the ability to be configured as forward proxy. In the forward proxy configuration, mesh clients can participate by appropriately configuring their http proxy to be an Envoy. At a high level the router takes an incoming HTTP request, matches it to an upstream cluster, acquires a connection pool to a host in the upstream cluster, and forwards the request. The router filter supports the following features: Virtual hosts that map domains/authorities to a set of routing rules. Prefix and exact path matching rules (both case sensitive and case insensitive). Regex/slug matching is not currently supported, mainly because it makes it difficult/impossible to programmatically determine whether routing rules conflict with each other. For this reason we don’t recommend regex/slug routing at the reverse proxy level, however we may add support in the future depending on demand. TLS redirection at the virtual host level. Path/host redirection at the route level. Direct (non-proxied) HTTP responses at the route level. Explicit host rewriting. Automatic host rewriting based on the DNS name of the selected upstream host. Prefix rewriting. Websocket upgrades at route level. Request retries specified either via HTTP header or via route configuration. Request timeout specified either via HTTP header or via route configuration. Traffic shifting from one upstream cluster to another via runtime values (see traffic shifting/splitting). Traffic splitting across multiple upstream clusters using weight/percentage-based routing (see traffic shifting/splitting). Arbitrary header matching routing rules. Virtual cluster specifications. A virtual cluster is specified at the virtual host level and is used by Envoy to generate additional statistics on top of the standard cluster level ones. Virtual clusters can use regex matching. Priority based routing. Hash policy based routing. Absolute urls are supported for non-tls forward proxies. 路由表 The configuration for the HTTP connection manager owns the route table that is used by all configured HTTP filters. Although the router filter is the primary consumer of the route table, other filters also have access in case they want to make decisions based on the ultimate destination of the request. For example, the built in rate limit filter consults the route table to determine whether the global rate limit service should be called based on the route. The connection manager makes sure that all calls to acquire a route are stable for a particular request, even if the decision involves randomness (e.g. in the case of a runtime configuration route rule). 重试语义 Envoy allows retries to be configured both in the route configuration as well as for specific requests via request headers. The following configurations are possible: Maximum number of retries: Envoy will continue to retry any number of times. An exponential backoff algorithm is used between each retry. Additionally, all retries are contained within the overall request timeout. This avoids long request times due to a large number of retries. Retry conditions: Envoy can retry on different types of conditions depending on application requirements. For example, network failure, all 5xx response codes, idempotent 4xx response codes, etc. Note that retries may be disabled depending on the contents of the x-envoy-overloaded. 优先级路由 Envoy supports priority routing at the route level. The current priority implementation uses different connection pool and circuit breaking settings for each priority level. This means that even for HTTP/2 requests, two physical connections will be used to an upstream host. In the future Envoy will likely support true HTTP/2 priority over a single connection. The currently supported priorities are default and high. 直接响应 Envoy supports the sending of “direct” responses. These are preconfigured HTTP responses that do not require proxying to an upstream server. There are two ways to specify a direct response in a Route: Set the direct_response field. This works for all HTTP response statuses. Set the redirect field. This works for redirect response statuses only, but it simplifies the setting of the Location header. A direct response has an HTTP status code and an optional body. The Route configuration can specify the response body inline or specify the pathname of a file containing the body. If the Route configuration specifies a file pathname, Envoy will read the file upon configuration load and cache the contents. Attention If a response body is specified, it must be no more than 4KB in size, regardless of whether it is provided inline or in a file. Envoy currently holds the entirety of the body in memory, so the 4KB limit is intended to keep the proxy’s memory footprint from growing too large. If response_headers_to_add has been set for the Route or the enclosing Virtual Host, Envoy will include the specified headers in the direct HTTP response. "},"intro/arch_overview/grpc.html":{"url":"intro/arch_overview/grpc.html","title":"gRPC","keywords":"","body":"gRPC gRpc 是来自 Google 的 RPC 框架。它使用协议缓冲区作为底层 序列化 /IDL(接口描述语言的缩写) 格式。在传输层，它使用 HTTP/2 进行请求/响应复用。Envoy 在传输层和应用层都提供对 gRPC 的一流支持： gRPC 使用 HTTP/2 trailers 特性（可以在 HTTP 请求和响应报文后追加 HTTP Header)来传送请求状态。Envoy 是能够正确支持 HTTP/2 trailers 的少数几个 HTTP 代理之一，因此也是可以传输 gRPC 请求和响应的代理之一。 某些语言的 gRPC 运行时相对不成熟。Envoy 支持 gRPC 桥接过滤器，允许 gRPC 请求通过 HTTP/1.1 发送给 Envoy。然后，Envoy 将请求转换为 HTTP/2 以传输到目标服务器。该响应被转换回 HTTP/1.1。 安装后，除了标准的全局 HTTP 统计数据之外，桥接过滤器还会根据每个 RPC 统计数据进行收集。 gRPC-Web 由一个指定的过滤器支持，该过滤器允许 gRPC-Web 客户端通过 HTTP/1.1 向 Envoy 发送请求并代理到 gRPC 服务器。目前相关团队正在积极开发中，预计它将成为 gRPC 桥接过滤器的后续产品。 gRPC-JSON 转码器由一个指定的过滤器支持，该过滤器允许 RESTful JSON API 客户端通过 HTTP 向 Envoy 发送请求并获取代理到 gRPC 服务。 gRPC 服务 除了在数据层面上代理 gRPC 外，Envoy 在控制层面也使用了 gRPC，它从中获取管理服务器的配置以及过滤器中的配置，例如用于速率限制)或授权检查。我们称之为 gRPC 服务。 当指定 gRPC 服务时，必须指定使用 Envoy gRPC 客户端或 Google C ++ gRPC 客户端。我们在下面的这个选择中讨论权衡。 Envoy gRPC 客户端是使用 Envoy 的 HTTP/2 上行连接管理的 gRPC 的最小自定义实现。服务被指定为常规 Envoy 集群，定期处理超时、重试、终端发现、负载平衡、故障转移、负载报告、断路、健康检查、异常检测。它们与 Envoy 的数据层面共享相同的连接池机制。同样，集群统计信息可用于 gRPC 服务。由于客户端是简化版的 gRPC 实现，因此不包括诸如 OAuth2 或 gRPC-LB 之类的高级 gRPC 功能后备。 Google C++ gRPC 客户端基于 Google 在 https://github.com/grpc/grpc 上提供的 gRPC 参考实现。它提供了 Envoy gRPC 客户端中缺少的高级 gRPC 功能。Google C++ gRPC 客户端独立于 Envoy 的集群管理，执行自己的负载平衡、重试、超时、端点管理等。Google C++ gRPC 客户端还支持自定义身份验证插件。 在大多数情况下，当你不需要 Google C++ gRPC 客户端的高级功能时，建议使用 Envoy gRPC 客户端。这使得配置和监控更加简单。如果 Envoy gRPC 客户端中缺少你所需要的功能，则应该使用 Google C++ gRPC 客户端。 "},"intro/arch_overview/websocket.html":{"url":"intro/arch_overview/websocket.html","title":"WebSocket 支持","keywords":"","body":"WebSocket 支持 Envoy 支持将 HTTP/1.1 连接升级到 WebSocket 连接。仅当下游客户端发送正确的升级请求头，并且被匹配的 HTTP 路由必须明确配置使用了 WebSocket （use_websocket）时才允许连接升级。如果请求到达 WebSocket 的路由没有必要的升级头，它将被视为常规的 HTTP/ 1.1请求。 由于 Envoy 将 WebSocket 连接视为纯 TCP 连接，因此它支持 WebSocket 协议的所有内容，而与它们的报文格式无关。WebSocket 路由不支持某些 HTTP 请求级别的功能，例如重定向，超时，重试，速率限制和阴影。但是，支持前缀重写，显式和自动主机重写，流量转移和拆分。 连接语义 尽管 WebSocket 升级可以通过 HTTP/1.1 连接进行，但 WebSockets 代理的工作模式与普通 TCP 代理类似，即 Envoy 不会解析 websocket 报文帧。下游客户端和/或上游服务器负责终止 WebSocket 连接（例如，通过发送关闭帧）和底层 TCP 连接。 当连接管理器通过支持 WebSocket 的路由接收到 WebSocket 升级请求时，它通过 TCP 连接将请求转发给上游服务器。Envoy 不知道上游服务器是否拒绝了升级请求。上游服务器负责终止 TCP 连接，这将导致 Envoy 终止相应的下游客户端连接。 "},"intro/arch_overview/cluster_manager.html":{"url":"intro/arch_overview/cluster_manager.html","title":"集群管理器","keywords":"","body":"集群管理器 Envoy 集群管理器管理所有配置的上游集群。正如 Envoy 配置可以包含任意数量的监听器一样，配置也可以包含任意数量的独立配置的上游集群。 上游集群和主机是从网络/HTTP 过滤器堆栈中抽象而来，因为上游集群和主机可用于任意数量的不同代理任务。集群管理器向过滤器堆栈暴露API，允许过滤器获得连接到上游集群的 L3/L4 连接，或者连接到上游集群的抽象 HTTP 连接池的句柄（上游主机是否支持 HTTP/1.1 或 HTTP/2 是隐藏的）。过滤器阶段判断是否需要 L3/L4 连接或新的 HTTP 流，而集群管理器处理所有的复杂性，包括获知哪些主机可用并且健康，负载均衡，上游连接数据的线程本地存储（因为大多数 Envoy 代码以单线程编写），上游连接类型（TCP/IP、UDS），适用的上游协议（HTTP/1.1、HTTP/2）等。 集群管理器获知集群的方式可以是静态配置，或者可以通过集群发现服务（CDS）API 动态获取。动态集群获取允许将更多配置存储在中央配置服务器中，因此可以减少 Envoy 重启和重新分配配置的次数。 集群管理器 配置。 CDS 配置。 Cluster warming 当集群在服务器启动或者通过 CDS 进行初始化时，它们会“热身”。这意味着集群在下列操作发生之前不可用。 初始服务发现加载 (例如，DNS 解析、EDS 更新等等)。 初始主动 健康检查 通过，如果配置了主动健康检查。Envoy 将发送健康检查请求到每个被发现的主机来判断它的初始健康状态。 上述项确保 Envoy 在开始将集群用于流量服务之前具有准确的集群视图。 在讨论集群热身时，集群 “变为可用” 意味着: 对于新加入的集群，在集群热身前，集群对于 Envoy 的其余部分是不存在的。即引用集群的 HTTP 路由将导致 404 或 503（取决于配置）。 对于更新后的集群，旧集群将继续存在并服务流量。当新集群被加热后，它将与旧集群进行原子交换，从而不会发生流量中断。 "},"intro/arch_overview/service_discovery.html":{"url":"intro/arch_overview/service_discovery.html","title":"服务发现","keywords":"","body":"Service discovery When an upstream cluster is defined in the configuration, Envoy needs to know how to resolve the members of the cluster. This is known as service discovery. Supported service discovery types Static Static is the simplest service discovery type. The configuration explicitly specifies the resolved network name (IP address/port, unix domain socket, etc.) of each upstream host. Strict DNS When using strict DNS service discovery, Envoy will continuously and asynchronously resolve the specified DNS targets. Each returned IP address in the DNS result will be considered an explicit host in the upstream cluster. This means that if the query returns three IP addresses, Envoy will assume the cluster has three hosts, and all three should be load balanced to. If a host is removed from the result Envoy assumes it no longer exists and will drain traffic from any existing connection pools. Note that Envoy never synchronously resolves DNS in the forwarding path. At the expense of eventual consistency, there is never a worry of blocking on a long running DNS query. Logical DNS Logical DNS uses a similar asynchronous resolution mechanism to strict DNS. However, instead of strictly taking the results of the DNS query and assuming that they comprise the entire upstream cluster, a logical DNS cluster only uses the first IP address returned when a new connection needs to be initiated. Thus, a single logical connection pool may contain physical connections to a variety of different upstream hosts. Connections are never drained. This service discovery type is optimal for large scale web services that must be accessed via DNS. Such services typically use round robin DNS to return many different IP addresses. Typically a different result is returned for each query. If strict DNS were used in this scenario, Envoy would assume that the cluster’s members were changing during every resolution interval which would lead to draining connection pools, connection cycling, etc. Instead, with logical DNS, connections stay alive until they get cycled. When interacting with large scale web services, this is the best of all possible worlds: asynchronous/eventually consistent DNS resolution, long lived connections, and zero blocking in the forwarding path. Original destination Original destination cluster can be used when incoming connections are redirected to Envoy either via an iptables REDIRECT or TPROXY target or with Proxy Protocol. In these cases requests routed to an original destination cluster are forwarded to upstream hosts as addressed by the redirection metadata, without any explicit host configuration or upstream host discovery. Connections to upstream hosts are pooled and unused hosts are flushed out when they have been idle longer thancleanup_interval_ms, which defaults to 5000ms. If the original destination address is is not available, no upstream connection is opened. Original destination service discovery must be used with the original destination load balancer. Service discovery service (SDS) The service discovery service is a generic REST based API used by Envoy to fetch cluster members. Lyft provides a reference implementation via the Python discovery service. That implementation uses AWS DynamoDB as the backing store, however the API is simple enough that it could easily be implemented on top of a variety of different backing stores. For each SDS cluster, Envoy will periodically fetch the cluster members from the discovery service. SDS is the preferred service discovery mechanism for a few reasons: Envoy has explicit knowledge of each upstream host (vs. routing through a DNS resolved load balancer) and can make more intelligent load balancing decisions. Extra attributes carried in the discovery API response for each host inform Envoy of the host’s load balancing weight, canary status, zone, etc. These additional attributes are used globally by the Envoy mesh during load balancing, statistic gathering, etc. Generally active health checking is used in conjunction with the eventually consistent service discovery service data to making load balancing and routing decisions. This is discussed further in the following section. On eventually consistent service discovery Many existing RPC systems treat service discovery as a fully consistent process. To this end, they use fully consistent leader election backing stores such as Zookeeper, etcd, Consul, etc. Our experience has been that operating these backing stores at scale is painful. Envoy was designed from the beginning with the idea that service discovery does not require full consistency. Instead, Envoy assumes that hosts come and go from the mesh in an eventually consistent way. Our recommended way of deploying a service to service Envoy mesh configuration uses eventually consistent service discovery along with active health checking (Envoy explicitly health checking upstream cluster members) to determine cluster health. This paradigm has a number of benefits: All health decisions are fully distributed. Thus, network partitions are gracefully handled (whether the application gracefully handles the partition is a different story). When health checking is configured for an upstream cluster, Envoy uses a 2x2 matrix to determine whether to route to a host: Discovery Status HC OK HC Failed Discovered Route Don’t Route Absent Route Don’t Route / Delete Host discovered / health check OK Envoy will route to the target host. Host absent / health check OK: Envoy will route to the target host. This is very important since the design assumes that the discovery service can fail at any time. If a host continues to pass health check even after becoming absent from the discovery data, Envoy will still route. Although it would be impossible to add new hosts in this scenario, existing hosts will continue to operate normally. When the discovery service is operating normally again the data will eventually re-converge. Host discovered / health check FAIL Envoy will not route to the target host. Health check data is assumed to be more accurate than discovery data. Host absent / health check FAIL Envoy will not route and will delete the target host. This is the only state in which Envoy will purge host data. "},"intro/arch_overview/health_checking.html":{"url":"intro/arch_overview/health_checking.html","title":"健康检查","keywords":"","body":"健康检查 Active health checking can be configured on a per upstream cluster basis. As described in the service discovery section, active health checking and the SDS service discovery type go hand in hand. However, there are other scenarios where active health checking is desired even when using the other service discovery types. Envoy supports three different types of health checking along with various settings (check interval, failures required before marking a host unhealthy, successes required before marking a host healthy, etc.): HTTP: During HTTP health checking Envoy will send an HTTP request to the upstream host. It expects a 200 response if the host is healthy. The upstream host can return 503 if it wants to immediately notify downstream hosts to no longer forward traffic to it. L3/L4: During L3/L4 health checking, Envoy will send a configurable byte buffer to the upstream host. It expects the byte buffer to be echoed in the response if the host is to be considered healthy. Envoy also supports connect only L3/L4 health checking. Redis: Envoy will send a Redis PING command and expect a PONG response. The upstream Redis server can respond with anything other than PONG to cause an immediate active health check failure. Optionally, Envoy can perform EXISTS on a user-specified key. If the key does not exist it is considered a passing healthcheck. This allows the user to mark a Redis instance for maintenance by setting the specified key to any value and waiting for traffic to drain. Seeredis_key. 被动健康检查 Envoy also supports passive health checking via outlier detection. 链接池交互 See here for more information. HTTP 健康检查过滤器 When an Envoy mesh is deployed with active health checking between clusters, a large amount of health checking traffic can be generated. Envoy includes an HTTP health checking filter that can be installed in a configured HTTP listener. This filter is capable of a few different modes of operation: No pass through: In this mode, the health check request is never passed to the local service. Envoy will respond with a 200 or a 503 depending on the current draining state of the server. No pass through, computed from upstream cluster health: In this mode, the health checking filter will return a 200 or a 503 depending on whether at least a specified percentage of the servers are healthy in one or more upstream clusters. (If the Envoy server is in a draining state, though, it will respond with a 503 regardless of the upstream cluster health.) Pass through: In this mode, Envoy will pass every health check request to the local service. The service is expected to return a 200 or a 503 depending on its health state. Pass through with caching: In this mode, Envoy will pass health check requests to the local service, but then cache the result for some period of time. Subsequent health check requests will return the cached value up to the cache time. When the cache time is reached, the next health check request will be passed to the local service. This is the recommended mode of operation when operating a large mesh. Envoy uses persistent connections for health checking traffic and health check requests have very little cost to Envoy itself. Thus, this mode of operation yields an eventually consistent view of the health state of each upstream host without overwhelming the local service with a large number of health check requests. Further reading: Health check filter configuration. /healthcheck/fail admin endpoint. /healthcheck/ok admin endpoint. 主动健康检查快速失败 When using active health checking along with passive health checking (outlier detection), it is common to use a long health checking interval to avoid a large amount of active health checking traffic. In this case, it is still useful to be able to quickly drain an upstream host when using the /healthcheck/fail admin endpoint. To support this, the router filter will respond to the x-envoy-immediate-health-check-fail header. If this header is set by an upstream host, Envoy will immediately mark the host as being failed for active health check. Note that this only occurs if the host’s cluster has active health checking configured. The health checking filter will automatically set this header if Envoy has been marked as failed via the /healthcheck/fail admin endpoint. 健康检查身份 Just verifying that an upstream host responds to a particular health check URL does not necessarily mean that the upstream host is valid. For example, when using eventually consistent service discovery in a cloud auto scaling or container environment, it’s possible for a host to go away and then come back with the same IP address, but as a different host type. One solution to this problem is having a different HTTP health checking URL for every service type. The downside of that approach is that overall configuration becomes more complicated as every health check URL is fully custom. The Envoy HTTP health checker supports the service_name option. If this option is set, the health checker additionally compares the value of the x-envoy-upstream-healthchecked-cluster response header to service_name. If the values do not match, the health check does not pass. The upstream health check filter appends x-envoy-upstream-healthchecked-cluster to the response headers. The appended value is determined by the --service-cluster command line option. "},"intro/arch_overview/connection_pooling.html":{"url":"intro/arch_overview/connection_pooling.html","title":"连接池","keywords":"","body":"连接池 对于 HTTP 流量，Envoy 支持在底层协议（HTTP/1.1 或 HTTP/2）之上的抽象连接池。过滤器代码不需要知道底层协议是否支持真正的复用。 在实践中，底层实现具有以下高级属性： HTTP/1.1 HTTP/1.1 连接池根据需要获取上游主机的连接（取决于断路限制）。当连接变得可用时，请求被绑定到连接，这可能是因为连接完成先前请求的处理，或者因为新的连接已经准备好可以接收第一次请求。HTTP/1.1 连接池不使用流水线，因此如果上游连接被切断，只有一个下游请求必须重置。 HTTP/2 HTTP/2连接池获取到上游主机的单个连接。所有请求都通过此连接复用。如果收到 GOAWAY 帧，或者连接达到最大流限制，连接池将创建新的连接并且耗尽现有连接。 HTTP/2 是首选的通信协议，因为连接很少会被切断。 健康检查交互 如果 Envoy 配置有主动或被动健康检查，则代表从健康状态转换为不健康状态的主机将关闭所有连接池连接。如果主机重新进入负载均衡轮换，它将创建新的连接，这将最大化解决流量不佳（由于 ECMP 路由或其他原因）的机会。 "},"intro/arch_overview/load_balancing.html":{"url":"intro/arch_overview/load_balancing.html","title":"负载均衡","keywords":"","body":"负载均衡 当过滤器需要获取到上游集群中主机的连接时，cluster manager 将使用负载均衡策略来确定选择哪个主机。负载均衡策略是可拔插的，并且在配置中以每个上游集群为基础进行指定。请注意，如果没有为集群配置活动的运行状况检查策略，则所有上游集群成员都认为是健康的。 支持的负载均衡器 Round robin 这是一个简单的策略，每个健康的上游主机按循环顺序选择。如果将权重分配给本地的端点，则使用加权 round robin（循环）调度，其中较高权重的端点将更频繁地出现在循环中以实现有效权重。 加权最少请求 请求最少的负载均衡器使用 O(1) 算法选择两个随机健康主机，并选择主动请求较少的主机（研究表明这种方法几乎与 O(N) 全扫描一样好）。如果集群中的任何主机的负载均衡权重大于1，则负载均衡器将转换为随机选择主机并使用该主机时间的模式。该算法对于负载测试来说简单且足够。它不应该用于需要真正的加权最小请求的地方（通常请求持续时间可变且较长）。我们可能会在将来添加一个真正的全扫描加权最小请求变体来涵盖此用例。 Ring hash Ring/modulo 哈希负载均衡器实现对上游主机的一致性哈希。该算法基于将所有主机映射到一个环上，使得从主机集添加或移除主机的更改仅影响 1/N 个请求。这种技术通常也被称为“ketama”哈希。一致的哈希负载均衡器只有在使用指定哈希值的协议路由时才有效。最小环大小控制环中每个主机的复制因子。例如，如果最小环大小为 1024 并且有 16 个主机，则每个主机将被复制 64 次。环哈希负载均衡器当前不支持加权。 当使用基于优先级的负载均衡时，优先级也通过哈希选择，因此当后端集合稳定时，选定的端点仍将保持一致。 注意 环哈希负载均衡器不支持所在地加权负载均衡。 Maglev Maglev（磁悬浮）负载均衡器对上游主机实施一致性的哈希。它使用本文第 3.4 节中描述的算法，固定表大小为65537（参见同一论文的第5.3节）。Maglev 可以用作环哈希负载均衡器的替代品，可以在任何需要一致性哈希的地方使用。就像环哈希负载均衡器一样，只有在使用指定哈希值的协议路由时，一致性哈希负载均衡器才有效。 一般来说，与环形散列（“ketama”）算法相比，Maglev 具有快得多的查表编译时间以及主机选择时间（当使用 256K 条目的大环时大约分别为 10 倍和 5 倍）。Maglev 的缺点是它不像环哈希那样稳定。当主机被移除时，更多的键将移动位置（模拟显示键将移动大约两倍）。据说，对于包括 Redis 在内的许多应用程序来说，Maglev 很可能是环形哈希替代品的一大优势。高级读者可以使用这个 benchmark 来比较具有不同参数的环形哈希与 Maglev。 随机 随机负载均衡器选择一个随机的健康主机。如果没有配置健康检查策略，则随机负载均衡器通常比 round robin 更好。随机选择可以避免主机出现故障后对集合中的主机造成偏见。 原始目的地 这是一种专用的负载均衡器，只能与原始目的集群一起使用。上游主机是根据下游连接元数据选择的，即连接被打开到与连接重定向到 envoy 之前传入与连接的目标地址相同的地址。新的目标由负载均衡器按需添加到集群，并且集群会定期清除集群中未使用的主机。原始目标集群不能使用其他负载均衡类型。 恐慌阈值 在负载均衡期间，Envoy 通常只会考虑上游集群中健康的主机。但是，如果集群中健康主机的百分比变得过低，envoy 将忽视所有主机中的健康状况和均衡。这被称为恐慌阈值。缺省恐慌阈值是 50％。这可以通过运行时以及集群配置进行配置。恐慌阈值用于避免在负载增加时主机故障导致整个集群中级联故障的情况。 请注意，恐慌阈值是有优先级的。这意味着如果单个优先级中健康节点的百分比低于阈值，该优先级将进入恐慌模式。一般而言，不鼓励将恐慌阈值与优先级结合使用，因为当有足够多的节点的状态不健康触发恐慌阈值时，大部分流量应该已经溢出到下一个优先级。 优先级划分 在负载均衡期间，Envoy 通常只考虑配置为最高优先级的主机。对于每个 EDS LocalityLbEndpoints，还可以指定一个可选的优先级。当最高优先级（P=0）的端点健康时，所有流量将以该优先级落在端点上。由于最高优先级的端点变得不健康，因此流量将开始慢慢降低优先级。 目前，假定每个优先级级别都由因子 1.4（硬编码）过度配置的。因此，如果 80％ 的端点是健康的，那么优先级仍然被认为是健康的，因为 80*1.4>100。当健康端点的数量下降到 72％ 以下时，优先级的健康状况低于100。此时，相当于到 P=0 健康状态的流量的百分比将转到 P=0，剩余流量将流向 P=1。 假设一个简单的设置有 2 个优先级，P=1 100％ 健康。 P=0 健康端点 到 P=0 的流量百分比 到 P=1 的流量百分比 100% 100% 0% 72% 100% 0% 71% 99% 1% 50% 70% 30% 25% 35% 65% 0% 0% 100% 如果 P=1 变得不健康，它将继续从 P=0 溢出负载，直到健康 P=0 + P=1 的总和低于 100 为止。此时，健康状态将被放大到 100％ 的“有效”健康状态。 P=0 健康端点 P=1 健康端点 Traffic to P=0 到 P=1 的流量 100% 100% 100% 0% 72% 72% 100% 0% 71% 71% 99% 1% 50% 50% 70% 30% 25% 100% 35% 65% 25% 25% 50% 50% 随着更多的优先级被添加，每个级别消耗等于其“缩放”有效健康的负载，所以如果 P=0 + P=1 的组合健康度小于 100，则 P=2 将仅接收流量。 P=0 健康端点 P=1 健康端点 P=2 健康端点 到 P=0 的流量 到 P=1 的流量 到 P=2 的流量 100% 100% 100% 100% 0% 0% 72% 72% 100% 100% 0% 0% 71% 71% 100% 99% 1% 0% 50% 50% 100% 70% 30% 0% 25% 100% 100% 35% 65% 0% 25% 25% 100% 25% 25% 50% 在伪代码中加和： load to P_0 = min(100, health(P_0) * 100 / total_health) health(P_X) = 140 * healthy_P_X_backends / total_P_X_backends total_health = min(100, Σ(health(P_0)...health(P_X)) load to P_X = 100 - Σ(percent_load(P_0)..percent_load(P_X-1)) Zone 感知路由 我们使用以下术语： 始发/上游集群：Envoy 将来自始发集群的请求路由到上游集群中。 本地 zone：包含始发和上游集群中主机子集的同一区域。 Zone 感知路由：尽量将请求路由到本地 zone 的上游集群主机上。 当始发和上游集群中的主机部署不同 zone 时，Envoy 执行 zone 感知路由。在执行 zone 感知路由之前有几个先决条件： 始发和上游集群都不处于恐慌模式。 Zone 感知路由已启用。 始发集群与上游集群具有相同数量的 zone。 上游集群有足够多的主机。浏览此处获取更多信息。 Zone 感知路由的目的是尽可能多地向上游集群的本地 zone 中发送流量，同时大致保持上游集群中的所有主机拥有相同的（取决于负载均衡策略）的每秒请求数量。 只要上游集群中每台主机的请求数量保持大致相同，Envoy 就会尝试尽可能多地将流量推送到本地上游区域。Envoy 路由到本地 zone 还是执行跨 zone 路由，这取决于本地 zone 中始发集群和上游集群中健康主机的百分比。关于始发和上游集群之间的本地 zone 百分比关系有以下两种情况： 始发集群的本地 zone 百分比大于上游集群中本地 zone 的百分比。在这种情况下，我们无法将来自始发集群本地 zone 的所有请求路由到上游集群的本地 zone，因为这会导致所有上游主机请求不均衡。相反，Envoy 计算可以直接路由到上游集群的本地 zone 的请求的百分比。其余的请求被路由到跨 zone。特定 zone 根据 zone 的剩余容量（该 zone 将获得一些本地 zone 流量并且可能具有 Envoy 可用于跨 zone 业务量的额外容量）来选择。 始发集群本地 zone 百分比小于上游集群中的本地 zone 百分比。在这种情况下，上游集群的本地 zone 可以获取来自始发集群本地 zone 的所有请求，并且还有一定空间允许来自集群中其他 zone 的流量（如果有必要的话）。 请注意，使用多个优先级时，zone 感知路由当前仅支持 P=0。 所在地加权负载均衡 另一种用于确定如何在不同 zone 和地理位置之间分配权重的方式是使用LocalityLbEndpoints消息中通过 EDS 提供的显式权重。这种方式与上述 zone 感知路由相互排斥，因为在所在地感知 LB 的情况下，我们通过管理服务器来提供所在地加权，而不是在 zone 感知路由中使用的 Envoy 侧启发式的方式。 当所有端点健康时，使用加权循环 round-robin 来挑选本地节点，其中地点权重用于加权。当某地的某些端点不健康时，我们通过调整地点的权重来反映这一点。与优先级一样，我们设置了一个过度提供因子（目前硬编码为 1.4），这意味着当一个地区只有少数端点不健康时，我们不会进行任何权重调整。 假设一个简单的设置，包含 2 个地点 X 和 Y，其中 X 的所在地权重为 1，Y 的所在地权重为 2，L=Y 100％ 健康。 L=X 健康端点 到 L=X 的流量百分比 到 L=Y 的流量百分比 100% 33% 67% 70% 33% 67% 69% 32% 68% 50% 26% 74% 25% 15% 85% 0% 0% 100% 在伪代码中加和： health(L_X) = 140 * healthy_X_backends / total_X_backends effective_weight(L_X) = locality_weight_X * min(100, health(L_X)) load to L_X = effective_weight(L_X) / Σ_c(effective_weight(L_c)) 请注意，在挑选优先级之后进行所在地加权选取。负载均衡器遵循以下步骤： 挑选优先级别。 从（1）中选择优先级别的所在地（如本节所述）。 从（2）中选择使用集群指定的负载均衡器所在地范围内的端点。 通过在集群配置中设置locality_weighted_lb_config并通过load_balancing_weight在LocalityLbEndpoints中提供权重来配置所在地加权负载均衡。 此功能与负载均衡器子集设置不兼容，因为将个别子集的所在地级权重与显而易见的权重进行协调并不容易。 负载均衡器子集 根据附加在主机上的元数据将上游集群中的主机划分为子集，可以这样来配置 Envoy。然后，路由可以指定主机必须匹配的元数据，有了这些元数据负载均衡器才能选择路由，也可以选择回退到预定义的主机集（包括任何主机）。 子集使用集群指定的负载均衡器策略。原始目的地策略不能与子集一起使用，因为上游主机预先不知道这些策略。子集与 zone 感知路由兼容，但请注意，子集的使用可能很容易违反上述最小主机条件。 如果已配置的子集路由未指定元数据或没有匹配元数据的子集存在，则子集负载均衡器将启动其回退策略。默认策略是NO_ENDPOINT，在这种情况下，请求失败，就好像该集群没有主机一样。相反，ANY_ENDPOINT后备策略会在集群中的所有主机上进行负载均衡，而不考虑主机元数据。最后，DEFAULT_SUBSET会导致回退到与特定元数据集匹配的主机之间进行负载均衡。 子集必须被预定义才能让子集负载均衡器有效地选择正确的主机子集。每个定义都是一组密钥，可以转换为零个或多个子集。从概念上讲，具有定义中所有键的元数据值的每个主机都会添加到其键-值对特定的子集。如果所有主机都不拥有密钥，那么定义不会产生子集。可以提供多个定义，并且如果单个主机与多个定义匹配，则可以在多个子集中出现。 在路由期间，路由的元数据匹配配置将用于查找特定的子集。如果存在具有路由指定的确切密钥和值的子集，则该子集将用于负载均衡。否则，使用回退策略。因此，集群的子集配置必须包含一个与给定路由具有相同密钥的定义才能实现子集负载均衡。 此功能只能在 V2 配置 API 中使用。此外，主机元数据仅在集群使用 EDS 发现类型时支持。子集负载均衡的主机元数据必须放在过滤器名称 \"envoy.lb\" 下。同样，路由元数据匹配条件使用 \"envoy.lb\" 过滤器名称。主机元数据可以是分层的（例如，顶级密钥的值可以是结构化值或列表），但子集负载均衡器仅比较顶级密钥和值。因此，在使用结构化值时，如果主机的元数据中出现相同的结构化值，则路由的匹配条件会匹配。 示例 我们将使用所有值都是字符串的简单元数据。假设定义了以下主机并将其与集群关联： 主机 元数据 host1 v: 1.0, stage: prod host2 v: 1.0, stage: prod host3 v: 1.1, stage: canary host4 v: 1.2-pre, stage: dev 集群可以像这样启用子集负载均衡： --- name: cluster-name type: EDS eds_cluster_config: eds_config: path: '.../eds.conf' connect_timeout: seconds: 10 lb_policy: LEAST_REQUEST lb_subset_config: fallback_policy: DEFAULT_SUBSET default_subset: stage: prod subset_selectors: - keys: - v - stage - keys: - stage 下表描述了一些路由及其对集群的应用结果。通常，匹配标准将与匹配请求的特定方面的路由一起使用，例如路径或 header 信息。 匹配标准 负载均衡位于 原因 stage: canary host3 所选主机的子集 v: 1.2-pre, stage: dev host4 所选主机的子集 v: 1.0 host1, host2 回退：没有仅具有 “v” 的子集选择器 other: x host1, host2 回退：没有 “other” 子集选择器 (none) host1, host2 回退：未请求子集 元数据匹配标准也可以在路由的加权集群上指定。来自选定加权集群的元数据匹配条件将与路由中的条件合并，并覆盖该原路由器中的条件： 路由匹配条件 加权集群匹配条件 最终匹配条件 stage: canary stage: prod stage: prod v: 1.0 stage: prod v: 1.0, stage: prod v: 1.0, stage: prod stage: canary v: 1.0, stage: canary v: 1.0, stage: prod v: 1.1, stage: canary v: 1.1, stage: canary (none) v: 1.0 v: 1.0 v: 1.0 (none) v: 1.0 具有元数据的示例主机 具有主机元数据的EDS LbEndpoint： --- endpoint: address: socket_address: protocol: TCP address: 127.0.0.1 port_value: 8888 metadata: filter_metadata: envoy.lb: version: '1.0' stage: 'prod' 具有元数据匹配标准的示例路由 具有元数据匹配标准的 RDS Route： --- match: prefix: / route: cluster: cluster-name metadata_match: filter_metadata: envoy.lb: version: '1.0' stage: 'prod' "},"intro/arch_overview/outlier.html":{"url":"intro/arch_overview/outlier.html","title":"异常点检测","keywords":"","body":"异常点检测 Outlier detection and ejection is the process of dynamically determining whether some number of hosts in an upstream cluster are performing unlike the others and removing them from the healthyload balancing set. Performance might be along different axes such as consecutive failures, temporal success rate, temporal latency, etc. Outlier detection is a form of passive health checking. Envoy also supports active health checking. Passive and active health checking can be enabled together or independently, and form the basis for an overall upstream health checking solution. 逐出算法 Depending on the type of outlier detection, ejection either runs inline (for example in the case of consecutive 5xx) or at a specified interval (for example in the case of periodic success rate). The ejection algorithm works as follows: A host is determined to be an outlier. If no hosts have been ejected, Envoy will eject the host immediately. Otherwise, it checks to make sure the number of ejected hosts is below the allowed threshold (specified via theoutlier_detection.max_ejection_percent setting). If the number of ejected hosts is above the threshold, the host is not ejected. The host is ejected for some number of milliseconds. Ejection means that the host is marked unhealthy and will not be used during load balancing unless the load balancer is in a panicscenario. The number of milliseconds is equal to the outlier_detection.base_ejection_time_msvalue multiplied by the number of times the host has been ejected. This causes hosts to get ejected for longer and longer periods if they continue to fail. An ejected host will automatically be brought back into service after the ejection time has been satisfied. Generally, outlier detection is used alongside active health checking for a comprehensive health checking solution. 检测类型 Envoy supports the following outlier detection types: Consecutive 5xx If an upstream host returns some number of consecutive 5xx, it will be ejected. Note that in this case a 5xx means an actual 5xx respond code, or an event that would cause the HTTP router to return one on the upstream’s behalf (reset, connection failure, etc.). The number of consecutive 5xx required for ejection is controlled by the outlier_detection.consecutive_5xx value. Consecutive Gateway Failure If an upstream host returns some number of consecutive “gateway errors” (502, 503 or 504 status code), it will be ejected. Note that this includes events that would cause the HTTP router to return one of these status codes on the upstream’s behalf (reset, connection failure, etc.). The number of consecutive gateway failures required for ejection is controlled by the outlier_detection.consecutive_gateway_failure value. Success Rate Success Rate based outlier ejection aggregates success rate data from every host in a cluster. Then at given intervals ejects hosts based on statistical outlier detection. Success Rate outlier ejection will not be calculated for a host if its request volume over the aggregation interval is less than theoutlier_detection.success_rate_request_volume value. Moreover, detection will not be performed for a cluster if the number of hosts with the minimum required request volume in an interval is less than the outlier_detection.success_rate_minimum_hosts value. 逐出事件记录 A log of outlier ejection events can optionally be produced by Envoy. This is extremely useful during daily operations since global stats do not provide enough information on which hosts are being ejected and for what reasons. The log uses a JSON format with one object per line: { \"time\": \"...\", \"secs_since_last_action\": \"...\", \"cluster\": \"...\", \"upstream_url\": \"...\", \"action\": \"...\", \"type\": \"...\", \"num_ejections\": \"...\", \"enforced\": \"...\", \"host_success_rate\": \"...\", \"cluster_success_rate_average\": \"...\", \"cluster_success_rate_ejection_threshold\": \"...\" } time The time that the event took place. secs_since_last_action The time in seconds since the last action (either an ejection or unejection) took place. This value will be -1 for the first ejection given there is no action before the first ejection. cluster The cluster that owns the ejected host. upstream_url The URL of the ejected host. E.g., tcp://1.2.3.4:80. action The action that took place. Either eject if a host was ejected or uneject if it was brought back into service. type If action is eject, specifies the type of ejection that took place. Currently type can be one of 5xx, GatewayFailure or SuccessRate. num_ejections If action is eject, specifies the number of times the host has been ejected (local to that Envoy and gets reset if the host gets removed from the upstream cluster for any reason and then re-added). enforced If action is eject, specifies if the ejection was enforced. true means the host was ejected.false means the event was logged but the host was not actually ejected. host_success_rate If action is eject, and type is SuccessRate, specifies the host’s success rate at the time of the ejection event on a 0-100 range. cluster_success_rate_average If action is eject, and type is SuccessRate, specifies the average success rate of the hosts in the cluster at the time of the ejection event on a 0-100 range. cluster_success_rate_ejection_threshold If action is eject, and type is SuccessRate, specifies success rate ejection threshold at the time of the ejection event. 参考配置 集群管理器全局配置 单集群配置 运行时设置 统计参考 "},"intro/arch_overview/circuit_breaking.html":{"url":"intro/arch_overview/circuit_breaking.html","title":"断路","keywords":"","body":"断路 断路（circuit breaking）是分布式系统的关键组成部分。尽快失败和向下游施加反向压力大体上会得到更好的效果。Envoy 网格的主要优点之一就是 Envoy 在网络级别强制实现断路限制，而不必独立配置和编写每个应用程序。Envoy 支持各种类型的完全分布式（非协调的）断路： 集群最大连接数：Envoy 将为上游集群中的所有主机建立的最大连接数。实际上，这仅适用于 HTTP/1.1集群，因为 HTTP/2 使用到每个主机的单个连接。 集群最大挂起请求数：在等待就绪连接池连接时将排队的最大请求数。实际上，这仅适用于 HTTP/1.1 集群，因为 HTTP/2 连接池不会排队请求。HTTP/2 请求会立即复用。如果该断路器溢出，则集群的upstream_rq_pending_overflowcounter 计数器将增加。 集群最大请求数：在任何给定时间内，集群中所有主机可以处理的最大请求数。实际上，这适用于仅 HTTP/2 集群，因为 HTTP/1.1 集群由最大连接断路器控制。如果该断路器溢出，集群的 upstream_rq_pending_overflow 计数器将递增。 集群最大活动重试次数：在任何给定时间内，集群中所有主机可以执行的最大重试次数。一般而言，我们建议积极进行断路重试，以便做零星故障重试而整体重试数量又不会爆炸式增加和导致大规模级联故障。如果该断路器溢出，集群的 upstream_rq_retry_overflow 计数器将递增。 断路限制可以根据每个上游集群和优先级进行配置和跟踪。这使得分布式系统的不同组件可以独立调整并具有不同的限制。 请注意，在 HTTP 请求的情况下，断路会导致路由器设置 x-envoy-overloaded 头。 "},"intro/arch_overview/global_rate_limiting.html":{"url":"intro/arch_overview/global_rate_limiting.html","title":"全局速率限制","keywords":"","body":"全局速率限制 Although distributed circuit breaking is generally extremely effective in controlling throughput in distributed systems, there are times when it is not very effective and global rate limiting is desired. The most common case is when a large number of hosts are forwarding to a small number of hosts and the average request latency is low (e.g., connections/requests to a database server). If the target hosts become backed up, the downstream hosts will overwhelm the upstream cluster. In this scenario it is extremely difficult to configure a tight enough circuit breaking limit on each downstream host such that the system will operate normally during typical request patterns but still prevent cascading failure when the system starts to fail. Global rate limiting is a good solution for this case. Envoy integrates directly with a global gRPC rate limiting service. Although any service that implements the defined RPC/IDL protocol can be used, Lyft provides a reference implementationwritten in Go which uses a Redis backend. Envoy’s rate limit integration has the following features: Network level rate limit filter: Envoy will call the rate limit service for every new connection on the listener where the filter is installed. The configuration specifies a specific domain and descriptor set to rate limit on. This has the ultimate effect of rate limiting the connections per second that transit the listener. Configuration reference. HTTP level rate limit filter: Envoy will call the rate limit service for every new request on the listener where the filter is installed and where the route table specifies that the global rate limit service should be called. All requests to the target upstream cluster as well as all requests from the originating cluster to the target cluster can be rate limited. Configuration reference Rate limit service configuration. "},"intro/arch_overview/ssl.html":{"url":"intro/arch_overview/ssl.html","title":"TLS","keywords":"","body":"TLS Envoy supports both TLS termination in listeners as well as TLS origination when making connections to upstream clusters. Support is sufficient for Envoy to perform standard edge proxy duties for modern web services as well as to initiate connections with external services that have advanced TLS requirements (TLS1.2, SNI, etc.). Envoy supports the following TLS features: Configurable ciphers: Each TLS listener and client can specify the ciphers that it supports. Client certificates: Upstream/client connections can present a client certificate in addition to server certificate verification. Certificate verification and pinning: Certificate verification options include basic chain verification, subject name verification, and hash pinning. Certificate revocation: Envoy can check peer certificates against a certificate revocation list (CRL) if one is provided. ALPN: TLS listeners support ALPN. The HTTP connection manager uses this information (in addition to protocol inference) to determine whether a client is speaking HTTP/1.1 or HTTP/2. SNI: SNI is supported for both server (listener) and client (upstream) connections. Session resumption: Server connections support resuming previous sessions via TLS session tickets (see RFC 5077). Resumption can be performed across hot restarts and between parallel Envoy instances (typically useful in a front proxy configuration). 底层实现 Currently Envoy is written to use BoringSSL as the TLS provider. 启用认证验证 Certificate verification of both upstream and downstream connections is not enabled unless the validation context specifies one or more trusted authority certificates. Example configuration static_resources: listeners: - name: listener_0 address: { socket_address: { address: 127.0.0.1, port_value: 10000 } } filter_chains: - filters: - name: envoy.http_connection_manager # ... tls_context: common_tls_context: validation_context: trusted_ca: filename: /usr/local/my-client-ca.crt clusters: - name: some_service connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: 127.0.0.2, port_value: 1234 }}] tls_context: common_tls_context: validation_context: trusted_ca: filename: /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt is the default path for the system CA bundle on Debian systems. This makes Envoy verify the server identity of 127.0.0.2:1234 in the same way as e.g. cURL does on standard Debian installations. Common paths for system CA bundles on Linux and BSD are /etc/ssl/certs/ca-certificates.crt (Debian/Ubuntu/Gentoo etc.) /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem (CentOS/RHEL 7) /etc/pki/tls/certs/ca-bundle.crt (Fedora/RHEL 6) /etc/ssl/ca-bundle.pem (OpenSUSE) /usr/local/etc/ssl/cert.pem (FreeBSD) /etc/ssl/cert.pem (OpenBSD) See the reference for UpstreamTlsContexts and DownstreamTlsContexts for other TLS options. 身份验证过滤器 Envoy provides a network filter that performs TLS client authentication via principals fetched from a REST VPN service. This filter matches the presented client certificate hash against the principal list to determine whether the connection should be allowed or not. Optional IP white listing can also be configured. This functionality can be used to build edge proxy VPN support for web infrastructure. Client TLS authentication filter configuration reference. "},"intro/arch_overview/statistics.html":{"url":"intro/arch_overview/statistics.html","title":"统计","keywords":"","body":"统计 One of the primary goals of Envoy is to make the network understandable. Envoy emits a large number of statistics depending on how it is configured. Generally the statistics fall into three categories: Downstream: Downstream statistics relate to incoming connections/requests. They are emitted by listeners, the HTTP connection manager, the TCP proxy filter, etc. Upstream: Upstream statistics relate to outgoing connections/requests. They are emitted by connection pools, the router filter, the TCP proxy filter, etc. Server: Server statistics describe how the Envoy server instance is working. Statistics like server uptime or amount of allocated memory are categorized here. A single proxy scenario typically involves both downstream and upstream statistics. The two types can be used to get a detailed picture of that particular network hop. Statistics from the entire mesh give a very detailed picture of each hop and overall network health. The statistics emitted are documented in detail in the operations guide. In the v1 API, Envoy only supports statsd as the statistics output format. Both TCP and UDP statsd are supported. As of the v2 API, Envoy has the ability to support custom, pluggable sinks. A few standard sink implementations are included in Envoy. Some sinks also support emitting statistics with tags/dimensions. Within Envoy and throughout the documentation, statistics are identified by a canonical string representation. The dynamic portions of these strings are stripped to become tags. Users can configure this behavior via the Tag Specifier configuration. Envoy emits three types of values as statistics: Counters: Unsigned integers that only increase and never decrease. E.g., total requests. Gauges: Unsigned integers that both increase and decrease. E.g., currently active requests. Histograms: Unsigned integers that are part of a stream of values that are then aggregated by the collector to ultimately yield summarized percentile values. E.g., upstream request time. Internally, counters and gauges are batched and periodically flushed to improve performance. Histograms are written as they are received. Note: what were previously referred to as timers have become histograms as the only difference between the two representations was the units. v1 API reference. v2 API reference. "},"intro/arch_overview/runtime.html":{"url":"intro/arch_overview/runtime.html","title":"运行时配置","keywords":"","body":"运行时配置 Envoy 支持“运行时”配置（也称为“功能标志”和“决策者”）。配置设置可以更改，这将影响运算而无需重新启动 Envoy 或更改主配置。当前支持的实现使用文件系统树。Envoy 监视配置目录中的符号链接交换，并在发生这种情况时重新加载树。这种类型的系统通常在大型分布式系统中部署。其他实现也并不难。支持的运行时配置设置记录在操作指南的相关章节。使用默认的运行时值和“null”提供者，Envoy 也能正确运行，因此不需要存在一个用于运行 Envoy 的系统。 运行时配置。 "},"intro/arch_overview/tracing.html":{"url":"intro/arch_overview/tracing.html","title":"追踪","keywords":"","body":"追踪 概览 Distributed tracing allows developers to obtain visualizations of call flows in large service oriented architectures. It can be invaluable in understanding serialization, parallelism, and sources of latency. Envoy supports three features related to system wide tracing: Request ID generation: Envoy will generate UUIDs when needed and populate the x-request-idHTTP header. Applications can forward the x-request-id header for unified logging as well as tracing. External trace service integration: Envoy supports pluggable external trace visualization providers. Currently Envoy supports LightStep, Zipkin or any Zipkin compatible backends (e.g. Jaeger). However, support for other tracing providers would not be difficult to add. Client trace ID joining: The x-client-trace-id header can be used to join untrusted request IDs to the trusted internal x-request-id. 如何初始化追踪 The HTTP connection manager that handles the request must have the tracing object set. There are several ways tracing can be initiated: By an external client via the x-client-trace-id header. By an internal service via the x-envoy-force-trace header. Randomly sampled via the random_sampling runtime setting. The router filter is also capable of creating a child span for egress calls via the start_child_spanoption. 追踪上下文传播 Envoy provides the capability for reporting tracing information regarding communications between services in the mesh. However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests. Whichever tracing provider is being used, the service should propagate the x-request-id to enable logging across the invoked services to be correlated. The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood. This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests. This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace. Alternatively the trace context can be manually propagated by the service: When using the LightStep tracer, Envoy relies on the service to propagate the x-ot-span-contextHTTP header while sending HTTP requests to other services. When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, and x-b3-flags). The x-b3-sampledheader can also be supplied by an external client to either enable or disable tracing for a particular request. 每条追踪中包含哪些数据 An end-to-end trace is comprised of one or more spans. A span represents a logical unit of work that has a start time and duration and can contain metadata associated with it. Each span generated by Envoy contains the following data: Originating service cluster set via --service-cluster. Start time and duration of the request. Originating host set via --service-node. Downstream cluster set via the x-envoy-downstream-service-cluster header. HTTP URL. HTTP method. HTTP response code. Tracing system-specific metadata. The span also includes a name (or operation) which by default is defined as the host of the invoked service. However this can be customized using a Decorator on the route. The name can also be overridden using the x-envoy-decorator-operation header. Envoy automatically sends spans to tracing collectors. Depending on the tracing collector, multiple spans are stitched together using common information such as the globally unique request ID x-request-id (LightStep) or the trace ID configuration (Zipkin). See v1 API reference v2 API reference for more information on how to setup tracing in Envoy. "},"intro/arch_overview/tcp_proxy.html":{"url":"intro/arch_overview/tcp_proxy.html","title":"TCP 代理","keywords":"","body":"TCP 代理 Envoy 本质上是一个 L3/L4 服务器，实现 L3/L4 的代理功能是一件轻而易举的事情。TCP 代理过滤器可以在下游的客户端与上游的集群间实现最基本的 1:1 网络连接代理功能。 这可以用来做一个安全通道的替代品，或者与其他的过滤器聚合如 MongoDB 过滤器或速率限制过滤器。 TCP 代理过滤器会被上游集群全局资源管理器中使用的连接限制约束。 TCP 代理过滤器需要和上游集群的资源管理器协商是否能在不逾越集群最大连接数的前提下，建立一个新连接，如果答案为否则 TCP 代理将不可以建立新连接。 TCP 代理过滤器参考配置。 "},"intro/arch_overview/access_logging.html":{"url":"intro/arch_overview/access_logging.html","title":"访问记录","keywords":"","body":"访问记录 HTTP 连接管理与 tcp 代理支持可扩展的访问记录，并拥有以下特性: 可按每个 HTTP 连接管理或 tcp 代理记录任何数量的访问记录。 异步 IO 架构。访问记录将永远不会阻塞主要的网络处理线程。 可定制化的访问记录格式,可使用预制定的字段,更可使用任意的 HTTP request 以及 response。 可定制化的访问记录过滤器,可允许不同类型的请求以及回复写入至不同的访问记录。 访问记录 配置。 "},"intro/arch_overview/mongo.html":{"url":"intro/arch_overview/mongo.html","title":"MongoDB","keywords":"","body":"MongoDB Envoy 支持网络层的 MongoDB 嗅探过滤器,并拥有如下特性： MongoDB wire 格式的 BSON 转换器。 考虑周全的 MongoDB 查询/操作统计分析报告，包括了时间以及路由集群中的散列/多次获取次数。 查询记录。 通过 $comment 参数做每个调用点的统计分析报告。 故障注入。 MongoDB 过滤器是表现 Envoy 扩展性以及核心抽象能力的典范案例。在 Lyft 我们将这个过滤器应用在所有的应用以及数据库中。 它提供了对应用程序平台和正在使用的特定 MongoDB 驱动程序不可知的重要数据源。 MongoDB 代理过滤器参考配置。 "},"intro/arch_overview/dynamo.html":{"url":"intro/arch_overview/dynamo.html","title":"DynamoDB","keywords":"","body":"DynamoDB Envoy supports an HTTP level DynamoDB sniffing filter with the following features: DynamoDB API request/response parser. DynamoDB per operation/per table/per partition and operation statistics. Failure type statistics for 4xx responses, parsed from response JSON, e.g., ProvisionedThroughputExceededException. Batch operation partial failure statistics. The DynamoDB filter is a good example of Envoy’s extensibility and core abstractions at the HTTP layer. At Lyft we use this filter for all application communication with DynamoDB. It provides an invaluable source of data agnostic to the application platform and specific AWS SDK in use. DynamoDB filter configuration. "},"intro/arch_overview/redis.html":{"url":"intro/arch_overview/redis.html","title":"Redis","keywords":"","body":"Redis Envoy can act as a Redis proxy, partitioning commands among instances in a cluster. In this mode, the goals of Envoy are to maintain availability and partition tolerance over consistency. This is the key point when comparing Envoy to Redis Cluster. Envoy is designed as a best-effort cache, meaning that it will not try to reconcile inconsistent data or keep a globally consistent view of cluster membership. The Redis project offers a thorough reference on partitioning as it relates to Redis. See “Partitioning: how to split data among multiple Redis instances”. Features of Envoy Redis: Redis protocol codec. Hash-based partitioning. Ketama distribution. Detailed command statistics. Active and passive healthchecking. Planned future enhancements: Additional timing stats. Circuit breaking. Request collapsing for fragmented commands. Replication. Built-in retry. Tracing. Hash tagging. 配置 For filter configuration details, see the Redis proxy filter configuration reference. The corresponding cluster definition should be configured with ring hash load balancing. If active healthchecking is desired, the cluster should be configured with a Redis healthcheck. If passive healthchecking is desired, also configure outlier detection. For the purposes of passive healthchecking, connect timeouts, command timeouts, and connection close map to 5xx. All other responses from Redis are counted as a success. 支持的命令 At the protocol level, pipelines are supported. MULTI (transaction block) is not. Use pipelining wherever possible for the best performance. At the command level, Envoy only supports commands that can be reliably hashed to a server. PING is the only exception, which Envoy responds to immediately with PONG. Arguments to PING are not allowed. All other supported commands must contain a key. Supported commands are functionally identical to the original Redis command except possibly in failure scenarios. For details on each command’s usage see the official Redis command reference. Command Group PING Connection DEL Generic DUMP Generic EXISTS Generic EXPIRE Generic EXPIREAT Generic PERSIST Generic PEXPIRE Generic PEXPIREAT Generic PTTL Generic RESTORE Generic TOUCH Generic TTL Generic TYPE Generic UNLINK Generic GEOADD Geo GEODIST Geo GEOHASH Geo GEOPOS Geo GEORADIUS_RO Geo GEORADIUSBYMEMBER_RO Geo HDEL Hash HEXISTS Hash HGET Hash HGETALL Hash HINCRBY Hash HINCRBYFLOAT Hash HKEYS Hash HLEN Hash HMGET Hash HMSET Hash HSCAN Hash HSET Hash HSETNX Hash HSTRLEN Hash HVALS Hash LINDEX List LINSERT List LLEN List LPOP List LPUSH List LPUSHX List LRANGE List LREM List LSET List LTRIM List RPOP List RPUSH List RPUSHX List EVAL Scripting EVALSHA Scripting SADD Set SCARD Set SISMEMBER Set SMEMBERS Set SPOP Set SRANDMEMBER Set SREM Set SSCAN Set ZADD Sorted Set ZCARD Sorted Set ZCOUNT Sorted Set ZINCRBY Sorted Set ZLEXCOUNT Sorted Set ZRANGE Sorted Set ZRANGEBYLEX Sorted Set ZRANGEBYSCORE Sorted Set ZRANK Sorted Set ZREM Sorted Set ZREMRANGEBYLEX Sorted Set ZREMRANGEBYRANK Sorted Set ZREMRANGEBYSCORE Sorted Set ZREVRANGE Sorted Set ZREVRANGEBYLEX Sorted Set ZREVRANGEBYSCORE Sorted Set ZREVRANK Sorted Set ZSCAN Sorted Set ZSCORE Sorted Set APPEND String BITCOUNT String BITFIELD String BITPOS String DECR String DECRBY String GET String GETBIT String GETRANGE String GETSET String INCR String INCRBY String INCRBYFLOAT String MGET String MSET String PSETEX String SET String SETBIT String SETEX String SETNX String SETRANGE String STRLEN String 失败模式 If Redis throws an error, we pass that error along as the response to the command. Envoy treats a response from Redis with the error datatype as a normal response and passes it through to the caller. Envoy can also generate its own errors in response to the client. Error Meaning no upstream host The ring hash load balancer did not have a healthy host available at the ring position chosen for the key. upstream failure The backend did not respond within the timeout period or closed the connection. invalid request Command was rejected by the first stage of the command splitter due to datatype or length. unsupported command The command was not recognized by Envoy and therefore cannot be serviced because it cannot be hashed to a backend server. finished with n errors Fragmented commands which sum the response (e.g. DEL) will return the total number of errors received if any were received. upstream protocol error A fragmented command received an unexpected datatype or a backend responded with a response that not conform to the Redis protocol. wrong number of arguments for command Certain commands check in Envoy that the number of arguments is correct. In the case of MGET, each individual key that cannot be fetched will generate an error response. For example, if we fetch five keys and two of the keys’ backends time out, we would get an error response for each in place of the value. $ redis-cli MGET a b c d e 1) \"alpha\" 2) \"bravo\" 3) (error) upstream failure 4) (error) upstream failure 5) \"echo\" "},"intro/arch_overview/hot_restart.html":{"url":"intro/arch_overview/hot_restart.html","title":"热重启","keywords":"","body":"热重启 Ease of operation is one of the primary goals of Envoy. In addition to robust statistics and a local administration interface, Envoy has the ability to “hot” or “live” restart itself. This means that Envoy can fully reload itself (both code and configuration) without dropping any connections. The hot restart functionality has the following general architecture: Statistics and some locks are kept in a shared memory region. This means that gauges will be consistent across both processes as restart is taking place. The two active processes communicate with each other over unix domain sockets using a basic RPC protocol. The new process fully initializes itself (loads the configuration, does an initial service discovery and health checking phase, etc.) before it asks for copies of the listen sockets from the old process. The new process starts listening and then tells the old process to start draining. During the draining phase, the old process attempts to gracefully close existing connections. How this is done depends on the configured filters. The drain time is configurable via the--drain-time-s option and as more time passes draining becomes more aggressive. After drain sequence, the new Envoy process tells the old Envoy process to shut itself down. This time is configurable via the --parent-shutdown-time-s option. Envoy’s hot restart support was designed so that it will work correctly even if the new Envoy process and the old Envoy process are running inside different containers. Communication between the processes takes place only using unix domain sockets. An example restarter/parent process written in Python is included in the source distribution. This parent process is usable with standard process control utilities such as monit/runit/etc. "},"intro/arch_overview/dynamic_configuration.html":{"url":"intro/arch_overview/dynamic_configuration.html","title":"动态配置","keywords":"","body":"动态配置 Envoy 的架构使得使用不同类型的配置管理方法成为可能。部署中采用的方法将取决于实现者的需求。简单部署可以通过全静态配置来实现。更复杂的部署可以递增地添加更复杂的动态配置，缺点是实现者必须提供一个或多个基于外部 REST 配置的提供者 API。本文档概述了当前可用的选项。 顶级配置参考 参考配置 Envoy v2 API 概述. 全静态 在全静态配置中，实现者提供一组监听器 (和过滤器链)、 集群和可选的 HTTP 路由配置。动态主机发现仅能通过基 于DNS 的服务发现。配置重载必须通过内置的热重启机制进行。 虽然简单，但可以使用静态配置和优雅的热重启来创建相当复杂的部署。 仅 SDS/EDS 服务发现服务（SDS）API 提供更高级的机制，Envoy 可以通过该机制发现上游集群中的成员。SDS 已在 v2 API 中重命名为 Endpoint Discovery Service（EDS）。 在静态配置之上，SDS 允许 Envoy 部署规避 DNS 的局限性（响应中的最大记录等），并使用更多可用于负载均衡和路由的信息（例如，金丝雀状态、区域等）。 SDS/EDS 和 CDS 集群发现服务 (CDS) API 是 Envoy 的一种机制，在路由期间可以用来发现使用的上游集群。Envoy 将优雅地添加、更新和删除由 API 指定的集群。该API允许实现者构建拓扑，在其中 Envoy 在初始配置时不需要知道所有上游群集。通常，在与 CDS（但没有路由发现服务）一起进行 HTTP 路由时，实现者将利用路由器的能力将请求转发到在 HTTP 请求头中指定的集群。 尽管可以通过指定全静态集群来使用不带 SDS/EDS 的 CDS，但我们仍建议为通过 CDS 指定的集群使用 SDS/EDS API。 在内部，更新集群定义时，操作是优雅的。但是，所有现有的连接池都将被排空并重新连接。SDS/EDS 不受此限制。当通过 SDS/EDS 添加和删除主机时，集群中的现有主机不受影响。 SDS/EDS、CDS 和 RDS 路由发现服务 (RDS) API 是 Envoy 的一种机制，可以在运行时发现用于HTTP连接管理器过滤器的整个路由配置。路由配置将优雅地交换，而不会影响现有的请求。这个 API 与 SDS/EDS 和 CDS 一起使用时，允许实现者构建复杂的路由拓扑（流量转移、蓝绿部署等），除了获取新的 Envoy 二进制文件外，不需要重启 Envoy。 SDS/EDS、CDS、RDS 和 LDS 监听器发现服务 (LDS) 是 Envoy 的一种机制，可以在运行时发现整个监听器。这包括所有的过滤器堆栈，直到并包括带有内嵌到 RDS 的应用的 HTTP 过滤器。将 LDS 添加到混和中，几乎可以动态配置 Envoy 的每个方面。只有非常少见的配置更改（管理员、追踪驱动程序等）或二进制更新时才需要热启动。 "},"intro/arch_overview/init.html":{"url":"intro/arch_overview/init.html","title":"初始化","keywords":"","body":"初始化 Envoy 在启动时的初始化是很复杂的。本章将在高级别解释这个过程是如何工作的。以下会在任何监听器启动监听并接收新连接前发生。 启动期间，集群管理器 会首先进行多阶段初始化，首先初始化静态/ DNS 集群，然后是预定义的 SDS 集群. 然后，如果适用，它会初始化 CDS ， 等待响应（或失败）， 并为 CDS 提供的集群执行相同主/次初始化。 如果集群使用 主动健康检查 ，Envoy也会执行单个主动HC轮次。 集群管理器初始化完成后， RDS 和 LDS 将初始化（如果适用）。在LDS / RDS请求至少有一次响应（或失败）之后，服务器才开始接受连接。 如果 LDS 本身返回需要 RDS 响应的监听器，则Envoy会进一步等待，直到收到 RDS 响应（或失败）。请注意，这个过程发生在未来的每个通过 LDS 添加的监听器上，并且被称为 监听器热身。 在先前所有的步骤发生之后，监听器开始接受新的连接。该流程可确保在热启动期间新流程完全能够在旧流程排除之前接受并处理新连接。 "},"intro/arch_overview/draining.html":{"url":"intro/arch_overview/draining.html","title":"排除","keywords":"","body":"排除 排除是 Envoy 相应各种事件的要求试图优雅地关闭连接的过程。排除发生在以下时机： 服务器通过 healthcheck/fail 管理端点手动设置健康状况检查失败。有关更多信息，请参阅 健康检查过滤器 架构概述。 服务器 热重启。 通过 LDS 修改或者移除单个监听器。 每个 已配置监听器 有 drain_type 设置，用来控制排除何时发生。目前支持的值有： default 对于所有上述三种情况（管理排除，热重启和 LDS 更新/删除），Envoy 将排除监听器。这是默认设置。 modify_only Envoy 只会响应上述第二和第三种情况（热重启和 LDS 更新/删除）而排除监听器。如果 Envoy 同时托管 ingress 和 egress 监听器，此设置很有用。需要在 egress 监听器上设置 modify_only，以便在尝试执行受控关闭，依赖 ingress 监听器排除来执行全服务器排除时，它们只在修改期间排除。 请注意，虽然排除是每监听器的概念，但它必须在网络过滤器级别被支持。目前支持优雅排除的过滤器只有 HTTP 连接管理器, Redis, 和 Mongo。 "},"intro/arch_overview/scripting.html":{"url":"intro/arch_overview/scripting.html","title":"脚本","keywords":"","body":"脚本 Envoy试验性的支持 Lua 脚本作为专用 HTTP 过滤器 的一部分。 "},"intro/deployment_types/deployment_types.html":{"url":"intro/deployment_types/deployment_types.html","title":"部署类型","keywords":"","body":"部署类型 Envoy 有多种使用场景，其中更多情况下会作为 mesh 被部署在基础设施的不同主机上。本章节将按照复杂性上升的顺序讨论 envoy 的三种推荐部署类型。 仅服务之间 Service to service egress listener Service to service ingress listener Optional external service egress listeners Discovery service integration Configuration template 服务见外加前端代理 Configuration template 服务间、前端代理、双向代理 Configuration template "},"intro/deployment_types/service_to_service.html":{"url":"intro/deployment_types/service_to_service.html","title":"仅服务之间","keywords":"","body":"仅服务之间 The above diagram shows the simplest Envoy deployment which uses Envoy as a communication bus for all traffic internal to a service oriented architecture (SOA). In this scenario, Envoy exposes several listeners that are used for local origin traffic as well as service to service traffic. 服务间 egress listener This is the port used by applications to talk to other services in the infrastructure. For example,http://localhost:9001. HTTP and gRPC requests use the HTTP/1.1 host header or the HTTP/2:authority header to indicate which remote cluster the request is destined for. Envoy handles service discovery, load balancing, rate limiting, etc. depending on the details in the configuration. Services only need to know about the local Envoy and do not need to concern themselves with network topology, whether they are running in development or production, etc. This listener supports both HTTP/1.1 or HTTP/2 depending on the capabilities of the application. 服务间 ingress listener This is the port used by remote Envoys when they want to talk to the local Envoy. For example,http://localhost:9211. Incoming requests are routed to the local service on the configured port(s). Multiple application ports may be involved depending on application or load balancing needs (for example if the service needs both an HTTP port and a gRPC port). The local Envoy performs buffering, circuit breaking, etc. as needed. Our default configurations use HTTP/2 for all Envoy to Envoy communication, regardless of whether the application uses HTTP/1.1 or HTTP/2 when egressing out of a local Envoy. HTTP/2 provides better performance via long lived connections and explicit reset notifications. 可选外部服务 egress listener Generally, an explicit egress port is used for each external service that a local service wants to talk to. This is done because some external service SDKs do not easily support overriding the hostheader to allow for standard HTTP reverse proxy behavior. For example, http://localhost:9250might be allocated for connections destined for DynamoDB. Instead of using host routing for some external services and dedicated local port routing for others, we recommend being consistent and using local port routing for all external services. 服务发现集成 The recommended service to service configuration uses an external discovery service for all cluster lookups. This provides Envoy with the most detailed information possible for use when performing load balancing, statistics gathering, etc. 配置模板 The source distribution includes an example service to service configuration that is very similar to the version that Lyft runs in production. See here for more information. "},"intro/deployment_types/front_proxy.html":{"url":"intro/deployment_types/front_proxy.html","title":"服务之间外加前端代理","keywords":"","body":"服务间外加前端代理 The above diagram shows the service to service configuration sitting behind an Envoy cluster used as an HTTP L7 edge reverse proxy. The reverse proxy provides the following features: Terminates TLS. Supports both HTTP/1.1 and HTTP/2. Full HTTP L7 routing support. Talks to the service to service Envoy clusters via the standard ingress port and using the discovery service for host lookup. Thus, the front Envoy hosts work identically to any other Envoy host, other than the fact that they do not run collocated with another service. This means that are operated in the same way and emit the same statistics. 配置模板 The source distribution includes an example front proxy configuration that is very similar to the version that Lyft runs in production. See here for more information. "},"intro/deployment_types/double_proxy.html":{"url":"intro/deployment_types/double_proxy.html","title":"服务间、前端代理、双向代理","keywords":"","body":"服务间、前端代理、双向代理 The above diagram shows the front proxy configuration alongside another Envoy cluster running as a double proxy. The idea behind the double proxy is that it is more efficient to terminate TLS and client connections as close as possible to the user (shorter round trip times for the TLS handshake, faster TCP CWND expansion, less chance for packet loss, etc.). Connections that terminate in the double proxy are then multiplexed onto long lived HTTP/2 connections running in the main data center. In the above diagram, the front Envoy proxy running in region 1 authenticates itself with the front Envoy proxy running in region 2 via TLS mutual authentication and pinned certificates. This allows the front Envoy instances running in region 2 to trust elements of the incoming requests that ordinarily would not be trustable (such as the x-forwarded-for HTTP header). Configuration template The source distribution includes an example double proxy configuration that is very similar to the version that Lyft runs in production. See here for more information. "},"intro/comparison.html":{"url":"intro/comparison.html","title":"与类似系统比较","keywords":"","body":"与类似系统比较 一言概之，我们相信 Envoy 有一套独有特色的优秀的功能集合，以支撑现代面向服务的架构。 以下我们将以 Envoy 与类似的其余系统进行比对。 虽然在某些特定领域(例如边缘代理，软负载均衡，服务消息传送层)，Envoy 未如以下的某些系统提供更加完备的支持，但在完整比对后，没有其他任何一个系统 能提供一套例如Envoy一般拥有完备功能，自包含，以及高性能的解决方案。 NOTE: 以下的许多项目还处在一个开发活跃期，我们所描述的信息也许会与项目最新的状态脱节，如果您发现这些情况，请立即告知我们，我们将会进行修正。 nginx nginx是一个经典的现代web服务器。 它支持静态内容展现， HTTP L7反向代理 负载均衡，HTTP/2，以及其他的许多特性。 作为一个边缘反向代理，nginx提供了远远多于 Envoy 的功能特性， 但我们认为大多数的现代面向服务的架构其实不需要用到那么多的特性。 而 Envoy 在下列边缘代理的特性做得比 nginx 更为出色: 完备的HTTP/2 透明代理支持。 Envoy支持HTTP/2 包括上游连接以及下游连接在内的双向通信。 而 nginx 仅仅支持HTTP/2 下游连接。 免费的高级负载功能。 而在 nginx 的世界，只有付费的 nginx plus 服务器才能提供类同于 Envoy 的高级负载功能。 可以在每一个服务节点的边界运行同样一套软件来处理事务。 在许多架构体系中，需要使用 nginx 与 haproxy 的混合部署架构。 相比之下，一个独立的代理解决方案会更有利于后续的运维维护。 haproxy haproxy 是一个经典的现代软负载均衡服务器。它提供基本的 HTTP 反向代理功能。 而 Envoy 在下列负载均衡的特性做得比 haproxy 更为出色： HTTP/2 支持。 可插拔架构。 与远程服务发现服务的整合。 与远程全局限速服务的整合。 提供大量的更为细致的统计分析。 AWS ELB Amazon 的 ELB 为在 Amazon EC2 上运行的程序提供服务发现以及负载均衡服务。而 Envoy 在下列负载均衡以及服务发现的特性做得比 ELB 更为出色: 统计与日志 (CloudWatch 的统计有延迟，并且在一些细节上严重缺乏，而 Amazon 的日志只能在 S3 以特定格式获取) 稳定性(使用 ELB 时不时会遇到不稳定事故，并且难以进行 debug 排查问题) 更为高级的负载均衡以及节点间的直连功能。 Envoy mesh 在进行硬件弹性伸缩时并不需要额外的网络跳转。 负载均衡可以根据区域，金丝雀状态的回馈提供更好的决策以及收集到许多有意思的统计分析结果。 而且负载均衡还支持例如重试这样的高级特性。 AWS最近发布了一个 application load balancer 产品。 这个产品增加了 HTTP/2 支持，可以将 HTTP/2 与基本的 HTTP L7 请求一般流转到不同的后端集群。 相比Envoy，这个产品所能提供的功能偏少，且性能与稳定性还未知，但不容置疑的是AWS将会在这个领域持续地进行研发。 SmartStack SmartStack 这个方案特别有意思，它在 haproxy 的基础上提供了更多的服务发现以及健康检查支持。 抽象地说，SmartStack 在大多数目标上与 Envoy 保持一致 (与进程无关的架构，对应用平台的不可知性等)。 而Envoy在下列负载均衡以及服务发现的特性做得比 SmartStack 更为出色: 上述所提及的做的比haproxy好的所有特性。 整合服务发现与积极的健康检查。 Envoy 将所有的功能都在一个高性能的方案内完整提供。 Finagle Finagle 是 Twitter 基于 Scala/JVM 开发的服务通讯库。 在Twitter 以及其他公司的基于 JVM 的架构中普遍使用。 它提供了服务发现，负载均衡，过滤器等 Envoy 也具有的功能。 而Envoy 在下列负载均衡以及服务发现的特性做得比 Finagle 更为出色: 最终一致的服务发现，基于在分布式的积极健康检查基础上实现 在各个维度上都有更优越的性能表现(内存占用率，CPU使用率，P99 时延) 与进程无关的架构，对应用平台的不可知性的架构。 Envoy与不同的应用技术栈都可一起工作。 proxygen 和 wangle proxygen是 Facebook 使用C+11 开发的高性能 HTTP 代理库， 是基于一个类同 Finagle 的 C++ 库 wangle 进行开发。 在代码实现层面，Envoy使用了类同的技术去实现一个高性能的的HTTP 库/代理。 除此之外，两个项目不具备可比性。 Envoy是一个完备的自包含服务，提供了大量的功能点。相比之下，proxygen 仅仅是一个库，各个项目还需要基于它继续构建功能。 gRPC gRPC是一个由 google 研发的跨平台的消息传送系统。 它使用 IDL 去描述 RPC 库，然后基于 IDL 为各种不同的编程语言生成不同的应用运行时。它底层基于HTTP/2去实现传输层。gRPC 似乎有一个终极的目标，在将来去实现许多 Enovy 的功能点(例如负载均衡)，但在我们写就这篇文档的时候，还有不少的运行时库还不成熟并且集中在序列以及反序列化上。我们认为 gRPC 是 Envoy 的搭档，而不是竞争者。 Envoy如何与gRPC进行整合，您可以查看此链接。 linkerd linkerd是一个独立的，开源的RPC路由代理，它基于 Netty 与 Finagle(Scala/JVM) 研发。 linkerd 提供了许多 Finagle 的特性，包括对延时敏感的负载均衡，连接池，断路器，重试预算，截至线，追踪，细粒度的植入， 以及对请求的流量路由层。linkerd提供一个可插拔的服务发现层 (Consul以及Zookeeper为此提供标准支持，就如 Marathon 以及Kubernetes 的API)。 linkerd的内存消耗以及CPU的要求都远远高于Envoy。 对比Enovy，linkerd仅提供了一个最小可用的配置语言， 且不支持热加载，而是用动态配置以及服务抽象的方式变相地 提供类似的功能。 linkerd支持HTTP/1。1， Thrift， ThriftMux， HTTP/2 (experimental) 和 gRPC (experimental)。 nghttp2 nghttp2 是一个包含了不同事物的项目。 首先，它包含了一个库 (nghttp2)，用来实现HTTP/2协议。Envoy使用这个库(在这个库上做了一层很浅的封装)来做HTTP/2的支持。 这个项目还包含了一个非常有用的压力测试工具(h2load) 以及一个反向代理 (nghttpx)。 如果要做个比对， 在nghttp2 所实现的众多功能点中，我们觉得 Envoy更类似于 nghttpx ， nghttpx是一个透明的 HTTP/1 HTTP/2 反向代理， 支持 TLS 终止， 支持gRPC代理。 我们认为 nghttpx 是一个表现不同代理功能点的杰出范例，而并不是一个健壮的生产可用的解决方案。Envoy的产品焦点更多地放在可监控性，操作的敏捷性，以及高级的负载均衡功能。 "},"intro/getting_help.html":{"url":"intro/getting_help.html","title":"获取帮助","keywords":"","body":"获取帮助 我们非常有兴致地正在建立一个围绕 Envoy 产品的技术社区。如果您在使用 Envoy 上需要帮助或者是您想为社区做出贡献，请尝试联系我们。 请查看 联系信息。 报告安全性缺陷 请查看 安全联系信息。 "},"intro/version_history.html":{"url":"intro/version_history.html","title":"历史版本","keywords":"","body":"历史版本 1.7.0 (Pending) access log: ability to log response trailers access log: ability to format START_TIME access log: added DYNAMIC_METADATA access log formatter. access log: added HeaderFilter to filter logs based on request headers admin: added GET /config_dump for dumping the current configuration and associated xDS version information (if applicable). admin: added GET /stats/prometheus as an alternative endpoint for getting stats in prometheus format. admin: added /runtime_modify endpoint to add or change runtime values admin: mutations must be sent as POSTs, rather than GETs. Mutations include:POST /cpuprofiler, POST /healthcheck/fail, POST /healthcheck/ok, POST /logging, POST /quitquitquit, POST /reset_counters,POST /runtime_modify?key1=value1&key2=value2&keyN=valueN, admin: removed /routes endpoint; route configs can now be found at the /config_dump endpoint. buffer filter: the buffer filter can be optionally disabled or overridden with route-local configuration. cli: added –config-yaml flag to the Envoy binary. When set its value is interpreted as a yaml representation of the bootstrap config and overrides –config-path. cluster: Add option to close tcp_proxy upstream connections when health checks fail. cluster: Add option to drain connections from hosts after they are removed from service discovery, regardless of health status. cluster: fixed bug preventing the deletion of all endpoints in a priority health check: added ability to set additional HTTP headers for HTTP health check. health check: added support for EDS delivered endpoint health status. health check: added interval overrides for health state transitions from healthy to unhealthy, unhealthy to healthy and for subsequent checks on unhealthy hosts. health check http filter: added generic header matching to trigger health check response. Deprecated the endpoint option. health check: added support for custom health check. http: filters can now optionally support virtual host, route, and weighted cluster local configuration. http: added the ability to pass DNS type Subject Alternative Names of the client certificate in the x-forwarded-client-cert header. listeners: added tcp_fast_open_queue_length option. load balancing: added weighted round robin support. The round robin scheduler now respects endpoint weights and also has improved fidelity across picks. load balancer: Locality weighted load balancing is now supported. load balancer: ability to configure zone aware load balancer settings through the API logger: added the ability to optionally set the log format via the --log-format option. logger: all logging levels can be configured at run-time: trace debug info warning error critical. sockets: added capture transport socket extension to support recording plain text traffic and PCAP generation. sockets: added IP_FREEBIND socket option support for listeners and upstream connections viacluster manager wide and cluster specific options. sockets: added IP_TRANSPARENT socket option support for listeners. sockets: added SO_KEEPALIVE socket option for upstream connections per cluster. stats: added support for histograms. stats: added option to configure the statsd prefix tls: removed support for legacy SHA-2 CBC cipher suites. tracing: the sampling decision is now delegated to the tracers, allowing the tracer to decide when and if to use it. For example, if the x-b3-sampled header is supplied with the client request, its value will override any sampling decision made by the Envoy proxy. websocket: support configuring idle_timeout and max_connect_attempts. 1.6.0 (March 20, 2018) access log: added DOWNSTREAM_REMOTE_ADDRESS, DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT, and DOWNSTREAM_LOCAL_ADDRESS access log formatters. DOWNSTREAM_ADDRESS access log formatter has been deprecated. access log: added less than or equal (LE) comparison filter. access log: added configuration to runtime filter to set default sampling rate, divisor, and whether to use independent randomness or not. admin: added /runtime admin endpoint to read the current runtime values. build: added support for building Envoy with exported symbols. This change allows scripts loaded with the Lua filter to load shared object libraries such as those installed via LuaRocks. config: added support for sending error details as grpc.rpc.Status in DiscoveryRequest. config: added support for inline delivery of TLS certificates and private keys. config: added restrictions for the backing config sources of xDS resources. For filesystem based xDS the file must exist at configuration time. For cluster based xDS the backing cluster must be statically defined and be of non-EDS type. grpc: the Google gRPC C++ library client is now supported as specified in the gRPC services overview and GrpcService. grpc-json: Added support for inline descriptors. health check: added gRPC health check based on grpc.health.v1.Health service. health check: added ability to set host header value for http health check. health check: extended the health check filter to support computation of the health check response based on the percentage of healthy servers in upstream clusters. health check: added setting for no-traffic interval. http : added idle timeout for upstream http connections. http: added support for proxying 100-Continue responses. http: added the ability to pass a URL encoded PEM encoded peer certificate in the x-forwarded-client-cert header. http: added support for trusting additional hops in the x-forwarded-for request header. http: added support for incoming HTTP/1.0. hot restart: added SIGTERM propagation to children to hot-restarter.py, which enables using it as a parent of containers. ip tagging: added HTTP IP Tagging filter. listeners: added support for listening for both IPv4 and IPv6 when binding to ::. listeners: added support for listening on UNIX domain sockets. listeners: added support for abstract unix domain sockets on Linux. The abstract namespace can be used by prepending ‘@’ to a socket path. load balancer: added cluster configuration for healthy panic threshold percentage. load balancer: added Maglev consistent hash load balancer. load balancer: added support for LocalityLbEndpoints priorities. lua: added headers replace() API. lua: extended to support metadata object API. redis: added local PING support to the Redis filter. redis: added GEORADIUS_RO and GEORADIUSBYMEMBER_RO to the Redis command splitter whitelist. router: added DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT, DOWNSTREAM_LOCAL_ADDRESS, DOWNSTREAM_LOCAL_ADDRESS_WITHOUT_PORT, PROTOCOL, and UPSTREAM_METADATA header formatters. The CLIENT_IP header formatter has been deprecated. router: added gateway-error retry-on policy. router: added support for route matching based on URL query string parameters. router: added support for more granular weighted cluster routing by allowing the total_weightto be specified in configuration. router: added support for custom request/response headers with mixed static and dynamic values. router: added support for direct responses. I.e., sending a preconfigured HTTP response without proxying anywhere. router: added support for HTTPS redirects on specific routes. router: added support for prefix_rewrite for redirects. router: added support for stripping the query string for redirects. router: added support for downstream request/upstream response header manipulation in weighted cluster. router: added support for range based header matching for request routing. squash: added support for the Squash microservices debugger. Allows debugging an incoming request to a microservice in the mesh. stats: added metrics service API implementation. stats: added native DogStatsd support. stats: added support for fixed stats tag values which will be added to all metrics. tcp proxy: added support for specifying a metadata matcher for upstream clusters in the tcp filter. tcp proxy: improved TCP proxy to correctly proxy TCP half-close. tcp proxy: added idle timeout. tcp proxy: access logs now bring an IP address without a port when using DOWNSTREAM_ADDRESS. Use DOWNSTREAM_REMOTE_ADDRESS instead. tracing: added support for dynamically loading an OpenTracing tracer. tracing: when using the Zipkin tracer, it is now possible for clients to specify the sampling decision (using the x-b3-sampled header) and have the decision propagated through to subsequently invoked services. tracing: when using the Zipkin tracer, it is no longer necessary to propagate the x-ot-span-context header. See more on trace context propagation here. transport sockets: added transport socket interface to allow custom implementations of transport sockets. A transport socket provides read and write logic with buffer encryption and decryption (if applicable). The existing TLS implementation has been refactored with the interface. upstream: added support for specifying an alternate stats name while emitting stats for clusters. Many small bug fixes and performance improvements not listed. 1.5.0 (December 4, 2017) access log: added fields for UPSTREAM_LOCAL_ADDRESS and DOWNSTREAM_ADDRESS. admin: added JSON output for stats admin endpoint. admin: added basic Prometheus output for stats admin endpoint. Histograms are not currently output. admin: added version_info to the /clusters admin endpoint. config: the v2 API is now considered production ready. config: added --v2-config-only CLI flag. cors: added CORS filter. health check: added x-envoy-immediate-health-check-fail header support. health check: added reuse_connection option. http: added per-listener stats. http: end-to-end HTTP flow control is now complete across both connections, streams, and filters. load balancer: added subset load balancer. load balancer: added ring size and hash configuration options. This used to be configurable via runtime. The runtime configuration was deleted without deprecation as we are fairly certain no one is using it. log: added the ability to optionally log to a file instead of stderr via the --log-path option. listeners: added drain_type option. lua: added experimental Lua filter. mongo filter: added fault injection. mongo filter: added “drain close” support. outlier detection: added HTTP gateway failure type. See DEPRECATED.md for outlier detection stats deprecations in this release. redis: the redis proxy filter is now considered production ready. redis: added “drain close” functionality. router: added x-envoy-overloaded support. router: added regex route matching. router: added custom request headers for upstream requests. router: added downstream IP hashing for HTTP ketama routing. router: added cookie hashing. router: added start_child_span option to create child span for egress calls. router: added optional upstream logs. router: added complete custom append/override/remove support of request/response headers. router: added support to specify response code during redirect. router: added configuration to return either a 404 or 503 if the upstream cluster does not exist. runtime: added comment capability. server: change default log level (-l) to info. stats: maximum stat/name sizes and maximum number of stats are now variable via the--max-obj-name-len and --max-stats options. tcp proxy: added access logging. tcp proxy: added configurable connect retries. tcp proxy: enable use of outlier detector. tls: added SNI support. tls: added support for specifying TLS session ticket keys. tls: allow configuration of the min and max TLS protocol versions. tracing: added custom trace span decorators. Many small bug fixes and performance improvements not listed. 1.4.0 (August 24, 2017) macOS is now supported. (A few features are missing such as hot restart and original destination routing). YAML is now directly supported for config files. Added /routes admin endpoint. End-to-end flow control is now supported for TCP proxy, HTTP/1, and HTTP/2. HTTP flow control that includes filter buffering is incomplete and will be implemented in 1.5.0. Log verbosity compile time flag added. Hot restart compile time flag added. Original destination cluster and load balancer added. WebSocket is now supported. Virtual cluster priorities have been hard removed without deprecation as we are reasonably sure no one is using this feature. Route validate_clusters option added. x-envoy-downstream-service-node header added. x-forwarded-client-cert header added. Initial HTTP/1 forward proxy support for absolute URLs has been added. HTTP/2 codec settings are now configurable. gRPC/JSON transcoder filter added. gRPC web filter added. Configurable timeout for the rate limit service call in the network and HTTP rate limit filters. x-envoy-retry-grpc-on header added. LDS API added. TLS require_client_certificate option added. Configuration check tool added. JSON schema check tool added. Config validation mode added via the --mode option. --local-address-ip-version option added. IPv6 support is now complete. UDP statsd_ip_address option added. Per-cluster DNS resolvers added. Fault filter enhancements and fixes. Several features are deprecated as of the 1.4.0 release. They will be removed at the beginning of the 1.5.0 release cycle. We explicitly call out that the HttpFilterConfigFactory filter API has been deprecated in favor of NamedHttpFilterConfigFactory. Many small bug fixes and performance improvements not listed. 1.3.0 (May 17, 2017) As of this release, we now have an official breaking change policy. Note that there are numerous breaking configuration changes in this release. They are not listed here. Future releases will adhere to the policy and have clear documentation on deprecations and changes. Bazel is now the canonical build system (replacing CMake). There have been a huge number of changes to the development/build/test flow. See /bazel/README.md and /ci/README.md for more information. Outlier detection has been expanded to include success rate variance, and all parameters are now configurable in both runtime and in the JSON configuration. TCP level listener and cluster connections now have configurable receive buffer limits at which point connection level back pressure is applied. Full end to end flow control will be available in a future release. Redis health checking has been added as an active health check type. Full Redis support will be documented/supported in 1.4.0. TCP health checking now supports a “connect only” mode that only checks if the remote server can be connected to without writing/reading any data. BoringSSL is now the only supported TLS provider. The default cipher suites and ECDH curves have been updated with more modern defaults for both listener and cluster connections. The header value match rate limit action has been expanded to include an expect matchparameter. Route level HTTP rate limit configurations now do not inherit the virtual host level configurations by default. The include_vh_rate_limits to inherit the virtual host level options if desired. HTTP routes can now add request headers on a per route and per virtual host basis via therequest_headers_to_add option. The example configurations have been refreshed to demonstrate the latest features. per_try_timeout_ms can now be configured in a route’s retry policy in addition to via the x-envoy-upstream-rq-per-try-timeout-ms HTTP header. HTTP virtual host matching now includes support for prefix wildcard domains (e.g., *.lyft.com). The default for tracing random sampling has been changed to 100% and is still configurable inruntime. HTTP tracing configuration has been extended to allow tags to be populated from arbitrary HTTP headers. The HTTP rate limit filter can now be applied to internal, external, or all requests via the request_type option. Listener binding now requires specifying an address field. This can be used to bind a listener to both a specific address as well as a port. The MongoDB filter now emits a stat for queries that do not have $maxTimeMS set. The MongoDB filter now emits logs that are fully valid JSON. The CPU profiler output path is now configurable. A watchdog system has been added that can kill the server if a deadlock is detected. A route table checking tool has been added that can be used to test route tables before use. We have added an example repo that shows how to compile/link a custom filter. Added additional cluster wide information related to outlier detection to the /clusters admin endpoint. Multiple SANs can now be verified via the verify_subject_alt_name setting. Additionally, URI type SANs can be verified. HTTP filters can now be passed opaque configuration specified on a per route basis. By default Envoy now has a built in crash handler that will print a back trace. This behavior can be disabled if desired via the --define=signal_trace=disabled Bazel option. Zipkin has been added as a supported tracing provider. Numerous small changes and fixes not listed here. 1.2.0 (March 7, 2017) Cluster discovery service (CDS) API. Outlier detection (passive health checking). Envoy configuration is now checked against a JSON schema. Ring hash consistent load balancer, as well as HTTP consistent hash routing based on a policy. Vastly enhanced global rate limit configuration via the HTTP rate limiting filter. HTTP routing to a cluster retrieved from a header. Weighted cluster HTTP routing. Auto host rewrite during HTTP routing. Regex header matching during HTTP routing. HTTP access log runtime filter. LightStep tracer parent/child span association. Route discovery service (RDS) API. HTTP router x-envoy-upstream-rq-timeout-alt-response header support. use_original_dst and bind_to_port listener options (useful for iptables based transparent proxy support). TCP proxy filter route table support. Configurable stats flush interval. Various third party library upgrades, including using BoringSSL as the default SSL provider. No longer maintain closed HTTP/2 streams for priority calculations. Leads to substantial memory savings for large meshes. Numerous small changes and fixes not listed here. 1.1.0 (November 30, 2016) Switch from Jannson to RapidJSON for our JSON library (allowing for a configuration schema in 1.2.0). Upgrade recommended version of various other libraries. Configurable DNS refresh rate for DNS service discovery types. Upstream circuit breaker configuration can be overridden via runtime. Zone aware routing support. Generic header matching routing rule. HTTP/2 graceful connection draining (double GOAWAY). DynamoDB filter per shard statistics (pre-release AWS feature). Initial release of the fault injection HTTP filter. HTTP rate limit filter enhancements (note that the configuration for HTTP rate limiting is going to be overhauled in 1.2.0). Added refused-stream retry policy. Multiple priority queues for upstream clusters (configurable on a per route basis, with separate connection pools, circuit breakers, etc.). Added max connection circuit breaking to the TCP proxy filter. Added CLI options for setting the logging file flush interval as well as the drain/shutdown time during hot restart. A very large number of performance enhancements for core HTTP/TCP proxy flows as well as a few new configuration flags to allow disabling expensive features if they are not needed (specifically request ID generation and dynamic response code stats). Support Mongo 3.2 in the Mongo sniffing filter. Lots of other small fixes and enhancements not listed. 1.0.0 (September 12, 2016) Initial open source release. "},"start/start.html":{"url":"start/start.html","title":"入门指南","keywords":"","body":"入门指南 本章节会介绍非常简单的配置，并提供一些示例。 Envoy 目前不提供单独的预先编译好的二进制文件，但提供了 Docker 镜像。这是开始使用 Envoy 的最快方式。如果您希望在 Docker 容器外使用 Envoy，则需要构建它。 这些示例使用 v2 Envoy API，但仅使用 API 的静态配置功能，这对于简单的需求非常有用。 更复杂的需求是由动态配置来支持的。 快速开始运行简单示例 根据 Envoy 存储库中的文件运行这些命令。下面的部分给出了配置文件和执行步骤更详细的解释。 configs/google_com_proxy.v2.yaml 中提供了一个非常简单的可用于验证基于纯 HTTP 代理的 Envoy 配置。 这不表示实际的 Envoy 部署： $ docker pull envoyproxy/envoy:latest $ docker run --rm -d -p 10000:10000 envoyproxy/envoy:latest $ curl -v localhost:10000 使用的 Docker 镜像将包含最新版本的 Envoy 和一个基本的 Envoy 配置。此基本配置告诉 Envoy 将入站请求路由到 *.google.com。 简单的配置 Envoy 通过 YAML 文件中传入的参数来进行配置。 admin message 是 administration 服务必须的配置。address 键指定监听地址，下面的例子监听地址是 0.0.0.0:9901。 admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources 包含 Envoy 启动时静态配置的所有内容，而不是 Envoy 在运行时动态配置的资源。v2 API Overview 描述了这一点。 static_resources: listeners 规范 listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { host_rewrite: www.google.com, cluster: service_google } http_filters: - name: envoy.router clusters 规范 clusters: - name: service_google connect_timeout: 0.25s type: LOGICAL_DNS # Comment out the following line to test on v6 networks dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: google.com, port_value: 443 }}] tls_context: { sni: www.google.com } 使用 Envoy Docker 镜像 创建一个简单的 Dockerfile 来执行 Envoy，假定 envoy.yaml（如上所述）位于本地目录中。您可以参考命令行选项。 FROM envoyproxy/envoy:latest COPY envoy.yaml /etc/envoy/envoy.yaml 使用以下命令构建您配置的 Docker 镜像： $ docker build -t envoy:v1 现在您可以执行它： $ docker run -d --name envoy -p 9901:9901 -p 10000:10000 envoy:v1 最后测试使用： $ curl -v localhost:10000 如果您想通过 docker-compose 使用 envoy，则可以使用 volume 覆盖提供的配置文件。 Sandbox 我们使用 Docker Compose 创建了许多 sandbox ，这些 sandbox 设置了不同的环境来测试 Envoy 的功能并显示示例配置。 当我们觉得人们更有兴趣时，将添加和展示更多不同特征的 sandbox。 以下 sandbox 可用： Front Proxy Zipkin Tracing Jaeger Tracing Jaeger Native Tracing gRPC Bridge 其他用例 除代理本身之外， Envoy 还被几个特定用例捆绑为开源发行版的一部分。 Envoy as an API Gateway in Kubernetes "},"start/sandboxes/front_proxy.html":{"url":"start/sandboxes/front_proxy.html","title":"前端代理","keywords":"","body":"前端代理 为了帮助大家了解如何使用 Envoy 作为前端代理，我们发布了一个 docker compose 沙箱，该沙箱中部署了一个前端 envoy 以及与服务 envoy 搭配的一组服务（简单的沙箱应用）。这三个容器将被部署在名为 envoymesh 的虚拟网络中。 下面是使用 docker compose 部署的架构图： 所有传入的请求都通过前端 envoy 进行路由，该 envoy 充当位于 envoymesh 网络边缘的反向代理。通过docker compose 将端口 80 映射到 8000 端口（请参阅 /examples/front-proxy/docker-compose.yml）。此外，请注意，由前端 envoy 由到服务容器的所有流量实际上路由到服务 envoy（在 /examples/front-proxy/front-envoy.yaml 中设置的路由）。反过来，服务 envoy 通过回环地址（/examples/front-proxy/service-envoy.yaml 中的路由设置）将请求路由到 flask 应用程序。此设置说明了运行服务 envoy 与您的服务搭配的优势：所有请求都由服务 envoy 处理，并有效地路由到您的服务。 运行 Sandbox 以下文档通过按照上图中所述组织的 envoy 集群的设置运行。 步骤 1：安装 Docker 确保您已经安装了最新版本的 docker、docker-compose 和 docker-machine。 安装这些软件最简单的方式是使用 Docker Toolbox。 步骤 2：配置 Docker Machine 首先创建一个容纳容器的新机器： $ docker-machine create --driver virtualbox default $ eval $(docker-machine env default) 步骤 3：克隆 Envoy repo，启动所有的容器 如果您还没有克隆 envoy repo，执行 git clone git@github.com:envoyproxy/envoy 或者 git clone https://github.com/envoyproxy/envoy.git 来克隆。 $ pwd envoy/examples/front-proxy $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- example_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp 步骤 4：测试 Envoy 的路由能力 您现在可以通过前端 envoy 向两项服务发送请求。 向 service1： $ curl -v $(docker-machine ip default):8000/service/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 向 service2： $ curl -v $(docker-machine ip default):8000/service/2 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/2 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 请注意，每个请求在发送给前端 envoy 时已正确路由到相应的应用程序。 步骤 5：测试 Envoy 的负载均衡能力 Now let’s scale up our service1 nodes to demonstrate the clustering abilities of envoy.: 现在扩展我们的 service1 节点来演示 envoy 的集群能力： $ docker-compose scale service1=3 Creating and starting example_service1_2 ... done Creating and starting example_service1_3 ... done 现在，如果我们多次向 service1 发送请求，前端 envoy 将通过循环轮询三台 service1 机器来负载均衡请求： $ curl -v $(docker-machine ip default):8000/service/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 步骤 6：进入容器并 curl 服务 In addition of using curl from your host machine, you can also enter the containers themselves and curl from inside them. To enter a container you can use docker-compose exec /bin/bash. For example we can enter the front-envoy container, and curl for services locally: 除了使用主机上的 curl 外，您还可以进入容器并从容器里面 curl。要进入容器，可以使用 docker-compose exec /bin/bash。例如，我们可以进入前端 envoy 容器，并在本地 curl 服务： $ docker-compose exec front-envoy /bin/bash root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: 85ac151715c6 resolvedhostname: 172.19.0.3 root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: 20da22cfc955 resolvedhostname: 172.19.0.5 root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: f26027f1ce28 resolvedhostname: 172.19.0.6 root@81288499f9d7:/# curl localhost:80/service/2 Hello from behind Envoy (service 2)! hostname: 92f4a3737bbc resolvedhostname: 172.19.0.2 步骤7：进入容器并 curl admin 当 envoy 运行时，它也会将 admin 连接到所需的端口。在示例配置 admin 绑定到 8001 端口。我们可以 curl 它获得有用的信息。例如，您可以 curl /server_info 来获取正在运行的 envoy 版本的信息。此外，你可以 curl /stats 来获得统计数据。例如在 frontenvoy 里面我们可以得到： $ docker-compose exec front-envoy /bin/bash root@e654c2c83277:/# curl localhost:8001/server_info envoy 10e00b/RELEASE live 142 142 0 root@e654c2c83277:/# curl localhost:8001/stats cluster.service1.external.upstream_rq_200: 7 ... cluster.service1.membership_change: 2 cluster.service1.membership_total: 3 ... cluster.service1.upstream_cx_http2_total: 3 ... cluster.service1.upstream_rq_total: 7 ... cluster.service2.external.upstream_rq_200: 2 ... cluster.service2.membership_change: 1 cluster.service2.membership_total: 1 ... cluster.service2.upstream_cx_http2_total: 1 ... cluster.service2.upstream_rq_total: 2 ... 请注意，我们可以获取上游集群的成员数量，它们实现的请求数量，有关 http 入口的信息以及大量其他有用的统计信息。 "},"start/sandboxes/zipkin_tracing.html":{"url":"start/sandboxes/zipkin_tracing.html","title":"Zipkin 追踪","keywords":"","body":"Zipkin 追踪 Zipkin 追踪 standbox 使用Zipkin 作为追踪提供者演示Envoy的请求追踪 功能。此沙箱与上述前端代理体系结构非常相似，但有一点不同：在返回响应之前，service1 会对 service2 进行 API 调用。 这三个容器将被部署在名为envoymesh的虚拟网络中。 所有传入的请求都通过前端 envoy 进行路由，该envoy 充当位于 envoymesh 网络边缘的反向代理。 端口80通过 docker compose 映射到端口8000（参见/examples/zipkin-tracing/docker-compose.yml）。请注意，所有 envoy 都配置为收集请求跟踪（例如 http_connection_manager/config/tracing 中的设置/examples/zipkin-tracing/front-envoy-zipkin.yaml）并设置为将 Zipkin 追踪器生成的跨度传播到 Zipkin 集群中（跟踪驱动程序设置/examples/zipkin-tracing/front-envoy-zipkin.yaml）。 在将请求路由到相应的服务 envoy 或应用程序之前，Envoy 将负责为跟踪生成适当的跨度（父/子/共享上下文跨度）。 在高层次上，每个跨度记录上游API调用的延迟以及将跨度与其他相关跨度（例如跟踪ID）关联所需的信息。 从 Envoy 进行跟踪的最重要的好处之一是，它将负责将跟踪传播到 Zipkin 服务群集。 但是，为了充分利用跟踪，应用程序必须传播 Envoy 生成的跟踪标头，同时调用其他服务。 在我们提供的沙箱中，简单的应用程序（请参阅/examples/front-proxy/service.py）作为 service1 传播跟踪头，同时对 service2 进行出站呼叫。 运行 Sandbox 以下文档通过按照上图中所述组织的 envoy 集群的设置运行。 第1步：构建 sandbox 要构建这个沙盒示例，并启动示例应用程序，请运行以下命令： $ pwd envoy/examples/zipkin-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- zipkintracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp zipkintracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp zipkintracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp 第2步：生成一些负载 您现在可以通过前台特使向 service1 发送请求，如下所示： $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 第3步：在 Zipkin UI 中查看追踪 使用您的浏览器访问 http://localhost:9411。 你应该看到 Zipkin 仪表板。 如果这个 IP 地址不正确，你可以运行：$ docker-machine ip default来找到正确的地址。, 将服务设置为“前台代理”，并将开始时间设置为在测试开始前几分钟（步骤2）并按回车。你应该看到前端代理的痕迹。 单击一个跟踪来探索从前端代理到service1到service2的请求所采用的路径，以及每个跃点产生的延迟。 "},"start/sandboxes/jaeger_tracing.html":{"url":"start/sandboxes/jaeger_tracing.html","title":"Jaeger 追踪","keywords":"","body":"Jaeger Tracing The Jaeger tracing sandbox demonstrates Envoy’s request tracing capabilities using Jaeger as the tracing provider. This sandbox is very similar to the front proxy architecture described above, with one difference: service1 makes an API call to service2 before returning a response. The three containers will be deployed inside a virtual network called envoymesh. All incoming requests are routed via the front envoy, which is acting as a reverse proxy sitting on the edge of the envoymesh network. Port 80 is mapped to port 8000 by docker compose (see /examples/jaeger-tracing/docker-compose.yml). Notice that all envoys are configured to collect request traces (e.g., http_connection_manager/config/tracing setup in /examples/jaeger-tracing/front-envoy-jaeger.yaml) and setup to propagate the spans generated by the Jaeger tracer to a Jaeger cluster (trace driver setup in /examples/jaeger-tracing/front-envoy-jaeger.yaml). Before routing a request to the appropriate service envoy or the application, Envoy will take care of generating the appropriate spans for tracing (parent/child context spans). At a high-level, each span records the latency of upstream API calls as well as information needed to correlate the span with other related spans (e.g., the trace ID). One of the most important benefits of tracing from Envoy is that it will take care of propagating the traces to the Jaeger service cluster. However, in order to fully take advantage of tracing, the application has to propagate trace headers that Envoy generates, while making calls to other services. In the sandbox we have provided, the simple flask app (see trace function in /examples/front-proxy/service.py) acting as service1 propagates the trace headers while making an outbound call to service2. Running the Sandbox The following documentation runs through the setup of an envoy cluster organized as is described in the image above. Step 1: Build the sandbox To build this sandbox example, and start the example apps run the following commands: $ pwd envoy/examples/jaeger-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- jaegertracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp Step 2: Generate some load You can now send a request to service1 via the front-envoy as follows: $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > Step 3: View the traces in Jaeger UI Point your browser to http://localhost:16686 . You should see the Jaeger dashboard. Set the service to “front-proxy” and hit ‘Find Traces’. You should see traces from the front-proxy. Click on a trace to explore the path taken by the request from front-proxy to service1 to service2, as well as the latency incurred at each hop. "},"start/sandboxes/jaeger_native_tracing.html":{"url":"start/sandboxes/jaeger_native_tracing.html","title":"Jaeger 原生追踪","keywords":"","body":"Jaeger 原生追踪 The Jaeger tracing sandbox demonstrates Envoy’s request tracing capabilities using Jaeger as the tracing provider and Jaeger’s native C++ client as a plugin. Using Jaeger with its native client instead of with Envoy’s builtin Zipkin client has the following advantages: Trace propagation will work with other other services using Jaeger without needing to make configuration changes. A variety of different sampling strategies can be used, including probabilistic or remote where sampling can be centrally controlled from Jaeger’s backend. Spans are sent to the collector in a more efficient binary encoding. This sandbox is very similar to the front proxy architecture described above, with one difference: service1 makes an API call to service2 before returning a response. The three containers will be deployed inside a virtual network called envoymesh. (Note: the sandbox only works on x86-64). All incoming requests are routed via the front envoy, which is acting as a reverse proxy sitting on the edge of the envoymesh network. Port 80 is mapped to port 8000 by docker compose (see /examples/jaeger-native-tracing/docker-compose.yml). Notice that all envoys are configured to collect request traces (e.g., http_connection_manager/config/tracing setup in /examples/jaeger-native-tracing/front-envoy-jaeger.yaml) and setup to propagate the spans generated by the Jaeger tracer to a Jaeger cluster (trace driver setup in /examples/jaeger-native-tracing/front-envoy-jaeger.yaml). Before routing a request to the appropriate service envoy or the application, Envoy will take care of generating the appropriate spans for tracing (parent/child context spans). At a high-level, each span records the latency of upstream API calls as well as information needed to correlate the span with other related spans (e.g., the trace ID). One of the most important benefits of tracing from Envoy is that it will take care of propagating the traces to the Jaeger service cluster. However, in order to fully take advantage of tracing, the application has to propagate trace headers that Envoy generates, while making calls to other services. In the sandbox we have provided, the simple flask app (see trace function in /examples/front-proxy/service.py) acting as service1 propagates the trace headers while making an outbound call to service2. Running the Sandbox The following documentation runs through the setup of an envoy cluster organized as is described in the image above. Step 1: Build the sandbox To build this sandbox example, and start the example apps run the following commands: $ pwd envoy/examples/jaeger-native-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- jaegertracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp Step 2: Generate some load You can now send a request to service1 via the front-envoy as follows: $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > Step 3: View the traces in Jaeger UI Point your browser to http://localhost:16686 . You should see the Jaeger dashboard. Set the service to “front-proxy” and hit ‘Find Traces’. You should see traces from the front-proxy. Click on a trace to explore the path taken by the request from front-proxy to service1 to service2, as well as the latency incurred at each hop. "},"start/distro/ambassador.html":{"url":"start/distro/ambassador.html","title":"Envoy 作为 Kubernetes 的 API 网关","keywords":"","body":"Envoy 作为 Kubernetes 的 API 网关 使用 Ambassador 的一个常见场景是将其部署为 Kubernetes 的 edge 服务（ API 网关）。Ambassador 是开源 Envoy 的分布式版本，专门为 kubernetes 设计的。 本例将介绍如何通过 Ambassador 在 Kubernetes 上部署 Ambassador 。 部署 Ambassador Ambassador 的设置是通过 kubernetes 部署的。为了在 kubernetes 安装 Ambassador/Envoy，如果你的集群启动了 RBAC： kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-rbac.yaml 如果您没启动 RBAC： kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-no-rbac.yaml 上面的 YAML 将会为 Ambassador 创建 kubernetes 部署，包含 readiness 和 liveness 检查。默认，将会创建3个 Ambassador 实例。每一个 Ambassador 实例包含一个 Envoy 代理以及一个 Ambassador 控制器。 我们现在需要创建一个 Kubernetes 服务来指向 Ambassador 的部署，我们将使用 LoadBalancer 服务。如果你的集群不支持 LoadBalancer 服务，你需要改成 NodePort 或者 ClusterIP。 --- apiVersion: v1 kind: Service metadata: labels: service: ambassador name: ambassador spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: service: ambassador 将上面的 YAML 文件保存成ambassador-svc.yaml文件。然后将这个服务部署到 kubernetes： kubectl apply -f ambassador-svc.yaml 这时候 Envoy 和 Ambassador 控制器已经在你的集群上运行。 配置 Ambassador Ambassador 使用 Kubernetes 注解来添加或删除配置。这个示例 YAML 将添加一条到 Google 的路由，类似于入门指南中的基本配置示例。 --- apiVersion: v1 kind: Service metadata: name: google annotations: getambassador.io/config: | --- apiVersion: ambassador/v0 kind: Mapping name: google_mapping prefix: /google/ service: https://google.com:443 host_rewrite: www.google.com spec: type: ClusterIP clusterIP: None 保存上面的文件，命名为 google.yaml。然后运行: kubectl apply -f google.yaml Ambassador 将发现您的 Kubernetes 注解的更改，并添加到 Envoy 的路由。注意，我们在这个例子中使用了一个虚拟服务;通常，您会将注解与真正的 Kubernetes 服务关联起来。 测试映射 您可以通过获得 Ambassador 服务的外部 IP 地址来测试这个映射，然后通过curl发送请求： $ kubectl get svc ambassador NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE ambassador 10.19.241.98 35.225.154.81 80:32491/TCP 15m $ curl -v 35.225.154.81/google/ 更多 Ambassador 在上公开了多个 Envoy 的特性映射，比如 CORS 、加权循环调度算法、gRPC、TLS 和超时设定。要了解更多信息，请阅读配置文档。 "},"install/building.html":{"url":"install/building.html","title":"构建","keywords":"","body":"构建 Envoy 构建系统使用了 Bazel 。为了简化初始构建和快速启动，我们提供了一个基于 Ubuntu 16的 docker 容器，它内部包含了所需的所有东西用来来构建和静态链接 envoy，详参 ci/README.md 为了手动创建，遵循 bazel/README.md 的说明。 要求 Envoy 最初是在 Ubuntu 14 LTS 上开发和部署的。它应该适用于任何最近的 Linux，包括 Ubuntu 16 LTS。 构建 Envoy 有以下要求: GCC 5+ (对 C++14 支持)。 这些预先构建的第三方依赖。 这些 Bazel native 的依赖。 请阅读 CI 和 Bazel 的文档。获取有关执行手动构建的更多信息。 预构建的二进制文件 在每一个 master 提交中，我们创建了一组轻量级 Docker 镜像，其中包含 Envoy 的二进制文件。我们在发布官方版本时也会使用发布版本号来标记 docker 镜像。 envoyproxy/envoy: 发布在 Ubuntu Xenial 基础之上带有标记的二进制文件 envoyproxy/envoy-alpine: 发布带有在 glibc 基础之上标记的二进制文件 envoyproxy/envoy-alpine-debug: 发布带有在 glibc 基础之上 debug 标记的二进制文件 我们将考虑在帮助 CI、包等方面创建额外的二进制类型，如果需要的话，请在 GitHub 上打开一个 issue。 修改 Envoy 如果你对修改 Envoy 和测试你的改变感兴趣，那么一种方法就是使用 Docker。本指南将介绍构建您自己的 Envoy 二进制文件的过程，并将二进制文件放入一个 Ubuntu 容器中。 构建 Envoy Docker 镜像 "},"install/ref_configs.html":{"url":"install/ref_configs.html","title":"参考配置","keywords":"","body":"参考配置 The source distribution includes a set of example configuration templates for each of the three major Envoy deployment types: Service to service Front proxy Double proxy The goal of this set of example configurations is to demonstrate the full capabilities of Envoy in a complex deployment. All features will not be applicable to all use cases. For full documentation see the configuration reference. Configuration generator Envoy configurations can become relatively complicated. At Lyft we use jinja templating to make the configurations easier to create and manage. The source distribution includes a version of the configuration generator that loosely approximates what we use at Lyft. We have also included three example configuration templates for each of the above three scenarios. Generator script: configs/configgen.py Service to service template: configs/envoy_service_to_service.template.json Front proxy template: configs/envoy_front_proxy.template.json Double proxy template: configs/envoy_double_proxy.template.json To generate the example configurations run the following from the root of the repo: mkdir -p generated/configs bazel build //configs:example_configs tar xvf $PWD/bazel-genfiles/configs/example_configs.tar -C generated/configs The previous command will produce three fully expanded configurations using some variables defined inside of configgen.py. See the comments inside of configgen.py for detailed information on how the different expansions work. A few notes about the example configurations: An instance of service discovery service is assumed to be running at discovery.yourcompany.net. DNS for yourcompany.net is assumed to be setup for various things. Search the configuration templates for different instances of this. Tracing is configured for LightStep. To disable this or enable Zipkin http://zipkin.io tracing, delete or change the tracing configuration accordingly. The configuration demonstrates the use of a global rate limiting service. To disable this delete the rate limit configuration. Route discovery service is configured for the service to service reference configuration and it is assumed to be running at rds.yourcompany.net. Cluster discovery service is configured for the service to service reference configuration and it is assumed that be running at cds.yourcompany.net. "},"install/tools/config_load_check_tool.html":{"url":"install/tools/config_load_check_tool.html","title":"配置负载检查工具","keywords":"","body":"配置负载检查工具 The config load check tool checks that a configuration file in JSON format is written using valid JSON and conforms to the Envoy JSON schema. This tool leverages the configuration test intest/config_test/config_test.cc. The test loads the JSON configuration file and runs server configuration initialization with it. Input The tool expects a PATH to the root of a directory that holds JSON Envoy configuration files. The tool will recursively go through the file system tree and run a configuration test for each file found. Keep in mind that the tool will try to load all files found in the path. Output The tool will output Envoy logs as it initializes the server configuration with the config it is currently testing. If there are configuration files where the JSON file is malformed or is does not conform to the Envoy JSON schema, the tool will exit with status EXIT_FAILURE. If the tool successfully loads all configuration files found it will exit with status EXIT_SUCCESS. Building The tool can be built locally using Bazel.bazel build //test/tools/config_load_check:config_load_check_tool Running The tool takes a path as described above.bazel-bin/test/tools/config_load_check/config_load_check_tool PATH "},"install/tools/route_table_check_tool.html":{"url":"install/tools/route_table_check_tool.html","title":"路由表检查工具","keywords":"","body":"路由表检查工具 The route table check tool checks whether the route parameters returned by a router match what is expected. The tool can also be used to check whether a path redirect, path rewrite, or host rewrite match what is expected. Input The tool expects two input JSON files:A router config JSON file. The router config JSON file schema is found in config. A tool config JSON file. The tool config JSON file schema is found in config. The tool config input file specifies urls (composed of authorities and paths) and expected route parameter values. Additional parameters such as additional headers are optional. Output The program exits with status EXIT_FAILURE if any test case does not match the expected route parameter value.The --details option prints out details for each test. The first line indicates the test name.If a test fails, details of the failed test cases are printed. The first field is the expected route parameter value. The second field is the actual route parameter value. The third field indicates the parameter that is compared. In the following example, Test_2 and Test_5 failed while the other tests passed. In the failed test cases, conflict details are printed.Test_1 Test_2 default other virtual_host_name Test_3 Test_4 Test_5 locations ats cluster_name Test_6Testing with valid runtime values is not currently supported, this may be added in future work. Building The tool can be built locally using Bazel.bazel build //test/tools/router_check:router_check_tool Running The tool takes two input json files and an optional command line parameter --details. The expected order of command line arguments is: 1. The router configuration json file. 2. The tool configuration json file. 3. The optional details flag.bazel-bin/test/tools/router_check/router_check_tool router_config.json tool_config.json bazel-bin/test/tools/router_check/router_check_tool router_config.json tool_config.json --details Testing A bash shell script test can be run with bazel. The test compares routes using different router and tool configuration json files. The configuration json files can be found in test/tools/router_check/test/config/… .bazel test //test/tools/router_check/... "},"install/tools/schema_validator_check_tool.html":{"url":"install/tools/schema_validator_check_tool.html","title":"Schema 验证器检查工具","keywords":"","body":"Schema 验证器检查工具 The schema validator tool validates that the passed in JSON conforms to a schema in the configuration. To validate the entire config, please refer to the config load check tool. Currently, only route config schema validation is supported. Input The tool expects two inputs:The schema type to check the passed in JSON against. The supported type is:route - for route configuration validation.The path to the JSON. Output If the JSON conforms to the schema, the tool will exit with status EXIT_SUCCESS. If the JSON does not conform to the schema, an error message is outputted detailing what doesn’t conform to the schema. The tool will exit with status EXIT_FAILURE. Building The tool can be built locally using Bazel.bazel build //test/tools/schema_validator:schema_validator_tool Running The tool takes a path as described above.bazel-bin/test/tools/schema_validator/schema_validator_tool --schema-type SCHEMA_TYPE --json-path PATH "},"configuration/overview/v1_overview.html":{"url":"configuration/overview/v1_overview.html","title":"v1 API 概览","keywords":"","body":"v1 API 概览 Attention The v1 configuration/API is now considered legacy and the deprecation schedule has been announced. Please upgrade and use the v2 configuration/API. The Envoy configuration format is written in JSON and is validated against a JSON schema. The schema can be found in source/common/json/config_schemas.cc. The main configuration for the server is contained within the listeners and cluster manager sections. The other top level elements specify miscellaneous configuration. YAML support is also provided as a syntactic convenience for hand-written configurations. Envoy will internally convert YAML to JSON if a file path ends with .yaml. In the rest of the configuration documentation, we refer exclusively to JSON. Envoy expects unambiguous YAML scalars, so if a cluster name (which should be a string) is called true, it should be written in the configuration YAML as “true”. The same applies to integer and floating point values (e.g. 1 vs. 1.0 vs. “1.0”). { \"listeners\": [], \"lds\": \"{...}\", \"admin\": \"{...}\", \"cluster_manager\": \"{...}\", \"flags_path\": \"...\", \"statsd_udp_ip_address\": \"...\", \"statsd_tcp_cluster_name\": \"...\", \"stats_flush_interval_ms\": \"...\", \"watchdog_miss_timeout_ms\": \"...\", \"watchdog_megamiss_timeout_ms\": \"...\", \"watchdog_kill_timeout_ms\": \"...\", \"watchdog_multikill_timeout_ms\": \"...\", \"tracing\": \"{...}\", \"rate_limit_service\": \"{...}\", \"runtime\": \"{...}\", } listeners (required, array) An array of listeners that will be instantiated by the server. A single Envoy process can contain any number of listeners. lds (optional, object) Configuration for the Listener Discovery Service (LDS). If not specified only static listeners are loaded. admin (required, object) Configuration for the local administration HTTP server. cluster_manager (required, object) Configuration for the cluster manager which owns all upstream clusters within the server. flags_path (optional, string) The file system path to search for startup flag files. statsd_udp_ip_address (optional, string) The UDP address of a running statsd compliant listener. If specified, statisticswill be flushed to this address. IPv4 addresses should have format host:port (ex: 127.0.0.1:855). IPv6 addresses should have URL format [host]:port (ex: [::1]:855). statsd_tcp_cluster_name (optional, string) The name of a cluster manager cluster that is running a TCP statsd compliant listener. If specified, Envoy will connect to this cluster to flush statistics. stats_flush_interval_ms (optional, integer) The time in milliseconds between flushes to configured stats sinks. For performance reasons Envoy latches counters and only flushes counters and gauges at a periodic interval. If not specified the default is 5000ms (5 seconds). watchdog_miss_timeout_ms (optional, integer) The time in milliseconds after which Envoy counts a nonresponsive thread in the “server.watchdog_miss” statistic. If not specified the default is 200ms. watchdog_megamiss_timeout_ms (optional, integer) The time in milliseconds after which Envoy counts a nonresponsive thread in the “server.watchdog_mega_miss” statistic. If not specified the default is 1000ms. watchdog_kill_timeout_ms (optional, integer) If a watched thread has been nonresponsive for this many milliseconds assume a programming error and kill the entire Envoy process. Set to 0 to disable kill behavior. If not specified the default is 0 (disabled). watchdog_multikill_timeout_ms (optional, integer) If at least two watched threads have been nonresponsive for at least this many milliseconds assume a true deadlock and kill the entire Envoy process. Set to 0 to disable this behavior. If not specified the default is 0 (disabled). tracing (optional, object) Configuration for an external tracing provider. If not specified, no tracing will be performed. rate_limit_service (optional, object) Configuration for an external rate limit service provider. If not specified, any calls to the rate limit service will immediately return success. runtime (optional, object) Configuration for the runtime configuration provider. If not specified, a “null” provider will be used which will result in all defaults being used. "},"configuration/overview/v2_overview.html":{"url":"configuration/overview/v2_overview.html","title":"v2 API 概览","keywords":"","body":"v2 API 概览 The Envoy v2 APIs are defined as proto3 Protocol Buffers in the data plane API repository. They evolve the existing v1 APIs and concepts to support: Streaming delivery of xDS API updates via gRPC. This reduces resource requirements and can lower the update latency. A new REST-JSON API in which the JSON/YAML formats are derived mechanically via the proto3 canonical JSON mapping. Delivery of updates via the filesystem, REST-JSON or gRPC endpoints. Advanced load balancing through an extended endpoint assignment API and load and resource utilization reporting to management servers. Stronger consistency and ordering properties when needed. The v2 APIs still maintain a baseline eventual consistency model. See the xDS protocol description for further details on aspects of v2 message exchange between Envoy and the management server. Bootstrap 配置 To use the v2 API, it’s necessary to supply a bootstrap configuration file. This provides static server configuration and configures Envoy to access dynamic configuration if needed. As with the v1 JSON/YAML configuration, this is supplied on the command-line via the -c flag, i.e.: ./envoy -c .{json,yaml,pb,pb_text} --v2-config-only where the extension reflects the underlying v2 config representation. The --v2-config-only flag is not strictly required as Envoy will attempt to autodetect the config file version, but this option provides an enhanced debug experience when configuration parsing fails. The Bootstrap message is the root of the configuration. A key concept in the Bootstrap message is the distinction between static and dynamic resouces. Resources such as a Listener or Cluster may be supplied either statically in static_resources or have an xDS service such as LDS or CDSconfigured in dynamic_resources. 示例 Below we will use YAML representation of the config protos and a running example of a service proxying HTTP from 127.0.0.1:10000 to 127.0.0.2:1234. 静态 A minimal fully static bootstrap config is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 127.0.0.1, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } http_filters: - name: envoy.router clusters: - name: some_service connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: 127.0.0.2, port_value: 1234 }}] 除了动态 EDS 大部分静态 A bootstrap config that continues from the above example with dynamic endpoint discovery via anEDS gRPC management server listening on 127.0.0.3:5678 is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 127.0.0.1, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } http_filters: - name: envoy.router clusters: - name: some_service connect_timeout: 0.25s lb_policy: ROUND_ROBIN type: EDS eds_cluster_config: eds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] - name: xds_cluster connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN http2_protocol_options: {} hosts: [{ socket_address: { address: 127.0.0.3, port_value: 5678 }}] Notice above that xds_cluster is defined to point Envoy at the management server. Even in an otherwise completely dynamic configurations, some static resources need to be defined to point Envoy at its xDS management server(s). In the above example, the EDS management server could then return a proto encoding of a DiscoveryResponse: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.ClusterLoadAssignment cluster_name: some_service endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 127.0.0.2 port_value: 1234 The versioning and type URL scheme that appear above are explained in more detail in the streaming gRPC subscription protocol documentation. 动态 A fully dynamic bootstrap configuration, in which all resources other than those belonging to the management server are discovered via xDS is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } dynamic_resources: lds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] cds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] static_resources: clusters: - name: xds_cluster connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN http2_protocol_options: {} hosts: [{ socket_address: { address: 127.0.0.3, port_value: 5678 }}] The management server could respond to LDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.Listener name: listener_0 address: socket_address: address: 127.0.0.1 port_value: 10000 filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO rds: route_config_name: local_route config_source: api_config_source: api_type: GRPC cluster_names: [xds_cluster] http_filters: - name: envoy.router The management server could respond to RDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.RouteConfiguration name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } The management server could respond to CDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.Cluster name: some_service connect_timeout: 0.25s lb_policy: ROUND_ROBIN type: EDS eds_cluster_config: eds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] The management server could respond to EDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.ClusterLoadAssignment cluster_name: some_service endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 127.0.0.2 port_value: 1234 管理服务器 A v2 xDS management server will implement the below endpoints as required for gRPC and/or REST serving. In both streaming gRPC and REST-JSON cases, a DiscoveryRequest is sent and aDiscoveryResponse received following the xDS protocol. gRPC streaming 端点 POST /envoy.api.v2.ClusterDiscoveryService/StreamClusters See cds.proto for the service definition. This is used by Envoy as a client when cds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /envoy.api.v2.EndpointDiscoveryService/StreamEndpoints See eds.proto for the service definition. This is used by Envoy as a client when eds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the eds_cluster_config field of the Cluster config. POST ``/envoy.api.v2.ListenerDiscoveryService/StreamListeners See lds.proto for the service definition. This is used by Envoy as a client when lds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /envoy.api.v2.RouteDiscoveryService/StreamRoutes See rds.proto for the service definition. This is used by Envoy as a client when route_config_name: some_route_name config_source: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the rds field of the HttpConnectionManager config. REST 端点 POST /v2/discovery:clusters See cds.proto for the service definition. This is used by Envoy as a client when cds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /v2/discovery:endpoints See eds.proto for the service definition. This is used by Envoy as a client when eds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the eds_cluster_config field of the Cluster config. POST /v2/discovery:listeners See lds.proto for the service definition. This is used by Envoy as a client when lds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /v2/discovery:routes See rds.proto for the service definition. This is used by Envoy as a client when route_config_name: some_route_name config_source: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the rds field of the HttpConnectionManager config. 聚合发现服务 While Envoy fundamentally employs an eventual consistency model, ADS provides an opportunity to sequence API update pushes and ensure affinity of a single management server for an Envoy node for API updates. ADS allows one or more APIs and their resources to be delivered on a single, bidirectional gRPC stream by the management server. Without this, some APIs such as RDS and EDS may require the management of multiple streams and connections to distinct management servers. ADS will allow for hitless updates of configuration by appropriate sequencing. For example, suppose foo.com was mappped to cluster X. We wish to change the mapping in the route table to point foo.com at cluster Y. In order to do this, a CDS/EDS update must first be delivered containing both clusters X and Y. Without ADS, the CDS/EDS/RDS streams may point at distinct management servers, or when on the same management server at distinct gRPC streams/connections that require coordination. The EDS resource requests may be split across two distinct streams, one for X and one for Y. ADS allows these to be coalesced to a single stream to a single management server, avoiding the need for distributed synchronization to correctly sequence the update. With ADS, the management server would deliver the CDS, EDS and then RDS updates on a single stream. ADS is only available for gRPC streaming (not REST) and is described more fully in this document. The gRPC endpoint is: POST /envoy.api.v2.AggregatedDiscoveryService/StreamAggregatedResources See discovery.proto for the service definition. This is used by Envoy as a client when ads_config: api_type: GRPC cluster_names: [some_ads_cluster] is set in the dynamic_resources of the Bootstrap config. When this is set, any of the configuration sources above can be set to use the ADS channel. For example, a LDS config could be changed from lds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] to lds_config: {ads: {}} with the effect that the LDS stream will be directed to some_ads_cluster over the shared ADS channel. 管理服务器不可达 When Envoy instance looses connectivity with the management server, Envoy will latch on to the previous configuration while actively retrying in the background to reestablish the connection with the management server. Envoy debug logs the fact that it is not able to establish a connection with the management server every time it attempts a connection. upstream_cx_connect_fail a cluster level statistic of the cluster pointing to management server provides a signal for monitoring this behavior. 状态 All features described in the v2 API reference are implemented unless otherwise noted. In the v2 API reference and the v2 API repository, all protos are frozen unless they are tagged as draft or experimental. Here, frozen means that we will not break wire format compatibility. Frozen protos may be further extended, e.g. by adding new fields, in a manner that does not break backwards compatibility. Fields in the above protos may be later deprecated, subject to thebreaking change policy, when their related functionality is no longer required. While frozen APIs have their wire format compatibility preserved, we reserve the right to change proto namespaces, file locations and nesting relationships, which may cause breaking code changes. We will aim to minimize the churn here. Protos tagged draft, meaning that they are near finalized, are likely to be at least partially implemented in Envoy but may have wire format breaking changes made prior to freezing. Protos tagged experimental, have the same caveats as draft protos and may have have major changes made prior to Envoy implementation and freezing. The current open v2 API issues are tracked here. "},"configuration/listeners/stats.html":{"url":"configuration/listeners/stats.html","title":"统计","keywords":"","body":"统计 监听器 每个监听器的统计树根在 listener.. ，有如下统计: 名称 类似 描述 downstream_cx_total Counter 连接总数 downstream_cx_destroy Counter 销毁连接总数 downstream_cx_active Gauge 活动连接总数 downstream_cx_length_ms Histogram 连接长度，单位毫秒 ssl.connection_error Counter TLS 连接错误总数，不包括证书认证失败 ssl.handshake Counter TLS连接握手成功总数 ssl.session_reused Counter TLS会话恢复成功总数 ssl.no_certificate Counter 不带客户端证书的TLS连接成功总数 ssl.fail_no_sni_match Counter 因为缺少SNI匹配而被拒绝的TLS连接总数 ssl.fail_verify_no_cert Counter 因为缺少客户端证书而失败的TLS连接总数 ssl.fail_verify_error Counter CA认证失败的TLS连接总数 ssl.fail_verify_san Counter SAN认证失败的TLS连接总数 ssl.fail_verify_cert_hash Counter 认证pinning认证失败的TLS连接总数 ssl.cipher. Counter 使用 的TLS连接总数 监听器管理器 监听器管理器的统计树根在 listener_manager. ，有以下统计。所有 stats 名称中的 : 字符被替换为 _。 名称 类型 描述 listener_added Counter 添加的监听器总数（不管是通过静态配置还是 LDS） listener_modified Counter 修改过的监听器总数（通过LDS） listener_removed Counter 删除过的监听器总数（通过LDS） listener_create_success Counter 成功添加到 workers 的监听器对象总数 listener_create_failure Counter 添加到 workers 失败的监听器对象总数 total_listeners_warming Gauge 当前热身中的监听器数量 total_listeners_active Gauge 当前活动中的监听器数量 total_listeners_draining Gauge 当前排除中的监听器数量 "},"configuration/listeners/runtime.html":{"url":"configuration/listeners/runtime.html","title":"运行时","keywords":"","body":"运行时 监听器支持下列运行时设置: ssl.alt_alpn 使用配置的 alt_alpn 协议字符串的请求百分比。默认为0。 "},"configuration/listeners/lds.html":{"url":"configuration/listeners/lds.html","title":"监听器发现服务（LDS）","keywords":"","body":"监听器发现服务（LDS） 监听器发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取监听器。Envoy 将协调 API 响应，并根据需要添加，修改或删除已知的监听器。 监听器更新的语义如下： 每个监听器必须有一个独特的名字。如果没有提供名称，Envoy 将创建一个 UUID。要动态更新的监听器，管理服务必须提供监听器的唯一名称。 当一个监听器被添加，在参与连接处理之前，会先进入“预热”阶段。例如，如果监听器引用 RDS 配置，那么在监听器迁移到 “active” 之前，将会解析并提取该配置。 监听器一旦创建，实际上就会保持不变。因此，更新监听器时，会创建一个全新的监听器（使用相同的侦听套接字）。新增加的监听者都会通过上面所描述的相同“预热”过程。 当更新或删除监听器时，旧的监听器将被置于 “draining（逐出）” 状态，就像整个服务重新启动时一样。监听器移除之后，该监听器所拥有的连接，经过一段时间优雅地关闭（如果可能的话）剩余的连接。逐出时间通过 --drain-time-s 选项设置。 注意 任何在 Envoy 配置中静态定义的监听器都不能通过 LDS API 进行修改或删除。 配置 v1 LDS API v2 LDS API 统计 LDS 的统计树是以 listener_manager.lds 为根，统计如下： 名称 类型 描述 config_reload Counter 由于不同的配置更新，导致配置API调用总数 update_attempt Counter LDS配置API调用重试总数 update_success Counter LDS配置API调用成功总数 update_failure Counter LDS配置API调用失败总数（网络或模式错误） version Gauge 上次成功调用的内容哈希值 "},"configuration/listener_filters/original_dst_filter.html":{"url":"configuration/listener_filters/original_dst_filter.html","title":"原始目的地","keywords":"","body":"原始目的地 原始目的地监听器过滤器读取 SO_ORIGINAL_DST 套接字属性，这个属性在连接被重定向时设置。重定向可以通过iptables REDIRECT target，或者 iptables REDIRECT target，或者 iptables TPROXY target 实现，结合使用监听器的 transparent 属性设置. Envoy 中的后续处理将恢复后的目标地址视为连接的本地地址，而不是监听器正在监听的地址。此外， 原始目标集群 可用于将 HTTP 请求或 TCP 连接转发到恢复后的目标地址。 v2 API 参考 "},"configuration/listener_filters/tls_inspector.html":{"url":"configuration/listener_filters/tls_inspector.html","title":"TLS 检查器","keywords":"","body":"TLS 检查器 TLS 检查器监听器筛选器允许检测传输是 TLS 还是明文，如果是 TLS，它将检测来自客户端的 服务器名称指示。这可以用来通过 FilterChainMatch 的 sni_domains 来选择 过滤器链。 SNI v2 API 参考 "},"configuration/network_filters/client_ssl_auth_filter.html":{"url":"configuration/network_filters/client_ssl_auth_filter.html","title":"客户端 TLS 身份验证","keywords":"","body":"客户端 TLS 身份验证 Client TLS authentication filter architecture overview v1 API reference v2 API reference Statistics Every configured client TLS authentication filter has statistics rooted at auth.clientssl..with the following statistics: Name Type Description update_success Counter Total principal update successes update_failure Counter Total principal update failures auth_no_ssl Counter Total connections ignored due to no TLS auth_ip_white_list Counter Total connections allowed due to the IP white list auth_digest_match Counter Total connections allowed due to certificate match auth_digest_no_match Counter Total connections denied due to no certificate match total_principals Gauge Total loaded principals REST API GET /v1/certs/list/approved The authentication filter will call this API every refresh interval to fetch the current list of approved certificates/principals. The expected JSON response looks like:{ \"certificates\": [] }certificates(required, array) list of approved certificates/principals.Each certificate object is defined as:{ \"fingerprint_sha256\": \"...\", }fingerprint_sha256(required, string) The SHA256 hash of the approved client certificate. Envoy will match this hash to the presented client certificate to determine whether there is a digest match. "},"configuration/network_filters/echo_filter.html":{"url":"configuration/network_filters/echo_filter.html","title":"Echo","keywords":"","body":"Echo The echo is a trivial network filter mainly meant to demonstrate the network filter API. If installed it will echo (write) all received data back to the connected downstream client. v1 API reference v2 API reference "},"configuration/network_filters/mongo_proxy_filter.html":{"url":"configuration/network_filters/mongo_proxy_filter.html","title":"Mongo 代理","keywords":"","body":"Mongo 代理 MongoDB architecture overview v1 API reference v2 API reference Fault injection The Mongo proxy filter supports fault injection. See the v1 and v2 API reference for how to configure. Statistics Every configured MongoDB proxy filter has statistics rooted at mongo.. with the following statistics: Name Type Description decoding_error Counter Number of MongoDB protocol decoding errors delay_injected Counter Number of times the delay is injected op_get_more Counter Number of OP_GET_MORE messages op_insert Counter Number of OP_INSERT messages op_kill_cursors Counter Number of OP_KILL_CURSORS messages op_query Counter Number of OP_QUERY messages op_query_tailable_cursor Counter Number of OP_QUERY with tailable cursor flag set op_query_no_cursor_timeout Counter Number of OP_QUERY with no cursor timeout flag set op_query_await_data Counter Number of OP_QUERY with await data flag set op_query_exhaust Counter Number of OP_QUERY with exhaust flag set op_query_no_max_time Counter Number of queries without maxTimeMS set op_query_scatter_get Counter Number of scatter get queries op_query_multi_get Counter Number of multi get queries op_query_active Gauge Number of active queries op_reply Counter Number of OP_REPLY messages op_reply_cursor_not_found Counter Number of OP_REPLY with cursor not found flag set op_reply_query_failure Counter Number of OP_REPLY with query failure flag set op_reply_valid_cursor Counter Number of OP_REPLY with a valid cursor cx_destroy_local_with_active_rq Counter Connections destroyed locally with an active query cx_destroy_remote_with_active_rq Counter Connections destroyed remotely with an active query cx_drain_close Counter Connections gracefully closed on reply boundaries during server drain Scatter gets Envoy defines a scatter get as any query that does not use an _id field as a query parameter. Envoy looks in both the top level document as well as within a $query field for _id. Multi gets Envoy defines a multi get as any query that does use an _id field as a query parameter, but where _id is not a scalar value (i.e., a document or an array). Envoy looks in both the top level document as well as within a $query field for _id. $comment parsing If a query has a top level $comment field (typically in addition to a $query field), Envoy will parse it as JSON and look for the following structure: { \"callingFunction\": \"...\" } callingFunction (required, string) the function that made the query. If available, the function will be used in callsite query statistics. Per command statistics The MongoDB filter will gather statistics for commands in the mongo..cmd..namespace. Name Type Description total Counter Number of commands reply_num_docs Histogram Number of documents in reply reply_size Histogram Size of the reply in bytes reply_time_ms Histogram Command time in milliseconds Per collection query statistics The MongoDB filter will gather statistics for queries in the mongo..collection..query. namespace. Name Type Description total Counter Number of queries scatter_get Counter Number of scatter gets multi_get Counter Number of multi gets reply_num_docs Histogram Number of documents in reply reply_size Histogram Size of the reply in bytes reply_time_ms Histogram Query time in milliseconds Per collection and callsite query statistics If the application provides the calling function in the $comment field, Envoy will generate per callsite statistics. These statistics match the per collection statistics but are found in the mongo..collection..callsite..query. namespace. Runtime The Mongo proxy filter supports the following runtime settings: mongo.connection_logging_enabled % of connections that will have logging enabled. Defaults to 100. This allows only a % of connections to have logging, but for all messages on those connections to be logged. mongo.proxy_enabled % of connections that will have the proxy enabled at all. Defaults to 100. mongo.logging_enabled % of messages that will be logged. Defaults to 100. If less than 100, queries may be logged without replies, etc. mongo.mongo.drain_close_enabled % of connections that will be drain closed if the server is draining and would otherwise attempt a drain close. Defaults to 100. mongo.fault.fixed_delay.percent Probability of an eligible MongoDB operation to be affected by the injected fault when there is no active fault. Defaults to the percent specified in the config. mongo.fault.fixed_delay.duration_ms The delay duration in milliseconds. Defaults to the duration_ms specified in the config. Access log format The access log format is not customizable and has the following layout: {\"time\": \"...\", \"message\": \"...\", \"upstream_host\": \"...\"} time System time that complete message was parsed, including milliseconds. message Textual expansion of the message. Whether the message is fully expanded depends on the context. Sometimes summary data is presented to avoid extremely large log sizes. upstream_host The upstream host that the connection is proxying to, if available. This is populated if the filter is used along with the TCP proxy filter. "},"configuration/network_filters/rate_limit_filter.html":{"url":"configuration/network_filters/rate_limit_filter.html","title":"速率限制","keywords":"","body":"速率限制 Global rate limiting architecture overview v1 API reference v2 API reference 统计 Every configured rate limit filter has statistics rooted at ratelimit.. with the following statistics: Name Type Description total Counter Total requests to the rate limit service error Counter Total errors contacting the rate limit service over_limit Counter Total over limit responses from the rate limit service ok Counter Total under limit responses from the rate limit service cx_closed Counter Total connections closed due to an over limit response from the rate limit service active Gauge Total active requests to the rate limit service 运行时 The network rate limit filter supports the following runtime settings: ratelimit.tcp_filter_enabled % of connections that will call the rate limit service. Defaults to 100. ratelimit.tcp_filter_enforcing % of connections that will call the rate limit service and enforce the decision. Defaults to 100. This can be used to test what would happen before fully enforcing the outcome. "},"configuration/network_filters/redis_proxy_filter.html":{"url":"configuration/network_filters/redis_proxy_filter.html","title":"Redis 代理","keywords":"","body":"Redis 代理 Redis architecture overview v1 API reference v2 API reference Statistics Every configured Redis proxy filter has statistics rooted at redis.. with the following statistics: Name Type Description downstream_cx_active Gauge Total active connections downstream_cx_protocol_error Counter Total protocol errors downstream_cx_rx_bytes_buffered Gauge Total received bytes currently buffered downstream_cx_rx_bytes_total Counter Total bytes received downstream_cx_total Counter Total connections downstream_cx_tx_bytes_buffered Gauge Total sent bytes currently buffered downstream_cx_tx_bytes_total Counter Total bytes sent downstream_cx_drain_close Counter Number of connections closed due to draining downstream_rq_active Gauge Total active requests downstream_rq_total Counter Total requests Splitter statistics The Redis filter will gather statistics for the command splitter in the redis..splitter. with the following statistics: Name Type Description invalid_request Counter Number of requests with an incorrect number of arguments unsupported_command Counter Number of commands issued which are not recognized by the command splitter Per command statistics The Redis filter will gather statistics for commands in the redis..command.. namespace. Name Type Description total Counter Number of commands Runtime The Redis proxy filter supports the following runtime settings: redis.drain_close_enabled % of connections that will be drain closed if the server is draining and would otherwise attempt a drain close. Defaults to 100. "},"configuration/network_filters/tcp_proxy_filter.html":{"url":"configuration/network_filters/tcp_proxy_filter.html","title":"TCP 代理","keywords":"","body":"TCP 代理 TCP proxy architecture overview v1 API reference v2 API reference Statistics The TCP proxy filter emits both its own downstream statistics as well as many of the cluster upstream statistics where applicable. The downstream statistics are rooted at tcp..with the following statistics: Name Type Description downstream_cx_total Counter Total number of connections handled by the filter downstream_cx_no_route Counter Number of connections for which no matching route was found or the cluster for the route was not found downstream_cx_tx_bytes_total Counter Total bytes written to the downstream connection downstream_cx_tx_bytes_buffered Gauge Total bytes currently buffered to the downstream connection downstream_cx_rx_bytes_total Counter Total bytes read from the downstream connection downstream_cx_rx_bytes_buffered Gauge Total bytes currently buffered from the downstream connection downstream_flow_control_paused_reading_total Counter Total number of times flow control paused reading from downstream downstream_flow_control_resumed_reading_total Counter Total number of times flow control resumed reading from downstream idle_timeout Counter Total number of connections closed due to idle timeout upstream_flush_total Counter Total number of connections that continued to flush upstream data after the downstream connection was closed upstream_flush_active Gauge Total connections currently continuing to flush upstream data after the downstream connection was closed "},"configuration/http_conn_man/http_conn_man.html":{"url":"configuration/http_conn_man/http_conn_man.html","title":"HTTP 连接管理器","keywords":"","body":"HTTP 连接管理器 HTTP 链接管理器架构概览 HTTP 协议架构概览 v1 API 参考 v2 API 参考 "},"configuration/http_conn_man/route_matching.html":{"url":"configuration/http_conn_man/route_matching.html","title":"路由匹配","keywords":"","body":"路由匹配 注意 本节为 v1 API编写，但概念也适用于 v2 API。 在未来的版本中将以 v2 API为目标重写为。 当 Envoy 匹配路由时，它使用如下步骤： HTTP 请求的 host 或者 :authority 头匹配到 虚拟主机。 按顺序检查虚拟主机中的每个 路由条目 。如果匹配，则使用该路由而不再进一步检查其他路由。 独立地，按顺序检查虚拟主机中的每个 虚拟集群 。如果匹配，则使用该虚拟集群而不再进一步检查其他虚拟集群。 "},"configuration/http_conn_man/traffic_splitting.html":{"url":"configuration/http_conn_man/traffic_splitting.html","title":"流量转移/拆分","keywords":"","body":"流量转移拆分 注意 本节为 v1 API 编写，但概念也适用于 v2 API。 在未来的版本中将以 v2 API 为目标重写为。 在两个上游之间转移流量 跨多个上游拆分流量 Envoy 的路由器可以跨两个或更多上游集群地将流量拆分到虚拟主机中的路由。有两个常见的用例。 版本升级：路由的流量逐渐从一个集群优雅地转移到另一个集群。 流量转移部分更详细地描述了这个场景。 A/B 测试或多变量测试：同时测试两个或更多版本的相同服务。流向路由的流量必须在运行同一服务的不同版本的集群之间进行拆分。 流量拆分部分更详细地描述了这种情况。 在两个上游之间转移流量 路由配置中的运行时对象判断选择特定路由（以及它的集群）的可能性（译者注：可以理解为百分比）。通过使用运行时配置，虚拟主机中到特定路由的流量可逐渐从一个集群转移到另一个集群。考虑以下示例配置，其中在 envoy 配置文件中声明了名为 helloworld 的服务的两个版本 helloworld_v1 和 helloworld_v2。 { \"route_config\": { \"virtual_hosts\": [ { \"name\": \"helloworld\", \"domains\": [\"*\"], \"routes\": [ { \"prefix\": \"/\", \"cluster\": \"helloworld_v1\", \"runtime\": { \"key\": \"routing.traffic_shift.helloworld\", \"default\": 50 } }, { \"prefix\": \"/\", \"cluster\": \"helloworld_v2\", } ] } ] } } Envoy 使用 first match 策略来匹配路由。如果路由具有运行时对象，则会根据运行时值另外匹配请求（如果未指定值，则为默认值）。因此，通过在上述示例中背靠背地放置路由并在第一个路由中指定运行时对象，可以通过更改运行时值来完成流量转移。以下是完成任务所需的大致操作顺序。 在开始时，将 routing.traffic_shift.helloworld 设置为 100, 因此所有到 helloworld 虚拟主机的请求都将匹配 v1 路由并由 helloworld_v1 集群提供服务。 为了开始将流量转移到 helloworld_v2 集群, 设置 routing.traffic_shift.helloworld 为 0 . 例如设置为 90 时，有1个不会与 v1 路由匹配，然后会落入 v2 路由。 逐渐减少 routing.traffic_shift.helloworld 中设置的值，以便大部分请求与 v2 路由匹配。 当 routing.traffic_shift.helloworld 设置为 0 时, 到 helloworld 虚拟主机的请求都不会匹配 v1 路由。现在，所有流量都会流向 v2 路由，并由 helloworld_v2 集群提供服务。 跨多个上游拆分流量 再次考虑 helloworld 示例，现在有三个版本（v1、v2和v3）而不是两个。要在三个版本间平均分配流量（即33％、33％、34％），可以使用 weighted_clusters选项指定每个上游集群的权重。 与前面的例子不同，单个路由条目就足够了。路由中的 weighted_clusters 配置块可用于指定多个上游集群以及权重，权证则表示要发送到每个上游集群的流量百分比。 { \"route_config\": { \"virtual_hosts\": [ { \"name\": \"helloworld\", \"domains\": [\"*\"], \"routes\": [ { \"prefix\": \"/\", \"weighted_clusters\": { \"runtime_key_prefix\" : \"routing.traffic_split.helloworld\", \"clusters\" : [ { \"name\" : \"helloworld_v1\", \"weight\" : 33 }, { \"name\" : \"helloworld_v2\", \"weight\" : 33 }, { \"name\" : \"helloworld_v3\", \"weight\" : 34 } ] } } ] } ] } } 默认情况下，权重必须总和精确为100。在 V2 API 中，总权重默认为100，但可以修改以允许更精细的粒度。 可以使用以下运行时变量动态调整分配给每个集群的权重：routing.traffic_split.helloworld.helloworld_v1，routing.traffic_split.helloworld.helloworld_v2 和 routing.traffic_split.helloworld.helloworld_v3。 "},"configuration/http_conn_man/headers.html":{"url":"configuration/http_conn_man/headers.html","title":"HTTP header 操作","keywords":"","body":"HTTP header manipulation The HTTP connection manager manipulates several HTTP headers both during decoding (when the request is being received) as well as during encoding (when the response is being sent). user-agent server x-client-trace-id x-envoy-downstream-service-cluster x-envoy-downstream-service-node x-envoy-external-address x-envoy-force-trace x-envoy-internal x-forwarded-client-cert x-forwarded-for x-forwarded-proto x-request-id x-ot-span-context x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags Custom request/response headers See https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers "},"configuration/http_conn_man/header_sanitizing.html":{"url":"configuration/http_conn_man/header_sanitizing.html","title":"HTTP header sanitizing","keywords":"","body":"HTTP header sanitizing For security reasons, Envoy will “sanitize” various incoming HTTP headers depending on whether the request is an internal or external request. The sanitizing action depends on the header and may result in addition, removal, or modification. Ultimately, whether the request is considered internal or external is governed by the x-forwarded-for header (please read the linked section carefully as how Envoy populates the header is complex and depends on the use_remote_address setting). Envoy will potentially sanitize the following headers: x-envoy-decorator-operation x-envoy-downstream-service-cluster x-envoy-downstream-service-node x-envoy-expected-rq-timeout-ms x-envoy-external-address x-envoy-force-trace x-envoy-internal x-envoy-ip-tags x-envoy-max-retries x-envoy-retry-grpc-on x-envoy-retry-on x-envoy-upstream-alt-stat-name x-envoy-upstream-rq-per-try-timeout-ms x-envoy-upstream-rq-timeout-alt-response x-envoy-upstream-rq-timeout-ms x-forwarded-client-cert x-forwarded-for x-forwarded-proto x-request-id "},"configuration/http_conn_man/stats.html":{"url":"configuration/http_conn_man/stats.html","title":"统计","keywords":"","body":"统计 Every connection manager has a statistics tree rooted at http.. with the following statistics: Name Type Description downstream_cx_total Counter Total connections downstream_cx_ssl_total Counter Total TLS connections downstream_cx_http1_total Counter Total HTTP/1.1 connections downstream_cx_websocket_total Counter Total WebSocket connections downstream_cx_http2_total Counter Total HTTP/2 connections downstream_cx_destroy Counter Total connections destroyed downstream_cx_destroy_remote Counter Total connections destroyed due to remote close downstream_cx_destroy_local Counter Total connections destroyed due to local close downstream_cx_destroy_active_rq Counter Total connections destroyed with 1+ active request downstream_cx_destroy_local_active_rq Counter Total connections destroyed locally with 1+ active request downstream_cx_destroy_remote_active_rq Counter Total connections destroyed remotely with 1+ active request downstream_cx_active Gauge Total active connections downstream_cx_ssl_active Gauge Total active TLS connections downstream_cx_http1_active Gauge Total active HTTP/1.1 connections downstream_cx_websocket_active Gauge Total active WebSocket connections downstream_cx_http2_active Gauge Total active HTTP/2 connections downstream_cx_protocol_error Counter Total protocol errors downstream_cx_length_ms Histogram Connection length milliseconds downstream_cx_rx_bytes_total Counter Total bytes received downstream_cx_rx_bytes_buffered Gauge Total received bytes currently buffered downstream_cx_tx_bytes_total Counter Total bytes sent downstream_cx_tx_bytes_buffered Gauge Total sent bytes currently buffered downstream_cx_drain_close Counter Total connections closed due to draining downstream_cx_idle_timeout Counter Total connections closed due to idle timeout downstream_flow_control_paused_reading_total Counter Total number of times reads were disabled due to flow control downstream_flow_control_resumed_reading_total Counter Total number of times reads were enabled on the connection due to flow control downstream_rq_total Counter Total requests downstream_rq_http1_total Counter Total HTTP/1.1 requests downstream_rq_http2_total Counter Total HTTP/2 requests downstream_rq_active Gauge Total active requests downstream_rq_response_before_rq_complete Counter Total responses sent before the request was complete downstream_rq_rx_reset Counter Total request resets received downstream_rq_tx_reset Counter Total request resets sent downstream_rq_non_relative_path Counter Total requests with a non-relative HTTP path downstream_rq_too_large Counter Total requests resulting in a 413 due to buffering an overly large body downstream_rq_1xx Counter Total 1xx responses downstream_rq_2xx Counter Total 2xx responses downstream_rq_3xx Counter Total 3xx responses downstream_rq_4xx Counter Total 4xx responses downstream_rq_5xx Counter Total 5xx responses downstream_rq_ws_on_non_ws_route Counter Total WebSocket upgrade requests rejected by non WebSocket routes downstream_rq_time Histogram Request time milliseconds rs_too_large Counter Total response errors due to buffering an overly large body Per user agent statistics Additional per user agent statistics are rooted at http..user_agent..Currently Envoy matches user agent for both iOS (ios) and Android (android) and produces the following statistics: Name Type Description downstream_cx_total Counter Total connections downstream_cx_destroy_remote_active_rq Counter Total connections destroyed remotely with 1+ active requests downstream_rq_total Counter Total requests Per listener statistics Additional per listener statistics are rooted at listener..http.. with the following statistics: Name Type Description downstream_rq_1xx Counter Total 1xx responses downstream_rq_2xx Counter Total 2xx responses downstream_rq_3xx Counter Total 3xx responses downstream_rq_4xx Counter Total 4xx responses downstream_rq_5xx Counter Total 5xx responses Per codec statistics Each codec has the option of adding per-codec statistics. Currently only http2 has codec stats. Http2 codec statistics All http2 statistics are rooted at http2. Name Type Description header_overflow Counter Total number of connections reset due to the headers being larger than Envoy::Http::Http2::ConnectionImpl::StreamImpl::MAX_HEADER_SIZE (63k) headers_cb_no_stream Counter Total number of errors where a header callback is called without an associated stream. This tracks an unexpected occurrence due to an as yet undiagnosed bug rx_messaging_error Counter Total number of invalid received frames that violated section 8 of the HTTP/2 spec. This will result in a tx_reset rx_reset Counter Total number of reset stream frames received by Envoy too_many_header_frames Counter Total number of times an HTTP2 connection is reset due to receiving too many headers frames. Envoy currently supports proxying at most one header frame for 100-Continue one non-100 response code header frame and one frame with trailers trailers Counter Total number of trailers seen on requests coming from downstream tx_reset Counter Total number of reset stream frames transmitted by Envoy Tracing statistics Tracing statistics are emitted when tracing decisions are made. All tracing statistics are rooted at http..tracing. with the following statistics: Name Type Description random_sampling Counter Total number of traceable decisions by random sampling service_forced Counter Total number of traceable decisions by server runtime flag tracing.global_enabled client_enabled Counter Total number of traceable decisions by request header x-envoy-force-trace not_traceable Counter Total number of non-traceable decisions by request id health_check Counter Total number of non-traceable decisions by health check "},"configuration/http_conn_man/runtime.html":{"url":"configuration/http_conn_man/runtime.html","title":"运行时","keywords":"","body":"运行时 The HTTP connection manager supports the following runtime settings: http_connection_manager.represent_ipv4_remote_address_as_ipv4_mapped_ipv6 % of requests with a remote address that will have their IPv4 address mapped to IPv6. Defaults to 0. use_remote_address must also be enabled. Seerepresent_ipv4_remote_address_as_ipv4_mapped_ipv6 for more details. tracing.client_enabled % of requests that will be force traced if the x-client-trace-id header is set. Defaults to 100. tracing.global_enabled % of requests that will be traced after all other checks have been applied (force tracing, sampling, etc.). Defaults to 100. tracing.random_sampling % of requests that will be randomly traced. See here for more information. This runtime control is specified in the range 0-10000 and defaults to 10000. Thus, trace sampling can be specified in 0.01% increments. "},"configuration/http_conn_man/rds.html":{"url":"configuration/http_conn_man/rds.html","title":"路由发现服务（RDS）","keywords":"","body":"路由发现服务（RDS） 路由发现服务（RDS）的API在 Envoy 里面是一个可选 API，用于动态获取路由配置。路由配置包括 HTTP 头部修改，虚拟主机以及每个虚拟主机中包含的单个路由规则。每个 HTTP连接管理器 都可以通过 API 独立地获取自身的路由配置。 v1 API reference v2 API reference 统计 RDS 的统计树以 http..rds..*.为根，route_config_name名称中的任何:字符在统计树中被替换为_。统计树包含以下统计信息： 名称 类型 描述 config_reload 计数器 加载配置不同导致重新调用API的总次数 update_attempt 计数器 调用API获取资源重试总数 update_success 计数器 调用API获取资源成功总数 update_failure 计数器 调用API获取资源失败总数（因网络、句法错误） version 测量 最后一次API获取资源成功的内容HASH "},"configuration/http_filters/buffer_filter.html":{"url":"configuration/http_filters/buffer_filter.html","title":"Buffer","keywords":"","body":"Buffer The buffer filter is used to stop filter iteration and wait for a fully buffered complete request. This is useful in different situations including protecting some applications from having to deal with partial requests and high network latency. v1 API reference v2 API reference Per-Route Configuration The buffer filter configuration can be overridden or disabled on a per-route basis by providing aBufferPerRoute configuration on the virtual host, route, or weighted cluster. Statistics The buffer filter outputs statistics in the http..buffer. namespace. The stat prefixcomes from the owning HTTP connection manager. Name Type Description rq_timeout Counter Total requests that timed out waiting for a full request "},"configuration/http_filters/cors_filter.html":{"url":"configuration/http_filters/cors_filter.html","title":"CORS","keywords":"","body":"CORS This is a filter which handles Cross-Origin Resource Sharing requests based on route or virtual host settings. For the meaning of the headers please refer to the pages below. https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS https://www.w3.org/TR/cors/ v1 API reference v2 API reference "},"configuration/http_filters/dynamodb_filter.html":{"url":"configuration/http_filters/dynamodb_filter.html","title":"DynamoDB","keywords":"","body":"DynamoDB DynamoDB architecture overview v1 API reference v2 API reference Statistics The DynamoDB filter outputs statistics in the http..dynamodb. namespace. The stat prefix comes from the owning HTTP connection manager. Per operation stats can be found in the http..dynamodb.operation.. namespace. Name Type Description upstream_rq_total Counter Total number of requests with upstream_rq_time Histogram Time spent on upstream_rq_total_xxx Counter Total number of requests with per response code (503/2xx/etc) upstream_rq_time_xxx Histogram Time spent on per response code (400/3xx/etc) Per table stats can be found in the http..dynamodb.table.. namespace. Most of the operations to DynamoDB involve a single table, but BatchGetItem and BatchWriteItem can include several tables, Envoy tracks per table stats in this case only if it is the same table used in all operations from the batch. Name Type Description upstream_rq_total Counter Total number of requests on table upstream_rq_time Histogram Time spent on table upstream_rq_total_xxx Counter Total number of requests on table per response code (503/2xx/etc) upstream_rq_time_xxx Histogram Time spent on table per response code (400/3xx/etc) Disclaimer: Please note that this is a pre-release Amazon DynamoDB feature that is not yet widely available. Per partition and operation stats can be found in the http..dynamodb.table.. namespace. For batch operations, Envoy tracks per partition and operation stats only if it is the same table used in all operations. Name Type Description capacity..__partition_id= Counter Total number of capacity for on table for a given Additional detailed stats: For 4xx responses and partial batch operation failures, the total number of failures for a given table and failure are tracked in the http..dynamodb.error..namespace. Name Type Description Counter Total number of specific for a given BatchFailureUnprocessedKeys Counter Total number of partial batch failures for a given Runtime The DynamoDB filter supports the following runtime settings: dynamodb.filter_enabled The % of requests for which the filter is enabled. Default is 100%. "},"configuration/http_filters/fault_filter.html":{"url":"configuration/http_filters/fault_filter.html","title":"故障注入","keywords":"","body":"故障注入 The fault injection filter can be used to test the resiliency of microservices to different forms of failures. The filter can be used to inject delays and abort requests with user-specified error codes, thereby providing the ability to stage different failure scenarios such as service failures, service overloads, high network latency, network partitions, etc. Faults injection can be limited to a specific set of requests based on the (destination) upstream cluster of a request and/or a set of pre-defined request headers. The scope of failures is restricted to those that are observable by an application communicating over the network. CPU and disk failures on the local host cannot be emulated. Currently, the fault injection filter has the following limitations: Abort codes are restricted to HTTP status codes only Delays are restricted to fixed duration. Future versions will include support for restricting faults to specific routes, injecting gRPC and HTTP/2 specific error codes and delay durations based on distributions. Configuration Note The fault injection filter must be inserted before any other filter, including the router filter. v1 API reference v2 API reference Runtime The HTTP fault injection filter supports the following global runtime settings: fault.http.abort.abort_percent % of requests that will be aborted if the headers match. Defaults to the abort_percent specified in config. If the config does not contain an abort block, then abort_percent defaults to 0. fault.http.abort.http_status HTTP status code that will be used as the of requests that will be aborted if the headers match. Defaults to the HTTP status code specified in the config. If the config does not contain an abortblock, then http_status defaults to 0. fault.http.delay.fixed_delay_percent % of requests that will be delayed if the headers match. Defaults to the delay_percent specified in the config or 0 otherwise. fault.http.delay.fixed_duration_ms The delay duration in milliseconds. If not specified, the fixed_duration_ms specified in the config will be used. If this field is missing from both the runtime and the config, no delays will be injected. Note, fault filter runtime settings for the specific downstream cluster override the default ones if present. The following are downstream specific runtime keys: fault.http..abort.abort_percent fault.http..abort.http_status fault.http..delay.fixed_delay_percent fault.http..delay.fixed_duration_ms Downstream cluster name is taken from the HTTP x-envoy-downstream-service-cluster header. If the following settings are not found in the runtime it defaults to the global runtime settings which defaults to the config settings. Statistics The fault filter outputs statistics in the http..fault. namespace. The stat prefix comes from the owning HTTP connection manager. Name Type Description delays_injected Counter Total requests that were delayed aborts_injected Counter Total requests that were aborted .delays_injected Counter Total delayed requests for the given downstream cluster .aborts_injected Counter Total aborted requests for the given downstream cluster "},"configuration/http_filters/grpc_http1_bridge_filter.html":{"url":"configuration/http_filters/grpc_http1_bridge_filter.html","title":"gRPC HTTP/1.1 bridge","keywords":"","body":"gRPC HTTP/1.1 bridge gRPC architecture overview v1 API reference v2 API reference This is a simple filter which enables the bridging of an HTTP/1.1 client which does not support response trailers to a compliant gRPC server. It works by doing the following: When a request is sent, the filter sees if the connection is HTTP/1.1 and the request content type is application/grpc. If so, when the response is received, the filter buffers it and waits for trailers and then checks the grpc-status code. If it is not zero, the filter switches the HTTP response code to 503. It also copies the grpc-status and grpc-message trailers into the response headers so that the client can look at them if it wishes. The client should send HTTP/1.1 requests that translate to the following pseudo headers: :method: POST :path: content-type: application/grpc The body should be the serialized grpc body which is: 1 byte of zero (not compressed). network order 4 bytes of proto message length. serialized proto message. Because this scheme must buffer the response to look for the grpc-status trailer it will only work with unary gRPC APIs. This filter also collects stats for all gRPC requests that transit, even if those requests are normal gRPC requests over HTTP/2. More info: wire format in gRPC over HTTP/2. Statistics The filter emits statistics in the cluster..grpc. namespace. Name Type Description ..success Counter Total successful service/method calls ..failure Counter Total failed service/method calls ..total Counter Total service/method calls "},"configuration/http_filters/grpc_json_transcoder_filter.html":{"url":"configuration/http_filters/grpc_json_transcoder_filter.html","title":"gRPC-JSON 转码","keywords":"","body":"gRPC-JSON 转码 gRPC architecture overview v1 API reference v2 API reference This is a filter which allows a RESTful JSON API client to send requests to Envoy over HTTP and get proxied to a gRPC service. The HTTP mapping for the gRPC service has to be defined by custom options. How to generate proto descriptor set Envoy has to know the proto descriptor of your gRPC service in order to do the transcoding. To generate a protobuf descriptor set for the gRPC service, you’ll also need to clone the googleapis repository from GitHub before running protoc, as you’ll need annotations.proto in your include path, to define the HTTP mapping. git clone https://github.com/googleapis/googleapis GOOGLEAPIS_DIR= Then run protoc to generate the descriptor set from bookstore.proto: protoc -I$(GOOGLEAPIS_DIR) -I. --include_imports --include_source_info \\ --descriptor_set_out=proto.pb test/proto/bookstore.proto If you have more than one proto source files, you can pass all of them in one command. "},"configuration/http_filters/grpc_web_filter.html":{"url":"configuration/http_filters/grpc_web_filter.html","title":"gRPC-Web","keywords":"","body":"gRPC-Web gRPC architecture overview v1 API reference v2 API reference This is a filter which enables the bridging of a gRPC-Web client to a compliant gRPC server by following https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-WEB.md. "},"configuration/http_filters/gzip_filter.html":{"url":"configuration/http_filters/gzip_filter.html","title":"Gzip","keywords":"","body":"Gzip Gzip is an HTTP filter which enables Envoy to compress dispatched data from an upstream service upon client request. Compression is useful in situations where large payloads need to be transmitted without compromising the response time. Configuration v2 API reference Attention The window bits is a number that tells the compressor how far ahead in the text the algorithm should be looking for repeated sequence of characters. Due to a known bug in the underlying zlib library, window bits with value eight does not work as expected. Therefore any number below that will be automatically set to 9. This issue might be solved in future releases of the library. How it works When gzip filter is enabled, request and response headers are inspected to determine whether or not the content should be compressed. The content is compressed and then sent to the client with the appropriate headers if either response and request allow. By default compression will be skipped when: A request does NOT contain accept-encoding header. A request includes accept-encoding header, but it does not contain “gzip”. A response contains a content-encoding header. A Response contains a cache-control header whose value includes “no-transform”. A response contains a transfer-encoding header whose value includes “gzip”. A response does not contain a content-type value that matches one of the selected mime-types, which default to application/javascript, application/json, application/xhtml+xml, image/svg+xml, text/css, text/html, text/plain, text/xml. Neither content-length nor transfer-encoding headers are present in the response. Response size is smaller than 30 bytes (only applicable when transfer-encoding is not chuncked). When compression is applied: The content-length is removed from response headers. Response headers contain “transfer-encoding: chunked” and “content-encoding: gzip”. The “vary: accept-encoding” header is inserted on every response. "},"configuration/http_filters/health_check_filter.html":{"url":"configuration/http_filters/health_check_filter.html","title":"健康检查","keywords":"","body":"健康检查 Health check filter architecture overview v1 API reference v2 API reference Note Note that the filter will automatically fail health checks and set the x-envoy-immediate-health-check-fail header if the /healthcheck/fail admin endpoint has been called. (The /healthcheck/okadmin endpoint reverses this behavior). "},"configuration/http_filters/ip_tagging_filter.html":{"url":"configuration/http_filters/ip_tagging_filter.html","title":"IP Tagging","keywords":"","body":"IP Tagging The HTTP IP Tagging filter sets the header x-envoy-ip-tags with the string tags for the trusted address from x-forwarded-for. If there are no tags for an address, the header is not set. The implementation for IP Tagging provides a scalable way to compare an IP address to a large list of CIDR ranges efficiently. The underlying algorithm for storing tags and IP address subnets is a Level-Compressed trie described in the paper IP-address lookup using LC-tries by S. Nilsson and G. Karlsson. Configuration v2 API reference Statistics The IP Tagging filter outputs statistics in the http..ip_tagging. namespace. The stat prefix comes from the owning HTTP connection manager. Name Type Description .hit Counter Total number of requests that have the applied to it no_hit Counter Total number of requests with no applicable IP tags total Counter Total number of requests the IP Tagging Filter operated on Runtime The IP Tagging filter supports the following runtime settings: ip_tagging.http_filter_enabled The % of requests for which the filter is enabled. Default is 100. "},"configuration/http_filters/lua_filter.html":{"url":"configuration/http_filters/lua_filter.html","title":"Lua","keywords":"","body":"Lua Attention By default Envoy is built without exporting symbols that you may need when interacting with Lua modules installed as shared objects. Envoy may need to be built with support for exported symbols. Please see the Bazel docs for more information. Overview The HTTP Lua filter allows Lua scripts to be run during both the request and response flows. LuaJITis used as the runtime. Because of this, the supported Lua version is mostly 5.1 with some 5.2 features. See the LuaJIT documentation for more details. The filter only supports loading Lua code in-line in the configuration. If local filesystem code is desired, a trivial in-line script can be used to load the rest of the code from the local environment. The design of the filter and Lua support at a high level is as follows: All Lua environments are per worker thread. This means that there is no truly global data. Any globals create and populated at load time will be visible from each worker thread in isolation. True global support may be added via an API in the future. All scripts are run as coroutines. This means that they are written in a synchronous style even though they may perform complex asynchronous tasks. This makes the scripts substantially easier to write. All network/async processing is performed by Envoy via a set of APIs. Envoy will yield the script as appropriate and resume it when async tasks are complete. Do not perform blocking operations from scripts. It is critical for performance that Envoy APIs are used for all IO. Currently supported high level features NOTE: It is expected that this list will expand over time as the filter is used in production. The API surface has been kept small on purpose. The goal is to make scripts extremely simple and safe to write. Very complex or high performance use cases are assumed to use the native C++ filter API. Inspection of headers, body, and trailers while streaming in either the request flow, response flow, or both. Modification of headers and trailers. Blocking and buffering the full request/response body for inspection. Performing an outbound async HTTP call to an upstream host. Such a call can be performed while buffering body data so that when the call completes upstream headers can be modified. Performing a direct response and skipping further filter iteration. For example, a script could make an upstream HTTP call for authentication, and then directly respond with a 403 response code. Configuration v1 API reference v2 API reference Script examples This section provides some concrete examples of Lua scripts as a more gentle introduction and quick start. Please refer to the stream handle API for more details on the supported API. -- Called on the request path. function envoy_on_request(request_handle) -- Wait for the entire request body and add a request header with the body size. request_handle:headers():add(\"request_body_size\", request_handle:body():length()) end -- Called on the response path. function envoy_on_response(response_handle) -- Wait for the entire response body and a response header with the the body size. response_handle:headers():add(\"response_body_size\", response_handle:body():length()) -- Remove a response header named 'foo' response_handle:headers():remove(\"foo\") end function envoy_on_request(request_handle) -- Make an HTTP call to an upstream host with the following headers, body, and timeout. local headers, body = request_handle:httpCall( \"lua_cluster\", { [\":method\"] = \"POST\", [\":path\"] = \"/\", [\":authority\"] = \"lua_cluster\" }, \"hello world\", 5000) -- Add information from the HTTP call into the headers that are about to be sent to the next -- filter in the filter chain. request_handle:headers():add(\"upstream_foo\", headers[\"foo\"]) request_handle:headers():add(\"upstream_body_size\", #body) end function envoy_on_request(request_handle) -- Make an HTTP call. local headers, body = request_handle:httpCall( \"lua_cluster\", { [\":method\"] = \"POST\", [\":path\"] = \"/\", [\":authority\"] = \"lua_cluster\" }, \"hello world\", 5000) -- Response directly and set a header from the HTTP call. No further filter iteration -- occurs. request_handle:respond( {[\":status\"] = \"403\", [\"upstream_foo\"] = headers[\"foo\"]}, \"nope\") end Stream handle API When Envoy loads the script in the configuration, it looks for two global functions that the script defines: function envoy_on_request(request_handle) end function envoy_on_response(response_handle) end A script can define either or both of these functions. During the request path, Envoy will run envoy_on_request as a coroutine, passing an API handle. During the response path, Envoy will run envoy_on_response as a coroutine, passing an API handle. Attention It is critical that all interaction with Envoy occur through the passed stream handle. The stream handle should not be assigned to any global variable and should not be used outside of the coroutine. Envoy will fail your script if the handle is used incorrectly. The following methods on the stream handle are supported: headers() headers = handle:headers() Returns the stream’s headers. The headers can be modified as long as they have not been sent to the next filter in the header chain. For example, they can be modified after an httpCall() or after a body() call returns. The script will fail if the headers are modified in any other situation. Returns a header object. body() body = handle:body() Returns the stream’s body. This call will cause Envoy to yield the script until the entire body has been buffered. Note that all buffering must adhere to the flow control policies in place. Envoy will not buffer more data than is allowed by the connection manager. Returns a buffer object. bodyChunks() iterator = handle:bodyChunks() Returns an iterator that can be used to iterate through all received body chunks as they arrive. Envoy will yield the script in between chunks, but will not buffer them. This can be used by a script to inspect data as it is streaming by. for chunk in request_handle:bodyChunks() do request_handle:log(0, chunk:length()) end Each chunk the iterator returns is a buffer object. trailers() trailers = handle:trailers() Returns the stream’s trailers. May return nil if there are no trailers. The trailers may be modified before they are sent to the next filter. Returns a header object. log*() handle:logTrace(message) handle:logDebug(message) handle:logInfo(message) handle:logWarn(message) handle:logErr(message) handle:logCritical(message) Logs a message using Envoy’s application logging. message is a string to log. httpCall() headers, body = handle:httpCall(cluster, headers, body, timeout) Makes an HTTP call to an upstream host. Envoy will yield the script until the call completes or has an error. cluster is a string which maps to a configured cluster manager cluster. headers is a table of key/value pairs to send. Note that the :method, :path, and :authority headers must be set. body is an optional string of body data to send. timeout is an integer that specifies the call timeout in milliseconds. Returns headers which is a table of response headers. Returns body which is the string response body. May be nil if there is no body. respond() handle:respond(headers, body) Respond immediately and do not continue further filter iteration. This call is only valid in the request flow. Additionally, a response is only possible if request headers have not yet been passed to subsequent filters. Meaning, the following Lua code is invalid: function envoy_on_request(request_handle) for chunk in request_handle:bodyChunks() do request_handle:respond( {[\":status\"] = \"100\"}, \"nope\") end end headers is a table of key/value pairs to send. Note that the :status header must be set. body is a string and supplies the optional response body. May be nil. metadata() metadata = handle:metadata() Returns the current route entry metadata. Note that the metadata should be specified under the filter name i.e. envoy.lua. Below is an example of a metadata in a route entry. metadata: filter_metadata: envoy.lua: foo: bar baz: - bad - baz Returns a metadata object. Header object API add() headers:add(key, value) Adds a header. key is a string that supplies the header key. value is a string that supplies the header value. Attention Envoy treats certain headers specially. These are known as the O(1) or inline headers. The list of inline headers can be found here. If an inline header is already present in the header map, add()will have no effect. If attempting to add() a non-inline header, the additional header will be added so that the resultant headers contains multiple header entries with the same name. Consider using the replace function if want to replace a header with another value. Note also that we understand this behavior is confusing and we may change it in a future release. get() headers:get(key) Gets a header. key is a string that supplies the header key. Returns a string that is the header value or nil if there is no such header. __pairs() for key, value in pairs(headers) do end Iterates through every header. key is a string that supplies the header key. value is a string that supplies the header value. Attention In the currently implementation, headers cannot be modified during iteration. Additionally, if it is desired to modify headers after iteration, the iteration must be completed. Meaning, do not use break or any other mechanism to exit the loop early. This may be relaxed in the future. remove() headers:remove(key) Removes a header. key supplies the header key to remove. replace() headers:replace(key, value) Replaces a header. key is a string that supplies the header key. value is a string that supplies the header value. If the header does not exist, it is added as per the add() function. Buffer API length() size = buffer:length() Gets the size of the buffer in bytes. Returns an integer. getBytes() buffer:getBytes(index, length) Get bytes from the buffer. By default Envoy will not copy all buffer bytes to Lua. This will cause a buffer segment to be copied. index is an integer and supplies the buffer start index to copy. lengthis an integer and supplies the buffer length to copy. index + length must be less than the buffer length. Metadata object API get() metadata:get(key) Gets a metadata. key is a string that supplies the metadata key. Returns the corresponding value of the given metadata key. The type of the value can be: null, boolean, number, string and table. __pairs() for key, value in pairs(metadata) do end Iterates through every metadata entry. key is a string that supplies a metadata key. value is metadata entry value. "},"configuration/http_filters/rate_limit_filter.html":{"url":"configuration/http_filters/rate_limit_filter.html","title":"速率限制","keywords":"","body":"速率限制 Global rate limiting architecture overview v1 API reference v2 API reference The HTTP rate limit filter will call the rate limit service when the request’s route or virtual host has one or more rate limit configurations that match the filter stage setting. The route can optionally include the virtual host rate limit configurations. More than one configuration can apply to a request. Each configuration results in a descriptor being sent to the rate limit service. If the rate limit service is called, and the response for any of the descriptors is over limit, a 429 response is returned. Composing Actions Attention This section is written for the v1 API but the concepts also apply to the v2 API. It will be rewritten to target the v2 API in a future release. Each rate limit action on the route or virtual host populates a descriptor entry. A vector of descriptor entries compose a descriptor. To create more complex rate limit descriptors, actions can be composed in any order. The descriptor will be populated in the order the actions are specified in the configuration. Example 1 For example, to generate the following descriptor: (\"generic_key\", \"some_value\") (\"source_cluster\", \"from_cluster\") The configuration would be: { \"actions\" : [ { \"type\" : \"generic_key\", \"descriptor_value\" : \"some_value\" }, { \"type\" : \"source_cluster\" } ] } Example 2 If an action doesn’t append a descriptor entry, no descriptor is generated for the configuration. For the following configuration: { \"actions\" : [ { \"type\" : \"generic_key\", \"descriptor_value\" : \"some_value\" }, { \"type\" : \"remote_address\" }, { \"type\" : \"souce_cluster\" } ] } If a request did not set x-forwarded-for, no descriptor is generated. If a request sets x-forwarded-for, the the following descriptor is generated: (\"generic_key\", \"some_value\") (\"remote_address\", \"\") (\"source_cluster\", \"from_cluster\") Statistics The buffer filter outputs statistics in the cluster..ratelimit. namespace. 429 responses are emitted to the normal cluster dynamic HTTP statistics. Name Type Description ok Counter Total under limit responses from the rate limit service error Counter Total errors contacting the rate limit service over_limit Counter total over limit responses from the rate limit service Runtime The HTTP rate limit filter supports the following runtime settings: ratelimit.http_filter_enabled % of requests that will call the rate limit service. Defaults to 100. ratelimit.http_filter_enforcing % of requests that will call the rate limit service and enforce the decision. Defaults to 100. This can be used to test what would happen before fully enforcing the outcome. ratelimit..http_filter_enabled % of requests that will call the rate limit service for a given route_key specified in the rate limit configuration. Defaults to 100. "},"configuration/http_filters/router_filter.html":{"url":"configuration/http_filters/router_filter.html","title":"路由","keywords":"","body":"路由 The router filter implements HTTP forwarding. It will be used in almost all HTTP proxy scenarios that Envoy is deployed for. The filter’s main job is to follow the instructions specified in the configured route table. In addition to forwarding and redirection, the filter also handles retry, statistics, etc. v1 API reference v2 API reference HTTP headers The router consumes and sets various HTTP headers both on the egress/request path as well as on the ingress/response path. They are documented in this section. x-envoy-expected-rq-timeout-ms x-envoy-max-retries x-envoy-retry-on x-envoy-retry-grpc-on x-envoy-upstream-alt-stat-name x-envoy-upstream-canary x-envoy-upstream-rq-timeout-alt-response x-envoy-upstream-rq-timeout-ms x-envoy-upstream-rq-per-try-timeout-ms x-envoy-upstream-service-time x-envoy-original-path x-envoy-immediate-health-check-fail x-envoy-overloaded x-envoy-decorator-operation x-envoy-expected-rq-timeout-ms This is the time in milliseconds the router expects the request to be completed. Envoy sets this header so that the upstream host receiving the request can make decisions based on the request timeout, e.g., early exit. This is set on internal requests and is either taken from the x-envoy-upstream-rq-timeout-ms header or the route timeout, in that order. x-envoy-max-retries If a retry policy is in place, Envoy will default to retrying one time unless explicitly specified. The number of retries can be explicitly set in the route retry config or by using this header. If a retry policy is not configured and x-envoy-retry-on or x-envoy-retry-grpc-on headers are not specified, Envoy will not retry a failed request. A few notes on how Envoy does retries: The route timeout (set via x-envoy-upstream-rq-timeout-ms or the route configuration) includesall retries. Thus if the request timeout is set to 3s, and the first request attempt takes 2.7s, the retry (including backoff) has .3s to complete. This is by design to avoid an exponential retry/timeout explosion. Envoy uses a fully jittered exponential backoff algorithm for retries with a base time of 25ms. The first retry will be delayed randomly between 0-24ms, the 2nd between 0-74ms, the 3rd between 0-174ms and so on. If max retries is set both by header as well as in the route configuration, the maximum value is taken when determining the max retries to use for the request. x-envoy-retry-on Setting this header on egress requests will cause Envoy to attempt to retry failed requests (number of retries defaults to 1 and can be controlled by x-envoy-max-retries header or the route config retry policy). The value to which the x-envoy-retry-on header is set indicates the retry policy. One or more policies can be specified using a ‘,’ delimited list. The supported policies are: 5xx Envoy will attempt a retry if the upstream server responds with any 5xx response code, or does not respond at all (disconnect/reset/read timeout). (Includes connect-failure and refused-stream)NOTE: Envoy will not retry when a request exceeds x-envoy-upstream-rq-timeout-ms(resulting in a 504 error code). Use x-envoy-upstream-rq-per-try-timeout-ms if you want to retry when individual attempts take too long. x-envoy-upstream-rq-timeout-ms is an outer time limit for a request, including any retries that take place. gateway-error This policy is similar to the 5xx policy but will only retry requests that result in a 502, 503, or 504. connect-failure Envoy will attempt a retry if a request is failed because of a connection failure to the upstream server (connect timeout, etc.). (Included in 5xx)NOTE: A connection failure/timeout is a the TCP level, not the request level. This does not include upstream request timeouts specified via x-envoy-upstream-rq-timeout-ms or via route configuration. retriable-4xx Envoy will attempt a retry if the upstream server responds with a retriable 4xx response code. Currently, the only response code in this category is 409.NOTE: Be careful turning on this retry type. There are certain cases where a 409 can indicate that an optimistic locking revision needs to be updated. Thus, the caller should not retry and needs to read then attempt another write. If a retry happens in this type of case it will always fail with another 409. refused-stream Envoy will attempt a retry if the upstream server resets the stream with a REFUSED_STREAM error code. This reset type indicates that a request is safe to retry. (Included in 5xx) The number of retries can be controlled via the x-envoy-max-retries header or via the route configuration. Note that retry policies can also be applied at the route level. By default, Envoy will not perform retries unless you’ve configured them per above. x-envoy-retry-grpc-on Setting this header on egress requests will cause Envoy to attempt to retry failed requests (number of retries defaults to 1, and can be controlled by x-envoy-max-retries header or the route config retry policy). gRPC retries are currently only supported for gRPC status codes in response headers. gRPC status codes in trailers will not trigger retry logic. One or more policies can be specified using a ‘,’ delimited list. The supported policies are: cancelled Envoy will attempt a retry if the gRPC status code in the response headers is “cancelled” (1) deadline-exceeded Envoy will attempt a retry if the gRPC status code in the response headers is “deadline-exceeded” (4) resource-exhausted Envoy will attempt a retry if the gRPC status code in the response headers is “resource-exhausted” (8) As with the x-envoy-retry-grpc-on header, the number of retries can be controlled via the x-envoy-max-retries header Note that retry policies can also be applied at the route level. By default, Envoy will not perform retries unless you’ve configured them per above. x-envoy-upstream-alt-stat-name Setting this header on egress requests will cause Envoy to emit upstream response code/timing statistics to a dual stat tree. This can be useful for application level categories that Envoy doesn’t know about. The output tree is documented here. This should not be confused with alt_stat_name which is specified while defining the cluster and when provided specifies an alternative name for the cluster at the root of the statistic tree. x-envoy-upstream-canary If an upstream host sets this header, the router will use it to generate canary specific statistics. The output tree is documented here. x-envoy-upstream-rq-timeout-alt-response Setting this header on egress requests will cause Envoy to set a 204 response code (instead of 504) in the event of a request timeout. The actual value of the header is ignored; only its presence is considered. See also x-envoy-upstream-rq-timeout-ms. x-envoy-upstream-rq-timeout-ms Setting this header on egress requests will cause Envoy to override the route configuration. The timeout must be specified in millisecond units. See also x-envoy-upstream-rq-per-try-timeout-ms. x-envoy-upstream-rq-per-try-timeout-ms Setting this header on egress requests will cause Envoy to set a per try timeout on routed requests. This timeout must be x-envoy-upstream-rq-timeout-ms) or it is ignored. This allows a caller to set a tight per try timeout to allow for retries while maintaining a reasonable overall timeout. x-envoy-upstream-service-time Contains the time in milliseconds spent by the upstream host processing the request. This is useful if the client wants to determine service time compared to network latency. This header is set on responses. x-envoy-original-path If the route utilizes prefix_rewrite, Envoy will put the original path header in this header. This can be useful for logging and debugging. x-envoy-immediate-health-check-fail If the upstream host returns this header (set to any value), Envoy will immediately assume the upstream host has failed active health checking (if the cluster has been configured for active health checking). This can be used to fast fail an upstream host via standard data plane processing without waiting for the next health check interval. The host can become healthy again via standard active health checks. See the health checking overview for more information. x-envoy-overloaded If this header is set by upstream, Envoy will not retry. Currently the value of the header is not looked at, only its presence. Additionally, Envoy will set this header on the downstream response if a request was dropped due to either maintenance mode or upstream circuit breaking. x-envoy-decorator-operation If this header is present on ingress requests, its value will override any locally defined operation (span) name on the server span generated by the tracing mechanism. Similarly, if this header is present on an egress response, its value will override any locally defined operation (span) name on the client span. Statistics The router outputs many statistics in the cluster namespace (depending on the cluster specified in the chosen route). See here for more information. The router filter outputs statistics in the http.. namespace. The stat prefix comes from the owning HTTP connection manager. Name Type Description no_route Counter Total requests that had no route and resulted in a 404 no_cluster Counter Total requests in which the target cluster did not exist and resulted in a 404 rq_redirect Counter Total requests that resulted in a redirect response rq_direct_response Counter Total requests that resulted in a direct response rq_total Counter Total routed requests Virtual cluster statistics are output in the vhost..vcluster.. namespace and include the following statistics: Name Type Description upstreamrq Counter Aggregate HTTP response codes (e.g., 2xx, 3xx, etc.) upstreamrq Counter Specific HTTP response codes (e.g., 201, 302, etc.) upstream_rq_time Histogram Request time milliseconds Runtime The router filter supports the following runtime settings: upstream.base_retry_backoff_ms Base exponential retry back off time. See here for more information. Defaults to 25ms. upstream.maintenance_mode. % of requests that will result in an immediate 503 response. This overrides any routing behavior for requests that would have been destined for . This can be used for load shedding, failure injection, etc. Defaults to disabled. upstream.use_retry % of requests that are eligible for retry. This configuration is checked before any other retry configuration and can be used to fully disable retries across all Envoys if needed. "},"configuration/http_filters/squash_filter.html":{"url":"configuration/http_filters/squash_filter.html","title":"Squash","keywords":"","body":"Squash Squash is an HTTP filter which enables Envoy to integrate with Squash microservices debugger. Code: https://github.com/solo-io/squash, API Docs: https://squash.solo.io/ Overview The main use case for this filter is in a service mesh, where Envoy is deployed as a sidecar. Once a request marked for debugging enters the mesh, the Squash Envoy filter reports its ‘location’ in the cluster to the Squash server - as there is a 1-1 mapping between Envoy sidecars and application containers, the Squash server can find and attach a debugger to the application container. The Squash filter also holds the request until a debugger is attached (or a timeout occurs). This enables developers (via Squash) to attach a native debugger to the container that will handle the request, before the request arrive to the application code, without any changes to the cluster. Configuration v1 API reference v2 API reference How it works When the Squash filter encounters a request containing the header ‘x-squash-debug’ it will: Delay the incoming request. Contact the Squash server and request the creation of a DebugAttachment On the Squash server side, Squash will attempt to attach a debugger to the application Envoy proxies to. On success, it changes the state of the DebugAttachment to attached. Wait until the Squash server updates the DebugAttachment object’s state to attached (or error state) Resume the incoming request "},"configuration/cluster_manager/cluster_manager.html":{"url":"configuration/cluster_manager/cluster_manager.html","title":"集群管理器","keywords":"","body":"集群管理器 集群管理器 架构概览 v1 API 参考 v2 API 参考 "},"configuration/cluster_manager/cluster_stats.html":{"url":"configuration/cluster_manager/cluster_stats.html","title":"统计","keywords":"","body":"统计 General Health check statistics Outlier detection statistics Dynamic HTTP statistics Alternate tree dynamic HTTP statistics Per service zone dynamic HTTP statistics Load balancer statistics Load balancer subset statistics General The cluster manager has a statistics tree rooted at cluster_manager. with the following statistics. Any : character in the stats name is replaced with _. Name Type Description cluster_added Counter Total clusters added (either via static config or CDS) cluster_modified Counter Total clusters modified (via CDS) cluster_removed Counter Total clusters removed (via CDS) active_clusters Gauge Number of currently active (warmed) clusters warming_clusters Gauge Number of currently warming (not active) clusters Every cluster has a statistics tree rooted at cluster.. with the following statistics: Name Type Description upstream_cx_total Counter Total connections upstream_cx_active Gauge Total active connections upstream_cx_http1_total Counter Total HTTP/1.1 connections upstream_cx_http2_total Counter Total HTTP/2 connections upstream_cx_connect_fail Counter Total connection failures upstream_cx_connect_timeout Counter Total connection connect timeouts upstream_cx_idle_timeout Counter Total connection idle timeouts upstream_cx_connect_attempts_exceeded Counter Total consecutive connection failures exceeding configured connection attempts upstream_cx_overflow Counter Total times that the cluster’s connection circuit breaker overflowed upstream_cx_connect_ms Histogram Connection establishment milliseconds upstream_cx_length_ms Histogram Connection length milliseconds upstream_cx_destroy Counter Total destroyed connections upstream_cx_destroy_local Counter Total connections destroyed locally upstream_cx_destroy_remote Counter Total connections destroyed remotely upstream_cx_destroy_with_active_rq Counter Total connections destroyed with 1+ active request upstream_cx_destroy_local_with_active_rq Counter Total connections destroyed locally with 1+ active request upstream_cx_destroy_remote_with_active_rq Counter Total connections destroyed remotely with 1+ active request upstream_cx_close_notify Counter Total connections closed via HTTP/1.1 connection close header or HTTP/2 GOAWAY upstream_cx_rx_bytes_total Counter Total received connection bytes upstream_cx_rx_bytes_buffered Gauge Received connection bytes currently buffered upstream_cx_tx_bytes_total Counter Total sent connection bytes upstream_cx_tx_bytes_buffered Gauge Send connection bytes currently buffered upstream_cx_protocol_error Counter Total connection protocol errors upstream_cx_max_requests Counter Total connections closed due to maximum requests upstream_cx_none_healthy Counter Total times connection not established due to no healthy hosts upstream_rq_total Counter Total requests upstream_rq_active Gauge Total active requests upstream_rq_pending_total Counter Total requests pending a connection pool connection upstream_rq_pending_overflow Counter Total requests that overflowed connection pool circuit breaking and were failed upstream_rq_pending_failure_eject Counter Total requests that were failed due to a connection pool connection failure upstream_rq_pending_active Gauge Total active requests pending a connection pool connection upstream_rq_cancelled Counter Total requests cancelled before obtaining a connection pool connection upstream_rq_maintenance_mode Counter Total requests that resulted in an immediate 503 due to maintenance mode upstream_rq_timeout Counter Total requests that timed out waiting for a response upstream_rq_per_try_timeout Counter Total requests that hit the per try timeout upstream_rq_rx_reset Counter Total requests that were reset remotely upstream_rq_tx_reset Counter Total requests that were reset locally upstream_rq_retry Counter Total request retries upstream_rq_retry_success Counter Total request retry successes upstream_rq_retry_overflow Counter Total requests not retried due to circuit breaking upstream_flow_control_paused_reading_total Counter Total number of times flow control paused reading from upstream upstream_flow_control_resumed_reading_total Counter Total number of times flow control resumed reading from upstream upstream_flow_control_backed_up_total Counter Total number of times the upstream connection backed up and paused reads from downstream upstream_flow_control_drained_total Counter Total number of times the upstream connection drained and resumed reads from downstream membership_change Counter Total cluster membership changes membership_healthy Gauge Current cluster healthy total (inclusive of both health checking and outlier detection) membership_total Gauge Current cluster membership total retry_or_shadow_abandoned Counter Total number of times shadowing or retry buffering was canceled due to buffer limits config_reload Counter Total API fetches that resulted in a config reload due to a different config update_attempt Counter Total cluster membership update attempts update_success Counter Total cluster membership update successes update_failure Counter Total cluster membership update failures update_empty Counter Total cluster membership updates ending with empty cluster load assignment and continuing with previous config update_no_rebuild Counter Total successful cluster membership updates that didn’t result in any cluster load balancing structure rebuilds version Gauge Hash of the contents from the last successful API fetch max_host_weight Gauge Maximum weight of any host in the cluster bind_errors Counter Total errors binding the socket to the configured source address Health check statistics If health check is configured, the cluster has an additional statistics tree rooted at cluster..health_check. with the following statistics: Name Type Description attempt Counter Number of health checks success Counter Number of successful health checks failure Counter Number of immediately failed health checks (e.g. HTTP 503) as well as network failures passive_failure Counter Number of health check failures due to passive events (e.g. x-envoy-immediate-health-check-fail) network_failure Counter Number of health check failures due to network error verify_cluster Counter Number of health checks that attempted cluster name verification healthy Gauge Number of healthy members Outlier detection statistics If outlier detection is configured for a cluster, statistics will be rooted at cluster..outlier_detection. and contain the following: Name Type Description ejections_enforced_total Counter Number of enforced ejections due to any outlier type ejections_active Gauge Number of currently ejected hosts ejections_overflow Counter Number of ejections aborted due to the max ejection % ejections_enforced_consecutive_5xx Counter Number of enforced consecutive 5xx ejections ejections_detected_consecutive_5xx Counter Number of detected consecutive 5xx ejections (even if unenforced) ejections_enforced_success_rate Counter Number of enforced success rate outlier ejections ejections_detected_success_rate Counter Number of detected success rate outlier ejections (even if unenforced) ejections_enforced_consecutive_gateway_failure Counter Number of enforced consecutive gateway failure ejections ejections_detected_consecutive_gateway_failure Counter Number of detected consecutive gateway failure ejections (even if unenforced) ejections_total Counter Deprecated. Number of ejections due to any outlier type (even if unenforced) ejections_consecutive_5xx Counter Deprecated. Number of consecutive 5xx ejections (even if unenforced) Dynamic HTTP statistics If HTTP is used, dynamic HTTP response code statistics are also available. These are emitted by various internal systems as well as some filters such as the router filter and rate limit filter. They are rooted at cluster.. and contain the following statistics: Name Type Description upstreamrq Counter Aggregate HTTP response codes (e.g., 2xx, 3xx, etc.) upstreamrq Counter Specific HTTP response codes (e.g., 201, 302, etc.) upstream_rq_time Histogram Request time milliseconds canary.upstreamrq Counter Upstream canary aggregate HTTP response codes canary.upstreamrq Counter Upstream canary specific HTTP response codes canary.upstream_rq_time Histogram Upstream canary request time milliseconds internal.upstreamrq Counter Internal origin aggregate HTTP response codes internal.upstreamrq Counter Internal origin specific HTTP response codes internal.upstream_rq_time Histogram Internal origin request time milliseconds external.upstreamrq Counter External origin aggregate HTTP response codes external.upstreamrq Counter External origin specific HTTP response codes external.upstream_rq_time Histogram External origin request time milliseconds Alternate tree dynamic HTTP statistics If alternate tree statistics are configured, they will be present in the cluster...namespace. The statistics produced are the same as documented in the dynamic HTTP statistics section above. Per service zone dynamic HTTP statistics If the service zone is available for the local service (via --service-zone) and the upstream cluster, Envoy will track the following statistics in cluster..zone...namespace. Name Type Description upstreamrq Counter Aggregate HTTP response codes (e.g., 2xx, 3xx, etc.) upstreamrq Counter Specific HTTP response codes (e.g., 201, 302, etc.) upstream_rq_time Histogram Request time milliseconds Load balancer statistics Statistics for monitoring load balancer decisions. Stats are rooted at cluster.. and contain the following statistics: Name Type Description lb_recalculate_zone_structures Counter The number of times locality aware routing structures are regenerated for fast decisions on upstream locality selection lb_healthy_panic Counter Total requests load balanced with the load balancer in panic mode lb_zone_cluster_too_small Counter No zone aware routing because of small upstream cluster size lb_zone_routing_all_directly Counter Sending all requests directly to the same zone lb_zone_routing_sampled Counter Sending some requests to the same zone lb_zone_routing_cross_zone Counter Zone aware routing mode but have to send cross zone lb_local_cluster_not_ok Counter Local host set is not set or it is panic mode for local cluster lb_zone_number_differs Counter Number of zones in local and upstream cluster different lb_zone_no_capacity_left Counter Total number of times ended with random zone selection due to rounding error Load balancer subset statistics Statistics for monitoring load balancer subset decisions. Stats are rooted at cluster.. and contain the following statistics: Name Type Description lb_subsets_active Gauge Number of currently available subsets lb_subsets_created Counter Number of subsets created lb_subsets_removed Counter Number of subsets removed due to no hosts lb_subsets_selected Counter Number of times any subset was selected for load balancing lb_subsets_fallback Counter Number of times the fallback policy was invoked "},"configuration/cluster_manager/cluster_runtime.html":{"url":"configuration/cluster_manager/cluster_runtime.html","title":"运行时","keywords":"","body":"运行时 Upstream clusters support the following runtime settings: Active health checking health_check.min_interval Min value for the health checking interval. Default value is 0. The health checking interval will be between min_interval and max_interval. health_check.max_interval Max value for the health checking interval. Default value is MAX_INT. The health checking interval will be between min_interval and max_interval. health_check.verify_cluster What % of health check requests will be verified against the expected upstream service as the health check filter will write the remote service cluster into the response. Outlier detection See the outlier detection architecture overview for more information on outlier detection. The runtime parameters supported by outlier detection are the same as the static configuration parameters, namely: outlier_detection.consecutive_5xx consecutive_5XX setting in outlier detection outlier_detection.consecutive_gateway_failure consecutive_gateway_failure setting in outlier detection outlier_detection.interval_ms interval_ms setting in outlier detection outlier_detection.base_ejection_time_ms base_ejection_time_ms setting in outlier detection outlier_detection.max_ejection_percent max_ejection_percent setting in outlier detection outlier_detection.enforcing_consecutive_5xx enforcing_consecutive_5xx setting in outlier detection outlier_detection.enforcing_consecutive_gateway_failure enforcing_consecutive_gateway_failure setting in outlier detection outlier_detection.enforcing_success_rate enforcing_success_rate setting in outlier detection outlier_detection.success_rate_minimum_hosts success_rate_minimum_hosts setting in outlier detection outlier_detection.success_rate_request_volume success_rate_request_volume setting in outlier detection outlier_detection.success_rate_stdev_factor success_rate_stdev_factor setting in outlier detection Core upstream.healthy_panic_threshold Sets the panic threshold percentage. Defaults to 50%. upstream.use_http2 Whether the cluster utilizes the http2 feature if configured. Set to 0 to disable HTTP/2 even if the feature is configured. Defaults to enabled. upstream.weight_enabled Binary switch to turn on or off weighted load balancing. If set to non 0, weighted load balancing is enabled. Defaults to enabled. Zone aware load balancing upstream.zone_routing.enabled % of requests that will be routed to the same upstream zone. Defaults to 100% of requests. upstream.zone_routing.min_cluster_size Minimal size of the upstream cluster for which zone aware routing can be attempted. Default value is 6. If the upstream cluster size is smaller than min_cluster_size zone aware routing will not be performed. Circuit breaking circuit_breakers...max_connections Max connections circuit breaker setting circuit_breakers...max_pending_requests Max pending requests circuit breaker setting circuit_breakers...max_requests Max requests circuit breaker setting circuit_breakers...max_retries Max retries circuit breaker setting "},"configuration/cluster_manager/cds.html":{"url":"configuration/cluster_manager/cds.html","title":"集群发现服务（CDS）","keywords":"","body":"集群发现服务（CDS） 集群发现服务（CDS）是一个可选的API，Envoy将调用该API来动态获取集群管理成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加，修改或删除已知的集群。 注意 在 Envoy 配置中静态定义的任何群集都不能通过 CDS API 进行修改或删除。 v1 CDS API v2 CDS API 统计 CDS 的统计树以 cluster_manager.cds. 为根，统计如下： 名字 类型 描述 config_reload 计数器 因配置不同而导致配置重新加载的总次数 update_attempt 计数器 尝试调用配置加载API的总次数 update_success 计数器 调用配置加载API成功的总次数 update_failure 计数器 调用配置加载API失败的总次数（网络或参数错误） version 测量 来自上次成功调用配置加载API的内容哈希 "},"configuration/cluster_manager/cluster_hc.html":{"url":"configuration/cluster_manager/cluster_hc.html","title":"健康检查","keywords":"","body":"健康检查 Health checking architecture overview. If health checking is configured for a cluster, additional statistics are emitted. They are documented here. v1 API documentation. v2 API documentation. TCP health checking Attention This section is written for the v1 API but the concepts also apply to the v2 API. It will be rewritten to target the v2 API in a future release. The type of matching performed is the following (this is the MongoDB health check request and response): { \"send\": [ {\"binary\": \"39000000\"}, {\"binary\": \"EEEEEEEE\"}, {\"binary\": \"00000000\"}, {\"binary\": \"d4070000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"746573742e\"}, {\"binary\": \"24636d6400\"}, {\"binary\": \"00000000\"}, {\"binary\": \"FFFFFFFF\"}, {\"binary\": \"13000000\"}, {\"binary\": \"01\"}, {\"binary\": \"70696e6700\"}, {\"binary\": \"000000000000f03f\"}, {\"binary\": \"00\"} ], \"receive\": [ {\"binary\": \"EEEEEEEE\"}, {\"binary\": \"01000000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"0000000000000000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"11000000\"}, {\"binary\": \"01\"}, {\"binary\": \"6f6b\"}, {\"binary\": \"00000000000000f03f\"}, {\"binary\": \"00\"} ] } During each health check cycle, all of the “send” bytes are sent to the target server. Each binary block can be of arbitrary length and is just concatenated together when sent. (Separating into multiple blocks can be useful for readability). When checking the response, “fuzzy” matching is performed such that each binary block must be found, and in the order specified, but not necessarily contiguous. Thus, in the example above, “FFFFFFFF” could be inserted in the response between “EEEEEEEE” and “01000000” and the check would still pass. This is done to support protocols that insert non-deterministic data, such as time, into the response. Health checks that require a more complex pattern such as send/receive/send/receive are not currently possible. If “receive” is an empty array, Envoy will perform “connect only” TCP health checking. During each cycle, Envoy will attempt to connect to the upstream host, and consider it a success if the connection succeeds. A new connection is created for each health check cycle. "},"configuration/cluster_manager/cluster_circuit_breakers.html":{"url":"configuration/cluster_manager/cluster_circuit_breakers.html","title":"断路","keywords":"","body":"断路 Circuit Breaking architecture overview. v1 API documentation. v2 API documentation. Runtime All circuit breaking settings are runtime configurable for all defined priorities based on cluster name. They follow the following naming scheme circuit_breakers.... cluster_name is the name field in each cluster’s configuration, which is set in the envoy config file. Available runtime settings will override settings set in the envoy config file. "},"configuration/health_checkers/redis.html":{"url":"configuration/health_checkers/redis.html","title":"Redis","keywords":"","body":"Redis Redis 健康检查器是一个用于检查 Redis 上游主机的定制检查器。主要通过发送 PING 命令和接收 PONG 命令进行工作。上游 Redis 主机可以使用 PONG 以外的任何其他响应来导致立即激活的运行状况检查失败。或者，Envoy 可以在用户指定的密钥上执行 EXISTS。如果密钥不存在，则认为它是合格的健康检查。这允许用户通过将执行的密钥设置为任意值并等待流量耗尽来标记 Redis 实例以进行维护。 v2 API reference "},"configuration/access_log.html":{"url":"configuration/access_log.html","title":"访问记录","keywords":"","body":"访问记录 Configuration Access logs are configured as part of the HTTP connection manager config or TCP Proxy. v1 API reference v2 API reference Format rules The access log format string contains either command operators or other characters interpreted as a plain string. The access log formatter does not make any assumptions about a new line separator, so one has to specified as part of the format string. See the default format for an example. Note that the access log line will contain a ‘-‘ character for every not set/empty value. The same format strings are used by different types of access logs (such as HTTP and TCP). Some fields may have slightly different meanings, depending on what type of log it is. Differences are noted. The following command operators are supported: %START_TIME% HTTPRequest start time including milliseconds.TCPDownstream connection start time including milliseconds.START_TIME can be customized using a format string, for example: %START_TIME(%Y/%m/%dT%H:%M:%S%z %s)% %BYTES_RECEIVED% HTTPBody bytes received.TCPDownstream bytes received on connection. %PROTOCOL% HTTPProtocol. Currently either HTTP/1.1 or HTTP/2.TCPNot implemented (“-“). %RESPONSE_CODE% HTTPHTTP response code. Note that a response code of ‘0’ means that the server never sent the beginning of a response. This generally means that the (downstream) client disconnected.TCPNot implemented (“-“). %BYTES_SENT% HTTPBody bytes sent.TCPDownstream bytes sent on connection. %DURATION% HTTPTotal duration in milliseconds of the request from the start time to the last byte out.TCPTotal duration in milliseconds of the downstream connection. %RESPONSE_FLAGS% Additional details about the response or connection, if any. For TCP connections, the response codes mentioned in the descriptions do not apply. Possible values are:HTTP and TCPUH: No healthy upstream hosts in upstream cluster in addition to 503 response code.UF: Upstream connection failure in addition to 503 response code.UO: Upstream overflow (circuit breaking) in addition to 503 response code.NR: No route configured for a given request in addition to 404 response code.HTTP onlyLH: Local service failed health check request in addition to 503 response code.UT: Upstream request timeout in addition to 504 response code.LR: Connection local reset in addition to 503 response code.UR: Upstream remote reset in addition to 503 response code.UC: Upstream connection termination in addition to 503 response code.DI: The request processing was delayed for a period specified via fault injection.FI: The request was aborted with a response code specified via fault injection.RL: The request was ratelimited locally by the HTTP rate limit filter in addition to 429 response code. %UPSTREAM_HOST% Upstream host URL (e.g., tcp://ip:port for TCP connections). %UPSTREAM_CLUSTER% Upstream cluster to which the upstream host belongs to. %UPSTREAM_LOCAL_ADDRESS% Local address of the upstream connection. If the address is an IP address it includes both address and port. %DOWNSTREAM_REMOTE_ADDRESS% Remote address of the downstream connection. If the address is an IP address it includes both address and port.NoteThis may not be the physical remote address of the peer if the address has been inferred from proxy proto or x-forwarded-for. %DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT% Remote address of the downstream connection. If the address is an IP address the output doesnot include port.NoteThis may not be the physical remote address of the peer if the address has been inferred from proxy proto or x-forwarded-for. %DOWNSTREAM_LOCAL_ADDRESS% Local address of the downstream connection. If the address is an IP address it includes both address and port. If the original connection was redirected by iptables REDIRECT, this represents the original destination address restored by the Original Destination Filter using SO_ORIGINAL_DST socket option. If the original connection was redirected by iptables TPROXY, and the listener’s transparent option was set to true, this represents the original destination address and port. %DOWNSTREAM_LOCAL_ADDRESS_WITHOUT_PORT% Same as %DOWNSTREAM_LOCAL_ADDRESS% excluding port if the address is an IP address. %REQ(X?Y):Z% HTTPAn HTTP request header where X is the main HTTP header, Y is the alternative one, and Z is an optional parameter denoting string truncation up to Z characters long. The value is taken from the HTTP request header named X first and if it’s not set, then request header Y is used. If none of the headers are present ‘-‘ symbol will be in the log.TCPNot implemented (“-“). %RESP(X?Y):Z% HTTPSame as %REQ(X?Y):Z% but taken from HTTP response headers.TCPNot implemented (“-“). %TRAILER(X?Y):Z% HTTPSame as %REQ(X?Y):Z% but taken from HTTP response trailers.TCPNot implemented (“-“). %DYNAMIC_METADATA(NAMESPACE:KEY*):Z% HTTPDynamic Metadata info, where NAMESPACE is the the filter namespace used when setting the metadata, KEY is an optional lookup up key in the namespace with the option of specifying nested keys separated by ‘:’, and Z is an optional parameter denoting string truncation up to Z characters long. Dynamic Metadata can be set by filters using the RequestInfo API: setDynamicMetadata. The data will be logged as a JSON string. For example, for the following dynamic metadata:com.test.my_filter: {\"test_key\": \"foo\", \"test_object\": {\"inner_key\": \"bar\"}}%DYNAMIC_METADATA(com.test.my_filter)% will log: {\"test_key\": \"foo\", \"test_object\": {\"inner_key\": \"bar\"}}%DYNAMIC_METADATA(com.test.my_filter:test_key)% will log: \"foo\"%DYNAMIC_METADATA(com.test.my_filter:test_object)% will log: {\"inner_key\": \"bar\"}%DYNAMIC_METADATA(com.test.my_filter:test_object:inner_key)% will log: \"bar\"%DYNAMIC_METADATA(com.unknown_filter)% will log: -%DYNAMIC_METADATA(com.test.my_filter:unknown_key)% will log: -%DYNAMIC_METADATA(com.test.my_filter):25% will log (truncation at 25 characters): {\"test_key\": \"foo\", \"testTCPNot implemented (“-“). Default format If custom format is not specified, Envoy uses the following default format: [%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\\n Example of the default Envoy access log format: [2016-04-15T20:17:00.310Z] \"POST /api/v1/locations HTTP/2\" 204 - 154 0 226 100 \"10.0.35.28\" \"nsq2http\" \"cc21d9b0-cf5c-432b-8c7e-98aeb7988cd2\" \"locations\" \"tcp://10.0.2.1:80\" "},"configuration/rate_limit.html":{"url":"configuration/rate_limit.html","title":"速率限制服务","keywords":"","body":"速率限制服务 The rate limit service configuration specifies the global rate limit service Envoy should talk to when it needs to make global rate limit decisions. If no rate limit service is configured, a “null” service will be used which will always return OK if called. v1 API reference v2 API reference gRPC service IDL Envoy expects the rate limit service to support the gRPC IDL specified in/source/common/ratelimit/ratelimit.proto. See the IDL documentation for more information on how the API works. See Lyft’s reference implementation here. "},"configuration/runtime.html":{"url":"configuration/runtime.html","title":"运行时","keywords":"","body":"运行时 The runtime configuration specifies the location of the local file system tree that contains re-loadable configuration elements. Values can be viewed at the /runtime admin endpoint. Values can be modified and added at the /runtime_modify admin endpoint. If runtime is not configured, an empty provider is used which has the effect of using all defaults built into the code, except for any values added via /runtime_modify. Attention Use the /runtime_modify endpoint with care. Changes are effectively immediately. It is criticalthat the admin interface is properly secured. v1 API reference v2 API reference File system layout Various sections of the configuration guide describe the runtime settings that are available. For example, here are the runtime settings for upstream clusters. Assume that the folder /srv/runtime/v1 points to the actual file system path where global runtime configurations are stored. The following would be a typical configuration setting for runtime: symlink_root: /srv/runtime/current subdirectory: envoy override_subdirectory: envoy_override Where /srv/runtime/current is a symbolic link to /srv/runtime/v1. Each ‘.’ in a runtime key indicates a new directory in the hierarchy, rooted at symlink_root +subdirectory. For example, the health_check.min_interval key would have the following full file system path (using the symbolic link): /srv/runtime/current/envoy/health_check/min_interval The terminal portion of a path is the file. The contents of the file constitute the runtime value. When reading numeric values from a file, spaces and new lines will be ignored. The override_subdirectory is used along with the --service-cluster CLI option. Assume that --service-cluster has been set to my-cluster. Envoy will first look for thehealth_check.min_interval key in the following full file system path: /srv/runtime/current/envoy_override/my-cluster/health_check/min_interval If found, the value will override any value found in the primary lookup path. This allows the user to customize the runtime values for individual clusters on top of global defaults. Comments Lines starting with # as the first character are treated as comments. Comments can be used to provide context on an existing value. Comments are also useful in an otherwise empty file to keep a placeholder for deployment in a time of need. Updating runtime values via symbolic link swap There are two steps to update any runtime value. First, create a hard copy of the entire runtime tree and update the desired runtime values. Second, atomically swap the symbolic link root from the old tree to the new runtime tree, using the equivalent of the following command: /srv/runtime:~$ ln -s /srv/runtime/v2 new && mv -Tf new current It’s beyond the scope of this document how the file system data is deployed, garbage collected, etc. Statistics The file system runtime provider emits some statistics in the runtime. namespace. Name Type Description load_error Counter Total number of load attempts that resulted in an error override_dir_not_exists Counter Total number of loads that did not use an override directory override_dir_exists Counter Total number of loads that did use an override directory load_success Counter Total number of load attempts that were successful num_keys Gauge Number of keys currently loaded "},"configuration/statistics.html":{"url":"configuration/statistics.html","title":"统计","keywords":"","body":"统计 A few statistics are emitted to report statistics system behavior: Name Type Description stats.overflow Counter Total number of times Envoy cannot allocate a statistic due to a shortage of shared memory Server Server related statistics are rooted at server. with following statistics: Name Type Description uptime Gauge Current server uptime in seconds memory_allocated Gauge Current amount of allocated memory in bytes memory_heap_size Gauge Current reserved heap size in bytes live Gauge 1 if the server is not currently draining, 0 otherwise parent_connections Gauge Total connections of the old Envoy process on hot restart total_connections Gauge Total connections of both new and old Envoy processes version Gauge Integer represented version number based on SCM revision days_until_first_cert_expiring Gauge Number of days until the next certificate being managed will expire File system Statistics related to file system are emitted in the filesystem. namespace. Name Type Description write_buffered Counter Total number of times file data is moved to Envoy’s internal flush buffer write_completed Counter Total number of times a file was written flushed_by_timer Counter Total number of times internal flush buffers are written to a file due to flush timeout reopen_failed Counter Total number of times a file was failed to be opened write_total_buffered Gauge Current total size of internal flush buffer in bytes "},"configuration/tools/router_check.html":{"url":"configuration/tools/router_check.html","title":"路由表检查工具","keywords":"","body":"路由表检查工具 NOTE: The following configuration is for the route table check tool only and is not part of the Envoy binary. The route table check tool is a standalone binary that can be used to verify Envoy’s routing for a given configuration file. The following specifies input to the route table check tool. The route table check tool checks if the route returned by a router matches what is expected. The tool can be used to check cluster name, virtual cluster name, virtual host name, manual path rewrite, manual host rewrite, path redirect, and header field matches. Extensions for other test cases can be added. Details about installing the tool and sample tool input/output can be found at installation. The route table check tool config is composed of an array of json test objects. Each test object is composed of three parts. Test name This field specifies the name of each test object. Input values The input value fields specify the parameters to be passed to the router. Example input fields include the :authority, :path, and :method header fields. The :authority and :path fields specify the url sent to the router and are required. All other input fields are optional. Validate The validate fields specify the expected values and test cases to check. At least one test case is required. A simple tool configuration json has one test case and is written as follows. The test expects a cluster name match of “instant-server”.: [ { \"test_name: \"Cluster_name_test\", \"input\": { \":authority\":\"api.lyft.com\", \":path\": \"/api/locations\" }, \"validate\": { \"cluster_name\": \"instant-server\" } } ] [ { \"test_name\": \"...\", \"input\": { \":authority\": \"...\", \":path\": \"...\", \":method\": \"...\", \"internal\" : \"...\", \"random_value\" : \"...\", \"ssl\" : \"...\", \"additional_headers\": [ { \"field\": \"...\", \"value\": \"...\" }, { \"...\" } ] }, \"validate\": { \"cluster_name\": \"...\", \"virtual_cluster_name\": \"...\", \"virtual_host_name\": \"...\", \"host_rewrite\": \"...\", \"path_rewrite\": \"...\", \"path_redirect\": \"...\", \"header_fields\" : [ { \"field\": \"...\", \"value\": \"...\" }, { \"...\" } ] } }, { \"...\" } ] test_name (required, string) The name of a test object. input (required, object) Input values sent to the router that determine the returned route.:authority(required, string) The url authority. This value along with the path parameter define the url to be matched. An example authority value is “api.lyft.com”.:path(required, string) The url path. An example path value is “/foo”.:method(optional, string) The request method. If not specified, the default method is GET. The options are GET, PUT, or POST.internal(optional, boolean) A flag that determines whether to set x-envoy-internal to “true”. If not specified, or if internal is equal to false, x-envoy-internal is not set.random_value(optional, integer) An integer used to identify the target for weighted cluster selection. The default value of random_value is 0.ssl(optional, boolean) A flag that determines whether to set x-forwarded-proto to https or http. By setting x-forwarded-proto to a given protocol, the tool is able to simulate the behavior of a client issuing a request via http or https. By default ssl is false which corresponds to x-forwarded-proto set to http.additional_headers(optional, array) Additional headers to be added as input for route determination. The “:authority”, “:path”, “:method”, “x-forwarded-proto”, and “x-envoy-internal” fields are specified by the other config options and should not be set here.field(required, string) The name of the header field to add.value(required, string) The value of the header field to add. validate (required, object) The validate object specifies the returned route parameters to match. At least one test parameter must be specificed. Use “” (empty string) to indicate that no return value is expected. For example, to test that no cluster match is expected use {“cluster_name”: “”}.cluster_name(optional, string) Match the cluster name.virtual_cluster_name(optional, string) Match the virtual cluster name.virtual_host_name(optional, string) Match the virtual host name.host_rewrite(optional, string) Match the host header field after rewrite.path_rewrite(optional, string) Match the path header field after rewrite.path_redirect(optional, string) Match the returned redirect path.header_fields(optional, array) Match the listed header fields. Examples header fields include the “:path”, “cookie”, and “date” fields. The header fields are checked after all other test cases. Thus, the header fields checked will be those of the redirected or rewriten routes when applicable.field(required, string) The name of the header field to match.value(required, string) The value of the header field to match. "},"operations/cli.html":{"url":"operations/cli.html","title":"命令行选项","keywords":"","body":"命令行选项 Envoy is driven both by a JSON configuration file as well as a set of command line options. The following are the command line options that Envoy supports. -c`` ``, ``--config-path`` (optional) The path to the v1 or v2 JSON/YAML/proto3 configuration file. If this flag is missing, --config-yaml is required. This will be parsed as a v2 bootstrap configuration file and on failure, subject to --v2-config-only, will be considered as a v1 JSON configuration file. For v2 configuration files, valid extensions are .json, .yaml, .pb and .pb_text, which indicate JSON, YAML, binary proto3 and text proto3 formats respectively. --config-yaml`` (optional) The YAML string for a v2 bootstrap configuration. If --config-path is also set,the values in this YAML string will override and merge with the bootstrap loaded from --config-path. Because YAML is a superset of JSON, a JSON string may also be passed to --config-yaml. --config-yaml is not compatible with bootstrap v1.Example overriding the node id on the command line:./envoy -c bootstrap.yaml –config-yaml “node: {id: ‘node1’}” `--v2-config-only``` (optional) This flag determines whether the configuration file should only be parsed as a v2 bootstrap configuration file. If false (default), when a v2 bootstrap config parse fails, a second attempt to parse the config as a v1 JSON configuration file will be made. --mode`` (optional) One of the operating modes for Envoy:serve: (default) Validate the JSON configuration and then serve traffic normally.validate: Validate the JSON configuration and then exit, printing either an “OK” message (in which case the exit code is 0) or any errors generated by the configuration file (exit code 1). No network traffic is generated, and the hot restart process is not performed, so no other Envoy process on the machine will be disturbed. --admin-address-path`` (optional) The output file path where the admin address and port will be written. --local-address-ip-version`` (optional) The IP address version that is used to populate the server local IP address. This parameter affects various headers including what is appended to the X-Forwarded-For (XFF) header. The options are v4 or v6. The default is v4. --base-id`` (optional) The base ID to use when allocating shared memory regions. Envoy uses shared memory regions during hot restart. Most users will never have to set this option. However, if Envoy needs to be run multiple times on the same machine, each running Envoy will need a unique base ID so that the shared memory regions do not conflict. --concurrency`` (optional) The number of worker threads to run. If not specified defaults to the number of hardware threads on the machine. -l`` ``, ``--log-level`` (optional) The logging level. Non developers should generally never set this option. See the help text for the available log levels and the default. --log-path`` (optional) The output file path where logs should be written. This file will be re-opened when SIGUSR1 is handled. If this is not set, log to stderr. --log-format`` (optional) The format string to use for laying out the log message metadata. If this is not set, a default format string \"[%Y-%m-%d %T.%e][%t][%l][%n] %v\" is used.The supported format flags are (with example output):%v:The actual message to log (“some user text”)%t:Thread id (“1232”)%P:Process id (“3456”)%n:Logger’s name (“filter”)%l:The log level of the message (“debug”, “info”, etc.)%L:Short log level of the message (“D”, “I”, etc.)%a:Abbreviated weekday name (“Tue”)%A:Full weekday name (“Tuesday”)%b:Abbreviated month name (“Mar”)%B:Full month name (“March”)%c:Date and time representation (“Tue Mar 27 15:25:06 2018”)%C:Year in 2 digits (“18”)%Y:Year in 4 digits (“2018”)%D, %x:Short MM/DD/YY date (“03/27/18”)%m:Month 01-12 (“03”)%d:Day of month 01-31 (“27”)%H:Hours in 24 format 00-23 (“15”)%I:Hours in 12 format 01-12 (“03”)%M:Minutes 00-59 (“25”)%S:Seconds 00-59 (“06”)%e:Millisecond part of the current second 000-999 (“008”)%f:Microsecond part of the current second 000000-999999 (“008789”)%F:Nanosecond part of the current second 000000000-999999999 (“008789123”)%p:AM/PM (“AM”)%r:12-hour clock (“03:25:06 PM”)%R:24-hour HH:MM time, equivalent to %H:%M (“15:25”)%T, %X:ISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S (“13:25:06”)%z:ISO 8601 offset from UTC in timezone ([+/-]HH:MM) (“-07:00”)%%:The % sign (“%”) --restart-epoch`` (optional) The hot restart epoch. (The number of times Envoy has been hot restarted instead of a fresh start). Defaults to 0 for the first start. This option tells Envoy whether to attempt to create the shared memory region needed for hot restart, or whether to open an existing one. It should be incremented every time a hot restart takes place. The hot restart wrapper sets the RESTART_EPOCH environment variable which should be passed to this option in most cases. `--hot-restart-version``` (optional) Outputs an opaque hot restart compatibility version for the binary. This can be matched against the output of the GET /hot_restart_version admin endpoint to determine whether the new binary and the running binary are hot restart compatible. --service-cluster`` (optional) Defines the local service cluster name where Envoy is running. The local service cluster name is first sourced from the Bootstrap node message’s cluster field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if any of the following features are used: statsd, health check cluster verification, runtime override directory, user agent addition, HTTP global rate limiting, CDS, and HTTP tracing, either via this CLI option or in the bootstrap configuration. --service-node`` (optional) Defines the local service node name where Envoy is running. The local service node name is first sourced from the Bootstrap node message’s id field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if any of the following features are used: statsd, CDS, and HTTP tracing, either via this CLI option or in the bootstrap configuration. --service-zone`` (optional) Defines the local service zone where Envoy is running. The local service zone is first sourced from the Bootstrap node message’s locality.zone field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if discovery service routing is used and the discovery service exposes zone data, either via this CLI option or in the bootstrap configuration. The meaning of zone is context dependent, e.g. Availability Zone (AZ) on AWS, Zone on GCP, etc. --file-flush-interval-msec`` (optional) The file flushing interval in milliseconds. Defaults to 10 seconds. This setting is used during file creation to determine the duration between flushes of buffers to files. The buffer will flush every time it gets full, or every time the interval has elapsed, whichever comes first. Adjusting this setting is useful when tailing access logs in order to get more (or less) immediate flushing. --drain-time-s`` (optional) The time in seconds that Envoy will drain connections during a hot restart. See thehot restart overview for more information. Defaults to 600 seconds (10 minutes). Generally the drain time should be less than the parent shutdown time set via the --parent-shutdown-time-soption. How the two settings are configured depends on the specific deployment. In edge scenarios, it might be desirable to have a very long drain time. In service to service scenarios, it might be possible to make the drain and shutdown time much shorter (e.g., 60s/90s). --parent-shutdown-time-s`` (optional) The time in seconds that Envoy will wait before shutting down the parent process during a hot restart. See the hot restart overview for more information. Defaults to 900 seconds (15 minutes). --max-obj-name-len`` (optional) The maximum name length (in bytes) of the name field in a cluster/route_config/listener. This setting is typically used in scenarios where the cluster names are auto generated, and often exceed the built-in limit of 60 characters. Defaults to 60.AttentionThis setting affects the output of --hot-restart-version. If you started envoy with this option set to a non default value, you should use the same option (and same value) for subsequent hot restarts. --max-stats`` (optional) The maximum number of stats that can be shared between hot-restarts. This setting affects the output of --hot-restart-version; the same value must be used to hot restart. Defaults to 16384. `--disable-hot-restart``` (optional) This flag disables Envoy hot restart for builds that have it enabled. By default, hot restart is enabled. "},"operations/hot_restarter.html":{"url":"operations/hot_restarter.html","title":"热重启 Python 包装器","keywords":"","body":"热重启 Python 包装器 通常情况下，Envoy 将会以热重启的方式进行配置变更和二进制更新。但是，在很多情况下，用户会希望使用标准进程管理器，例如 monit、runit 等。我们提供 /restarter/hot-restarter.py 来使这个过程简单明了。 调用重启程序方式如下: hot-restarter.py start_envoy.sh start_envoy.sh 可能像这样定义（使用 salt/jinja 类似的语法）： #!/bin/bash ulimit -n {{ pillar.get('envoy_max_open_files', '102400') }} exec /usr/sbin/envoy -c /etc/envoy/envoy.cfg --restart-epoch $RESTART_EPOCH --service-cluster {{ grains['cluster_name'] }} --service-node {{ grains['service_node'] }} --service-zone {{ grains.get('ec2_availability-zone', 'unknown') }} 在每次重启时，RESTART_EPOCH 环境变量是由重启程序设置，并且可以传递给 --restart-epoch 选项 重启程序可以处理以下信号： SIGTERM：将干净地终止所有子进程并退出。 SIGHUP：将重新调用作为第一个参数传递给热重启程序的脚本，来进行热重启。 SIGCHLD：如果任何子进程意外关闭，那么重启脚本将关闭所有内容并退出以避免处于意外状态。随后，控制进程管理器应该重新启动重启脚本以再次启动Envoy。 SIGUSR1：将作为重新打开所有访问日志的信号，转发给Envoy。可用于原子移动以及重新打开日志轮转。 "},"operations/admin.html":{"url":"operations/admin.html","title":"管理接口","keywords":"","body":"管理接口 Envoy 在本地提供了一个管理界面，可以使用这一界面查询或修改服务器的各种数据。 v1 API 参考 v2 API 参考 注意 目前管理界面可以进行破坏性操作（例如关闭服务器），也可能暴露私有数据（例如统计数据、集群名称、证书信息等）。将管理界面限制到只能在安全网络之内进行访问是 非常必要 的。同时还要注意，提供管理界面服务的网络接口只接入到安全网络之中（防止 CSRF 攻击等目的）。要实现这些目标，可以进行相应的防火墙设置，或者只允许 localhost 访问。可以使用如下的 v2 配置来完成： admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } 未来还会在管理界面中加入更多的安全相关的选项。这部分工作的进度在 Github Issue #2763 上进行跟踪。 所有的变更操作都应该通过 HTTP POST 方式进行。一段时间之内还是允许 HTTP GET 访问的，但是会有一条 Warning 日志。 GET / 渲染一个 HTML 主页，其中包含指向所有可用选项的链接。 GET /help 以字符表格的形式输出所有可用选项。 GET /certs 列出所有载入的 TLS 认证，包括文件名、序列号以及过期时间。 GET /clusters 列出集群管理器中配置的所有集群。这种信息中包含了已被发现的每个集群中的所有上游主机，以及每个主机的统计信息。如果服务发现出现问题需要除错，这些信息就很有帮助了。 集群管理器信息 version_info：string，最近载入的 CDS 的版本信息字符串。如果 Envoy 没有设置 CDS，则会输出来自于 version_info::static。 集群层信息 各优先级的断路器设置。 如果设置了外部检测，则显示相关信息。目前包含了平均成功率以及驱逐阈值的检测器。上一个检测周期中所获取的数据量不足，这些变量的值会被设置为-1。 added_via_api 标志：如果是静态配置添加的集群，这个项目的值就是 false；如果是使用 CDS API 添加的集群，这个值就会设置为 true。 主机统计数据 名称 类型 描述 cx_total Counter 连接总数 cx_active Gauge 活动连接总数 cx_connect_fail Counter 连接失败总数 rq_total Counter 请求总数 rq_timeout Counter 超时请求总数 rq_success Counter 收到非 5xx 响应的请求总数 rq_error Counter 收到 5xx 响应的请求总数 rq_active Gauge 活动请求总数 healthy String 主机健康状态，下面会有讲解 weight Integer 负载均衡权重（0-100） zone String 服务区域 canary Boolean 本主机是否为金丝雀 success_rate Double 请求成功率（0 - 100）。如果统计周期内没有足够的请求量进行运算，则返回 -1 主机健康状态 如果主机是健康的，那么 healthy 的输出为 healthy。 如果主机不健康，则 healthy 返回的是下面几个状态之一： /failed_active_hc：主动健康监测失败。 /failed_eds_health：EDS 标记该主机不健康。 /failed_outlier_check：外部检测失败。 GET /config_dump 用 JSON 序列化格式从多种 Envoy 组件中导出当前的配置。可以延伸阅读 response definition 的内容，来获得更详细的信息。 POST /cpuprofiler 启用或停用 CPU Profiler。编译时需要启用 gperftools。 POST /healthcheck/fail 设置健康状况为失败。需要配合 HTTP 健康检查过滤器来使用。这一功能可以停用一个服务器而无需进行关闭或者重启动操作。这个命令执行之后，不论过滤器如何设置，都会把全局范围内的健康检查设为失败。 POST /healthcheck/ok POST /healthcheck/fail 的逆向操作。同样需要配合 HTTP 健康检查过滤器来使用。 GET /hot_restart_version 参考 --hot-restart-version。 POST /logging 在不同的子组件上启用或者禁用不同级别的日志。一般只会在开发期间使用。 POST /quitquitquit 完全退出服务。 POST /reset_counters 把所有的计数器复位为 0。这在使用 GET /stats 协助调试的时候是很有用的功能。注意这个功能并不会删除任何发送给 statsd 的数据，它只会对 GET /stats 命令的输出造成影响。 GET /server_info 输出关于服务器的信息，内容格式类似： envoy 267724/RELEASE live 1571 1571 0 其中的字段包括： 进程名称 编译的 SHA 以及 Build 类型 当前热启动周期的启动时间 总的启动时间（包括所有的热启动周期） 当前的热启动周期 GET /stats 按需输出所有的统计内容。这个命令对于本地调试非常有用。Histograms 能够计算分位数并进行输出，即 P0，P25，P50，P75，P90，P99，P99.9和P100。每个分位数都是一种（区间值，累计值）的形式，区间值代表的是上次刷新以后的数值，累计值代表的是该实例启动以后的总计值。统计概览章节中提供了更多这方面的内容。 GET /stats?format=json 使用 JSON 格式输出 /stats，编程访问统计信息时可以使用这一格式。Counter 和 Gauge 会以（键，值）的形式出现。Histograms 会放在 \"histograms\" 节点之下，其中包含了 \"supported_quantiles\" 节点，其中列出了支持的分位数，以及这些分位数的计算结果。只有包含值的 Histograms 才会输出。 如果一个 Histogram 在这一区间没有更新，那么这一区间的所有分位数输出都是空的。 下面是一个输出样例： { \"histograms\": { \"supported_quantiles\": [ 0, 25, 50, 75, 90, 95, 99, 99.9, 100 ], \"computed_quantiles\": [ { \"name\": \"cluster.external_auth_cluster.upstream_cx_length_ms\", \"values\": [ {\"interval\": 0, \"cumulative\": 0}, {\"interval\": 0, \"cumulative\": 0}, {\"interval\": 1.0435787, \"cumulative\": 1.0435787}, {\"interval\": 1.0941565, \"cumulative\": 1.0941565}, {\"interval\": 2.0860023, \"cumulative\": 2.0860023}, {\"interval\": 3.0665233, \"cumulative\": 3.0665233}, {\"interval\": 6.046609, \"cumulative\": 6.046609}, {\"interval\": 229.57333,\"cumulative\": 229.57333}, {\"interval\": 260,\"cumulative\": 260} ] }, { \"name\": \"http.admin.downstream_rq_time\", \"values\": [ {\"interval\": null, \"cumulative\": 0}, {\"interval\": null, \"cumulative\": 0}, {\"interval\": null, \"cumulative\": 1.0435787}, {\"interval\": null, \"cumulative\": 1.0941565}, {\"interval\": null, \"cumulative\": 2.0860023}, {\"interval\": null, \"cumulative\": 3.0665233}, {\"interval\": null, \"cumulative\": 6.046609}, {\"interval\": null, \"cumulative\": 229.57333}, {\"interval\": null, \"cumulative\": 260} ] } ] } } GET /stats?format=prometheus 或者换个方式 GET /stats/prometheus，使用 Prometheus v0.0.4 格式输出统计数据。这样就可以和 Prometheus 进行集成了。当前只有 Counter 和 Gauge 会进行输出。未来的更新中会输出 Histogram。 GET /runtime 使用 JSON 格式按需输出所有运行时数值。运行时配置一节中，更详细的讲述了这些值的配置和使用。输出内容包括活动的重载后的运行时数值，以及每个键的堆栈。空字符串代表没有值，来自堆栈的最终有效值会用单独的键来做出标识，例如下面的输出： { \"layers\": [ \"disk\", \"override\", \"admin\", ], \"entries\": { \"my_key\": { \"layer_values\": [ \"my_disk_value\", \"\", \"\" ], \"final_value\": \"my_disk_value\" }, \"my_second_key\": { \"layer_values\": [ \"my_second_disk_value\", \"my_disk_override_value\", \"my_admin_override_value\" ], \"final_value\": \"my_admin_override_value\" } } } POST /runtime_modify?key1=value1&key2=value2&keyN=valueN 通过提交的参数对运行时数值进行添加或修改。要删除一个之前加入的键，只需要使用一个空值即可。注意这种删除操作，只适用于这一端点中使用重载方式加入的值；从磁盘中载入的值是能通过重载进行修改，无法删除。 注意 使用 /runtime_modify 端点要当心，这一变更是即时生效的。保障管理界面的安全性至关重要。 "},"operations/stats_overview.html":{"url":"operations/stats_overview.html","title":"统计概览","keywords":"","body":"统计概览 Envoy 基于服务器的配置输出数字统计信息。统计信息在本地可以通过 GET /stats 命令查看，通常情况下，统计信息会发送到 statsd cluster。输出的统计信息记录在配置指南的相关部分中。一些更重要的统计信息总是会被使用到，这些信息可以查阅以下章节： HTTP 链接管理器 Upstream 集群 "},"operations/runtime.html":{"url":"operations/runtime.html","title":"运行时","keywords":"","body":"运行时 Runtime configuration can be used to modify various server settings without restarting Envoy. The runtime settings that are available depend on how the server is configured. They are documented in the relevant sections of the configuration guide. "},"operations/fs_flags.html":{"url":"operations/fs_flags.html","title":"文件系统标志","keywords":"","body":"文件系统标志 Envoy supports file system “flags” that alter state at startup. This is used to persist changes between restarts if necessary. The flag files should be placed in the directory specified in the flags_path configuration option. The currently supported flag files are: drain If this file exists, Envoy will start in HC failing mode, similar to after the POST /healthcheck/failcommand has been executed. "},"operations/traffic_capture.html":{"url":"operations/traffic_capture.html","title":"流量捕获","keywords":"","body":"流量捕获 Envoy 当前提供了一个实验性的传输套接字扩展，用于拦截流量并写入一个 protobuf 文件中。 警告 这个功能是实验性的，并存在一个已知的问题，当在给定的 socket 上出现很长的跟踪调用的时候会 OOM。 如果担心存在安全问题，也可以在构建时禁用它，请参阅https://github.com/envoyproxy/envoy/blob/master/bazel/README.md#disabling-extensions. 配置 捕获行为可以被配置在 Listener 和 Cluster 上，提供了一种能力，可以分别针对上行流量和下行流量进行拦截。 要配置流量捕获, 添加一个envoy.transport_sockets.capture配置到 listener 或 cluster 上. 如： transport_socket: name: envoy.transport_sockets.capture config: file_sink: path_prefix: /some/capture/path transport_socket: name: raw_buffer 若支持 TLS, 如： transport_socket: name: envoy.transport_sockets.capture config: file_sink: path_prefix: /some/capture/path transport_socket: name: ssl config: 这里的 TLS 配置会分别替换现有在 listener 或 cluster 上 下行流量 或 上行流量 的 TLS 配置. 任一的 socket 实例都会生成一个包含path前缀的跟踪文件. 如：/some/capture/path_0.pb PCAP 传播 生成的跟踪文件可以被转成 libpcap format, 可以使用如 Wireshark 和 capture2pcap 这样的工具进行分析, 如: bazel run @envoy_api//tools:capture2pcap /some/capture/path_0.pb path_0.pcap tshark -r path_0.pcap -d \"tcp.port==10000,http2\" -P 1 0.000000 127.0.0.1 → 127.0.0.1 HTTP2 157 Magic, SETTINGS, WINDOW_UPDATE, HEADERS 2 0.013713 127.0.0.1 → 127.0.0.1 HTTP2 91 SETTINGS, SETTINGS, WINDOW_UPDATE 3 0.013820 127.0.0.1 → 127.0.0.1 HTTP2 63 SETTINGS 4 0.128649 127.0.0.1 → 127.0.0.1 HTTP2 5586 HEADERS 5 0.130006 127.0.0.1 → 127.0.0.1 HTTP2 7573 DATA 6 0.131044 127.0.0.1 → 127.0.0.1 HTTP2 3152 DATA, DATA "},"extending/extending.html":{"url":"extending/extending.html","title":"为自定义用例扩展 Envoy","keywords":"","body":"为自定义用例扩展 Envoy The Envoy architecture makes it fairly easily extensible via both network filters and HTTP filters. An example of how to add a network filter and structure the repository and build dependencies can be found at envoy-filter-example. "},"faq/how_fast_is_envoy.html":{"url":"faq/how_fast_is_envoy.html","title":"Envoy 有多快？","keywords":"","body":"Envoy 有多快？ We are frequently asked how fast is Envoy? or how much latency will Envoy add to my requests? The answer is: it depends. Performance depends a great deal on which Envoy features are being used and the environment in which Envoy is run. In addition, doing accurate performance testing is an incredibly difficult task that the project does not currently have resources for. Although we have done quite a bit of performance tuning of Envoy in the critical path and we believe it performs extremely well, because of the previous points we do not currently publish any official benchmarks. We encourage users to benchmark Envoy in their own environments with a configuration similar to what they plan on using in production. "},"faq/binaries.html":{"url":"faq/binaries.html","title":"从哪里能获得二进制文件？","keywords":"","body":"从哪里能获得二进制文件？ 请参考 这里。 "},"faq/sni.html":{"url":"faq/sni.html","title":"如何设置 SNI？","keywords":"","body":"如何设置 SNI？ SNI 仅被 v2 配置/API 支持。 目前的实现中要求所有 过滤器链 中的 过滤器 必须是相同的。在以后的发布中,这个约束将会放宽, 我们将可以将SNI运用到完全不同的过滤器链中。 我们还可以将 域名匹配 运用到HTTP 连接管理中去选择不同的路由路线。 这是截至目前最常见的SNI使用场景。 以下是一个如何满足上诉条件的 YAML 范例。 address: socket_address: { address: 127.0.0.1, port_value: 1234 } filter_chains: - filter_chain_match: sni_domains: \"example.com\" tls_context: common_tls_context: tls_certificates: - certificate_chain: { filename: \"example_com_cert.pem\" } private_key: { filename: \"example_com_key.pem\" } filters: - name: envoy.http_connection_manager config: route_config: virtual_hosts: - routes: - match: { prefix: \"/\" } route: { cluster: service_foo } - filter_chain_match: sni_domains: \"www.example.com\" tls_context: common_tls_context: tls_certificates: - certificate_chain: { filename: \"www_example_com_cert.pem\" } private_key: { filename: \"www_example_com_key.pem\" } filters: - name: envoy.http_connection_manager config: route_config: virtual_hosts: - routes: - match: { prefix: \"/\" } route: { cluster: service_foo } "},"faq/zone_aware_routing.html":{"url":"faq/zone_aware_routing.html","title":"如何设置 zone 可感知路由？","keywords":"","body":"如何设置 zone 可感知路由？ 在源服务（“cluster_a”）和目标服务（“cluster_b”）之间启用区域感知路由需要执行几个步骤。 源服务上的 Envoy 配置 本节介绍与源服务一起运行的 Envoy 的具体配置。要求如下： Envoy 必须使用 --service-zone 选项启动，该选项为当前主机定义区域。 源和目的地集群的定义都必须具有 sds 类型。 必须将 local_cluster_name 设置为源集群。 以下配置中仅列出了集群管理器的主要部分。 { \"sds\": \"{...}\", \"local_cluster_name\": \"cluster_a\", \"clusters\": [ { \"name\": \"cluster_a\", \"type\": \"sds\", }, { \"name\": \"cluster_b\", \"type\": \"sds\" } ] } 目的地服务上的 Envoy 配置 没有必要与目的地服务并排运行 Envoy，但重要的是目的地集群中的每台主机都注册源服务 Envoy 查询的发现服务。区域信息必须作为该响应的一部分提供。 下面的应答中只列出了与区域相关的数据。 { \"tags\": { \"az\": \"us-east-1d\" } } 基础设施搭建 上述配置对于区域感知路由是必需的，但是在不执行区域感知路由时存在某些情况。 验证步骤 使用每区域 Envoy 统计信息来监控跨区域流量。 "},"faq/zipkin_tracing.html":{"url":"faq/zipkin_tracing.html","title":"如何设置 Zipkin 追踪？","keywords":"","body":"如何设置 Zipkin 追踪？ Refer to the zipkin sandbox setup for an example of zipkin tracing configuration. "},"faq/lb_panic_threshold.html":{"url":"faq/lb_panic_threshold.html","title":"我设置了健康检查，但是当有节点失败时，Envoy 又路由到那些节点，这是怎么回事？","keywords":"","body":"我设置了健康检查，但是当有节点失败时，Envoy 又路由到那些节点，这是怎么回事？ This feature is known as the load balancer panic threshold. It is used to prevent cascading failure when upstream hosts start failing health checks in large numbers. "},"faq/concurrency_lb.html":{"url":"faq/concurrency_lb.html","title":"为什么 Round Robin 负载均衡看起来不起作用？","keywords":"","body":"为什么 Round Robin 负载均衡看起来不起作用？ Envoy 使用隔离方式的线程模型。这意味着工作线程以及相应的负载均衡器彼此不能互相协助。当使用例如 round robin 的负载均衡策略时，该策略也许不能非常好地操作多个工作线程。此时我们可以使用 --concurrency选项来调整需要运行的工作线程数。 隔离方式的运行模型也引致了如下情形，我们时常需要为每一个上游建立多个 HTTP/2 连接；且工作线程间无法共享连接池。 "}}