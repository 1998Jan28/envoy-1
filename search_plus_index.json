{"./":{"url":"./","title":"说明","keywords":"","body":"Envoy 官方文档中文版 Envoy 官方文档中文版，基于 Envoy 最新的 1.7 版本。 本项目文档地址：https://github.com/servicemesher/envoy 英文官方文档地址：https://www.envoyproxy.io/docs/envoy/latest 在线阅读地址：https://servicemesher.github.io/envoy/ 本文档使用 Gitbook 生成。 参与 参与翻译请先阅读翻译规范。 如果你发现文中的纰漏与错误请提交 Issue。 致谢 感谢所有为本文档作出贡献的 Service Mesh 爱好者。 查看贡献者名单 注意 本书中不包含官方文档中的 v1 API 参考 和 v2 API 参考 部分，书中凡是指向 API 参考的链接都直接跳转到官方页面。 ServiceMesher 微信公众号 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 15:23:07 "},"about_docs.html":{"url":"about_docs.html","title":"关于本文档","keywords":"","body":"关于本文档 Envoy 文档由以下几个主要部分组成： 简介：本部分介绍 Envoy 的概况、体系结构概述、典型部署方式等。 入门：使用 Docker 快速开始使用 Envoy。 安装：如何使用 Docker 构建/安装 Envoy。 配置：遗留 v1 API 和新 v2 API 共同的详细配置指令。相关时，配置指南还包含有关统计信息、运行时配置和 API 的信息。 操作：有关如何操作 Envoy 的常规信息，包括命令行界面、热重启包装器、管理界面、常规统计概览等。 扩展 Envoy：有关如何为 Envoy 编写自定义过滤器的信息。 v1 API 参考：特定于遗留 v1 API 的配置详细信息。 v2 API 参考：特定于新 v2 API 的配置细节。 Envoy 常见问题：有疑问？希望我们的答案能让您满意。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-16 17:43:44 "},"intro/what_is_envoy.html":{"url":"intro/what_is_envoy.html","title":"Envoy 是什么？","keywords":"","body":"Envoy 是什么？ Envoy 是专为大型现代 SOA（面向服务架构）架构设计的 L7 代理和通信总线。该项目源于以下理念： 网络应该对应用程序透明。当网络和应用程序出现问题时，应该很容易确定问题的根源。 实际上，实现上述的目标是非常困难的。 Envoy 通过提供以下高级功能来尝试做到这些： 进程外体系结构： Envoy 是一个独立进程，设计为伴随每个应用程序服务运行。所有的 Envoy 形成一个透明的通信网格，每个应用程序发送消息到本地主机或从本地主机接收消息，但不知道网络拓扑。在服务间通信的场景下，进程外架构与传统的代码库方式相比，具有两大优点： Envoy 可以使用任何应用程序语言。Envoy 部署可以在 Java、C++、Go、PHP、Python 等之间形成一个网格。面向服务的体系结构使用多个应用程序框架和语言的趋势越来越普遍。Envoy 透明地弥合了其间的差异。 任何处理过大型面向服务架构的人都知道，部署库升级可能会非常痛苦。 Envoy可以透明地在整个基础架构上快速部署和升级。 现代 C++11 代码库： Envoy 是用 C++11 编写的。选择本地代码是因为我们认为像 Envoy 这样的架构组件应该尽可能避让（资源争用）。由于在共享云环境中部署以及使用了非常有生产力但不是特别高效的语言（如 PHP，Python，Ruby，Scala 等），现代应用程序开发人员已经难以找出尾延迟的原因。本地代码通常提供了优秀的延迟属性而不会对已经混乱的情况增加额外麻烦。与用 C 编写的其他本地代码代理的解决方案不同，C++11 提供了出色的开发生产力和性能。 L3/L4 filter 架构： Envoy 的核心是一个 L3/L4 网络代理。可插入 filter 链机制允许编写 filter 来执行不同的 TCP 代理任务并将其插入到主体服务中。已经存在用来支持各种任务的 filter，如原始 TCP 代理，HTTP 代理，TLS 客户端证书认证等。 HTTP L7 filter 架构： HTTP 是现代应用程序体系结构的关键组件，Envoy 支持额外的 HTTP L7 filter 层。可以将 HTTP filter 插入执行不同任务的 HTTP 连接管理子系统，例如缓存，速率限制，路由/转发，嗅探 Amazon 的 DynamoDB 等等。 顶级 HTTP/2 支持： 当以 HTTP 模式运行时，Envoy 同时支持 HTTP/1.1 和 HTTP/2。Envoy 可以作为 HTTP/1.1 和 HTTP/2 之间的双向透明代理。这意味着它可以桥接 HTTP/1.1 和 HTTP/2 客户端以及目标服务器的任意组合。建议在服务之间配置所有 Envoy 使用 HTTP/2 来创建持久连接的网格，以便可以复用请求和响应。随着协议的逐步淘汰，Envoy 将不支持 SPDY。 HTTP L7 路由： 当以 HTTP 模式运行时，Envoy 支持一种路由子系统，能够根据路径、权限、内容类型、运行时及参数值等对请求进行路由和重定向。这项功能在将Envoy用作前端/边缘代理时非常有用，同时，在构建服务网格时也会使用此功能。 gRPC支持：gRPC 是一个来自 Google 的 RPC 框架，它使用 HTTP/2 作为底层多路复用传输协议。 Envoy 支持被 gRPC 请求和响应作为路由和负载平衡底层的所有 HTTP/2 功能。这两个系统是非常互补的。 MongoDB L7 支持：MongoDB 是一种用于现代 Web 应用程序的流行数据库。Envoy 支持对 MongoDB 连接进行 L7 嗅探、统计和日志记录。 DynamoDB L7 支持：DynamoDB 是亚马逊的托管键/值 NOSQL 数据存储。Envoy 支持对 DynamoDB 连接进行 L7 嗅探和统计。 服务发现：服务发现是面向服务体系结构的关键组件。Envoy 通过一种服务发现服务（service discovery service）的方式支持多种服务发现方法，包括异步 DNS 解析和基于 REST 的查找。 健康检查： 建立 Envoy 网格的推荐方式是将服务发现视为最终一致的过程。Envoy 包含了一个健康检查子系统，可以选择对上游服务集群执行主动健康检查。然后，Envoy 联合使用服务发现和健康检查信息来确定健康的负载均衡目标。Envoy 还通过异常检测子系统支持被动健康检查。 高级负载均衡：负载均衡是分布式系统中不同组件之间的一个复杂问题。由于 Envoy 是一个独立代理而不是库，因此它可以在一个地方实现高级负载负载均衡，并让它们可供任何应用程序访问。目前，Envoy 支持自动重试 、熔断、通过外部速率限制服务的全局速率限制、请求映射和异常值检测。未来还计划支持请求竞争。 前端/边缘代理支持： 尽管 Envoy 主要设计为一个服务到服务的通信系统，但在边缘使用相同的软件也有益处（可观察性、管理、相同的服务发现和负载均衡算法等）。Envoy 包含足够的功能，使其可作为大多数现代 Web 应用程序用例的边缘代理。这包括 TLS 终止、HTTP/1.1 和 HTTP/2 支持，以及 HTTP L7 路由。 最佳的可观察性： 如上所述，Envoy 的主要目标是使网络透明。但是，问题在网络层面和应用层面都可能会出现。 Envoy 包含对所有子系统强大的统计功能支持。目前支持 statsd（和兼容的提供程序）作为统计信息接收器，但是插入不同的接收器并不困难。统计信息也可以通过管理 端口查看。Envoy 还通过第三方提供商支持分布式追踪。 动态配置： Envoy 可以选择使用动态配置API 的分层集合。如果需要，实现者可以使用这些 API 来构建复杂的集中管理部署。 设计目标 关于代码本身的设计目标的简短说明：尽管 Envoy 绝对不慢（我们花费了相当多的时间来优化某些快速路径），相对以最大可能的绝对性能为目标，代码还是按照模块化和易于测试来编写。我们认为这会更有效的利用时间，因为典型部署将和速度慢数倍、内存占用高数倍的语言和运行时在一起。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 21:59:56 "},"intro/arch_overview/terminology.html":{"url":"intro/arch_overview/terminology.html","title":"术语","keywords":"","body":"术语 在深入主架构文档之前的一些定义。部分定义在行业中略有争议，下面将展开在 Envoy 文档和代码库中如何使用它们。 Host/主机：能够进行网络通信的实体（如移动设备、服务器上的应用程序）。在此文档中，主机是逻辑网络应用程序。一块物理硬件上可能运行有多个主机，只要它们是可以独立寻址的。 Downstream/下游：下游主机连接到 Envoy，发送请求并接收响应。 Upstream/上游：上游主机接收来自 Envoy 的连接和请求，并返回响应。 Listener/监听器：监听器是命名网地址（例如，端口、unix domain socket等)，可以被下游客户端连接。Envoy 暴露一个或者多个监听器给下游主机连接。 Cluster/集群：集群是指 Envoy 连接到的逻辑上相同的一组上游主机。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到哪个集群成员。 Mesh/网格：一组主机，协调好以提供一致的网络拓扑。在本文档中，“Envoy mesh”是一组 Envoy 代理，它们构成了分布式系统的消息传递基础，这个分布式系统由很多不同服务和应用程序平台组成。 Runtime configuration/运行时配置：外置实时配置系统，和 Envoy 一起部署。可以更改配置设置，影响操作，而无需重启 Envoy 或更改主要配置。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 15:18:23 "},"intro/arch_overview/threading_model.html":{"url":"intro/arch_overview/threading_model.html","title":"线程模型","keywords":"","body":"线程模型 Envoy 使用单进程 - 多线程的架构模型。一个 master 线程管理各种琐碎的任务，而一些 worker 线程则负责执行监听、过滤和转发。当监听器接收到一个连接请求时，该连接将其生命周期绑定到一个单独的 worker 线程。这使得 Envoy 主要使用大量单线程（ embarrassingly parallel ）处理工作，并且只有少量的复杂代码用于实现 worker 线程之间的协调工作。通常情况下，Envoy 实现了100%的非阻塞。对于大多数工作负载，我们建议将 worker 线程数配置为物理机器的线程数。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 17:26:22 "},"intro/arch_overview/listeners.html":{"url":"intro/arch_overview/listeners.html","title":"监听器","keywords":"","body":"监听器 Envoy配置支持在单个进程中启用任意数量的监听器。通常建议每台机器运行单个Envoy，而不必介意配置的监听器数量。这样运维更简单，而且只有单个统计来源。目前Envoy只支持TCP监听器。 每个监听器都独立配置有一些（L3 / L4）网络级别的过滤器。当监听器接接收到新连接时，配置好的连接本地过滤器将被实例化，并开始处理后续事件。通用监听器架构用于执行绝大多数不同的代理任务（例如，限速，TLS客户端认证, HTTP 连接管理, MongoDB sniffing, 原始 TCP 代理等）。 监听器也可以选择性的配置某些监听器过滤器。 这些过滤器的处理在网络级别过滤器之前进行，并有机会操纵连接元数据，通常会影响后续过滤器或集群处理连接的方式。 监听器也可以通过监听器发现服务 (LDS)动态获取。 监听器 配置. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-16 16:13:32 "},"intro/arch_overview/listener_filters.html":{"url":"intro/arch_overview/listener_filters.html","title":"监听器过滤器","keywords":"","body":"监听器过滤器 在监听器一节中讨论到，监听器过滤器可以用于操纵连接元数据。监听器过滤器的主要目的是来更方便地添加系统集成功能，而无需更改 Envoy 核心功能，并且还可以让多个此类功能之间的交互更加明确。 监听器过滤器的API相对简单，因为最终这些过滤器都在新接收的套接字上操作。可停止链中的过滤器并继续执行后续的过滤器。这允许去运作更复杂的业务场景，例如调用限速服务等。Envoy 包含多个监听器过滤器，这些过滤器在此架构概述以及配置参考中都有记录。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 10:25:39 "},"intro/arch_overview/network_filters.html":{"url":"intro/arch_overview/network_filters.html","title":"网络 (L3/L4) 过滤器","keywords":"","body":"网络 (L3/L4) 过滤器 如监听器一节所述，网络级（L3/L4）过滤器构成Envoy连接处理的核心。过滤器API允许混合不同的过滤器组合，并匹配和附加到给定的监听器。有三种不同类型的网络过滤器： 读: 当Envoy从下游连接接收数据时，调用读过滤器。 写: 当Envoy要发送数据到下游连接时，调用写过滤器。 读/写: 当Envoy从下游连接接收数据和要发送数据到下游连接时，调用读/写过滤器。 网络级过滤器的API相对简单，因为最终过滤器只操作原始字节和少量连接事件（例如，TLS握手完成，连接在本地或远程断开等）。链中的过滤器可以停止后续的过滤器，并随后继续。这可以实现更复杂的场景，例如调用限速服务等。Envoy包含多个网络级的过滤器，这些过滤器在此架构概述和配置参考中都有说明。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-16 17:56:29 "},"intro/arch_overview/http_connection_management.html":{"url":"intro/arch_overview/http_connection_management.html","title":"HTTP 连接管理","keywords":"","body":"HTTP 连接管理 HTTP 是现代面向服务价格如此关键的一个元素，以至于 Envoy 实现了大量特定于 HTTP 的功能。Envoy 有一个内建的网络层过滤器称为 HTTP 连接管理器。该管理器将原始字节翻译为 HTTP 层消息和事件。(即，收到的头，收到的体数据，收到的尾，等等)。它还处理对所有 HTTP 连接共同的功能和请求，如访问日志、请求 ID 生成和追踪、请求/相应头操控、路由表管理以及统计。 HTTP 连接管理器配置。 HTTP 协议 Envoy 的 HTTP 连接管理器内建支持 HTTP/1.1、WebSockets 和 HTTP/2。它不支持 SPDY。Envoy 的 HTTP 支持被设计为首先是一个 HTTP/2 多路复用代理。在内部，HTTP/2 这一名词用于描述系统元素。例如，一个 HTTP 请求和响应发生在一个流上。一个编解码器 API 被用于为流、请求、响应等等将不同的连线协议翻译为一个与协议不可知的形式。对于 HTTP/1.1，编解码器将协议的串行/流水线能力转换为对更高层看起来像 HTTP/2 的某种东西。这意味着大部分代码不需要理解一个流是否来源于一个 HTTP/1.1 或 HTTP/2 连接。 HTTP 头净化 出于安全原因，HTTP 连接管理器执行不同的头净化操作。 路由表配置 每个 HTTP 连接管理器过滤器有一个相关的路由表。路由表可以两种方式中的一种指定： 静态地。 通过 RDS API 动态地。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 21:41:51 "},"intro/arch_overview/http_filters.html":{"url":"intro/arch_overview/http_filters.html","title":"HTTP 过滤器","keywords":"","body":"HTTP 级过滤器 比较类似网络级过滤器栈，Envoy 在连接管理器内支持 HTTP 级过滤器栈。 我们可以在不了解底层物理协议（如 HTTP/1.1、HTTP/2等）或多路复用技术相关知识的情况下，写过滤器去操作 HTTP 级的信息。 有三种类型的 HTTP 级过滤器: 解码器：解码器过滤器在连接管理器在解码部分请求流（头部、正文和尾部）时被调用。 编码器：编码器过滤器在连接管理器即将对响应流(头部、正文和尾部）的部分进行编码时被调用。 解码器/编码器：解码器/编码器过滤器在连接管理器在解码请求流时以及当连接管理器将要编码响应流时被调用。 HTTP 级过滤器的 API 允许过滤器在不知道底层协议的情况下运行。 就如网络级过滤器一样，可停止 HTTP 过滤器并继续执行后续的过滤器。 由此可以实现更复杂的业务场景，例如运行状况检查处理、调用限速服务、缓冲、路由、为应用程序流量（例如 DynamoDB 等）生成统计数据。Envoy 已包含多个 HTTP 级过滤器，相关文档可查阅这份体系结构概述以及配置参考。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 21:48:39 "},"intro/arch_overview/http_routing.html":{"url":"intro/arch_overview/http_routing.html","title":"HTTP 路由","keywords":"","body":"HTTP路由 Envoy 包括一个 HTTP 路由器过滤器，它可以被安装来执行高级路由任务。这对于处理边缘流量（传统的反向代理请求处理）以及为服务 Envoy 网格构建服务（通常是通过主机/授权 HTTP 头的路由到达特定的上游服务集群）非常有用。Envoy 也可以被配置为转发代理。在正向代理配置中，网格客户端可以通过适当地配置他们的 http 代理来作为 Envoy。在较高的级路由接受一个传入的 HTTP 请求，将其与上游集群相匹配，在上游集群中获得一个连接池，并转发请求。路由过滤器支持以下特性： 将域/授权映射到一组路由规则的虚拟主机。 前缀和精确路径匹配规则（都有敏感和不敏感）。目前还不支持正则 /slug 匹配，这主要是因为以编程方式确定路由规则是否相互冲突变得非常困难,然而，我们可能会根据需求增加对未来的支持。 在虚拟主机级别上的 TLS 重定向。 在路由级别的路径/主机重定向。 在路由级别的直接（非代理）HTTP 响应。 显式重写。 自动主机重写，基于所选的上游主机的 DNS 名称。 前缀重写。 在路由级别上的 Websocket升级。 请求通过 HTTP 头或路径配置指定的重试。 请求通过 HTTP头 或路径配置指定超时。 通过运行时从一个上游集群转移到另一个集群（参见流量转移/分割）。 使用基于重量/百分比的路由的流量跨越多个上游集群（参见流量转移/分割）。 任意标题匹配路由规则。 虚拟集群规范。虚拟集群是在虚拟主机级别上指定的，由 Envoy 使用，在标准集群级别上生成额外的统计信息。虚拟集群可以使用正则表达式匹配。 基于优先级的路由。 基于哈希策略的路由。 绝对 url 支持非 tls 转发代理。 路由表 HTTP 连接管理器的配置拥有所有配置的 HTTP 过滤器所使用的路由表。尽管路由器过滤器是路由表的主要使用者，但是其他过滤器也有访问权限，以防它们想根据请求的最终目的地做出决定。例如，内置的速率限制过滤器查询路由表，以确定是否应该根据路由调用全局速率限制服务。连接管理器确保对特定请求的所有调用都是稳定的，即使决策涉及到随机性（例如，在运行时配置路由规则的情况下）。 重试语义 Envoy 允许在路由配置和通过请求头的特定请求中配置重试。以下配置是可能的： 最大数量的重试:Envoy 将继续进行多次尝试。在每次重试之间使用一个指数回溯算法。此外，所有重试都包含在总体请求超时中。这避免了由于大量重试而导致的长时间请求。 重试条件：Envoy 可以根据应用程序的要求对不同类型的条件进行重试。例如，网络故障，所有5xx响应代码，幂等4xx响应码，等等。 请注意，根据 x-envoy-overloaded 的内容，重试可能会被禁用。 优先级路由 Envoy 支持路由级别的优先路由。当前的优先级实现为每个优先级使用不同的连接池和断路设置。这意味着，即使对于 http/2 请求，也将使用两个物理连接到上游主机。在未来，特使可能会支持真正的 http/2 优先级而不是单一连接。 当前支持的优先级是默认的和高的。 直接响应 Envoy 支持发送“直接”回应。这些是预先配置的 HTTP 响应，不需要代理到上游服务器。 有两种方法可以在一条路由中指定直接响应： 设置 direct_response 字段。这适用于所有 HTTP 响应状态。 设置重定向。这只适用于重定向响应状态，但它简化了位置头的设置。 直接响应有一个 HTTP 状态码和一个可选体。路由配置可以内联地指定响应体，或者指定包含体的文件的路径名。如果路线配置指定了一个文件路径名，Envoy 将在配置负载上读取文件并缓存内容。 注意 如果指定了响应体，那么它的大小必须不超过4 KB ，不管它是内联还是在文件中。Envoy 目前将整体保存在内存中，因此4 KB的限制是为了防止代理的内存占用太大。 如果应答器添加被设置为路由或封闭虚拟主机，则Envoy 将在直接 HTTP 响应中包含指定的头部。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 14:01:45 "},"intro/arch_overview/grpc.html":{"url":"intro/arch_overview/grpc.html","title":"gRPC","keywords":"","body":"gRPC gRpc 是来自 Google 的 RPC 框架。它使用协议缓冲区作为底层 序列化 /IDL(接口描述语言的缩写) 格式。在传输层，它使用 HTTP/2 进行请求/响应复用。Envoy 在传输层和应用层都提供对 gRPC 的一流支持： gRPC 使用 HTTP/2 trailers 特性（可以在 HTTP 请求和响应报文后追加 HTTP Header)来传送请求状态。Envoy 是能够正确支持 HTTP/2 trailers 的少数几个 HTTP 代理之一，因此也是可以传输 gRPC 请求和响应的代理之一。 某些语言的 gRPC 运行时相对不成熟。Envoy 支持 gRPC 桥接过滤器，允许 gRPC 请求通过 HTTP/1.1 发送给 Envoy。然后，Envoy 将请求转换为 HTTP/2 以传输到目标服务器。该响应被转换回 HTTP/1.1。 安装后，除了标准的全局 HTTP 统计数据之外，桥接过滤器还会根据每个 RPC 统计数据进行收集。 gRPC-Web 由一个指定的过滤器支持，该过滤器允许 gRPC-Web 客户端通过 HTTP/1.1 向 Envoy 发送请求并代理到 gRPC 服务器。目前相关团队正在积极开发中，预计它将成为 gRPC 桥接过滤器的后续产品。 gRPC-JSON 转码器由一个指定的过滤器支持，该过滤器允许 RESTful JSON API 客户端通过 HTTP 向 Envoy 发送请求并获取代理到 gRPC 服务。 gRPC 服务 除了在数据层面上代理 gRPC 外，Envoy 在控制层面也使用了 gRPC，它从中获取管理服务器的配置以及过滤器中的配置，例如用于速率限制)或授权检查。我们称之为 gRPC 服务。 当指定 gRPC 服务时，必须指定使用 Envoy gRPC 客户端或 Google C ++ gRPC 客户端。我们在下面的这个选择中讨论权衡。 Envoy gRPC 客户端是使用 Envoy 的 HTTP/2 上行连接管理的 gRPC 的最小自定义实现。服务被指定为常规 Envoy 集群，定期处理超时、重试、终端发现、负载平衡、故障转移、负载报告、断路、健康检查、异常检测。它们与 Envoy 的数据层面共享相同的连接池机制。同样，集群统计信息可用于 gRPC 服务。由于客户端是简化版的 gRPC 实现，因此不包括诸如 OAuth2 或 gRPC-LB 之类的高级 gRPC 功能后备。 Google C++ gRPC 客户端基于 Google 在 https://github.com/grpc/grpc 上提供的 gRPC 参考实现。它提供了 Envoy gRPC 客户端中缺少的高级 gRPC 功能。Google C++ gRPC 客户端独立于 Envoy 的集群管理，执行自己的负载平衡、重试、超时、端点管理等。Google C++ gRPC 客户端还支持自定义身份验证插件。 在大多数情况下，当你不需要 Google C++ gRPC 客户端的高级功能时，建议使用 Envoy gRPC 客户端。这使得配置和监控更加简单。如果 Envoy gRPC 客户端中缺少你所需要的功能，则应该使用 Google C++ gRPC 客户端。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:55:48 "},"intro/arch_overview/websocket.html":{"url":"intro/arch_overview/websocket.html","title":"WebSocket 支持","keywords":"","body":"WebSocket 支持 Envoy 支持将 HTTP/1.1 连接升级到 WebSocket 连接。仅当下游客户端发送正确的升级请求头，并且被匹配的 HTTP 路由必须明确配置使用了 WebSocket （use_websocket）时才允许连接升级。如果请求到达 WebSocket 的路由没有必要的升级头，它将被视为常规的 HTTP/ 1.1请求。 由于 Envoy 将 WebSocket 连接视为纯 TCP 连接，因此它支持 WebSocket 协议的所有内容，而与它们的报文格式无关。WebSocket 路由不支持某些 HTTP 请求级别的功能，例如重定向，超时，重试，速率限制和阴影。但是，支持前缀重写，显式和自动主机重写，流量转移和拆分。 连接语义 尽管 WebSocket 升级可以通过 HTTP/1.1 连接进行，但 WebSockets 代理的工作模式与普通 TCP 代理类似，即 Envoy 不会解析 websocket 报文帧。下游客户端和/或上游服务器负责终止 WebSocket 连接（例如，通过发送关闭帧）和底层 TCP 连接。 当连接管理器通过支持 WebSocket 的路由接收到 WebSocket 升级请求时，它通过 TCP 连接将请求转发给上游服务器。Envoy 不知道上游服务器是否拒绝了升级请求。上游服务器负责终止 TCP 连接，这将导致 Envoy 终止相应的下游客户端连接。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 14:28:29 "},"intro/arch_overview/cluster_manager.html":{"url":"intro/arch_overview/cluster_manager.html","title":"集群管理器","keywords":"","body":"集群管理器 Envoy 集群管理器管理所有配置的上游集群。正如 Envoy 配置可以包含任意数量的监听器一样，配置也可以包含任意数量的独立配置的上游集群。 上游集群和主机是从网络/HTTP 过滤器堆栈中抽象而来，因为上游集群和主机可用于任意数量的不同代理任务。集群管理器向过滤器堆栈暴露API，允许过滤器获得连接到上游集群的 L3/L4 连接，或者连接到上游集群的抽象 HTTP 连接池的句柄（上游主机是否支持 HTTP/1.1 或 HTTP/2 是隐藏的）。过滤器阶段判断是否需要 L3/L4 连接或新的 HTTP 流，而集群管理器处理所有的复杂性，包括获知哪些主机可用并且健康，负载均衡，上游连接数据的线程本地存储（因为大多数 Envoy 代码以单线程编写），上游连接类型（TCP/IP、UDS），适用的上游协议（HTTP/1.1、HTTP/2）等。 集群管理器获知集群的方式可以是静态配置，或者可以通过集群发现服务（CDS）API 动态获取。动态集群获取允许将更多配置存储在中央配置服务器中，因此可以减少 Envoy 重启和重新分配配置的次数。 集群管理器 配置。 CDS 配置。 Cluster warming 当集群在服务器启动或者通过 CDS 进行初始化时，它们会“热身”。这意味着集群在下列操作发生之前不可用。 初始服务发现加载 (例如，DNS 解析、EDS 更新等等)。 初始主动 健康检查 通过，如果配置了主动健康检查。Envoy 将发送健康检查请求到每个被发现的主机来判断它的初始健康状态。 上述项确保 Envoy 在开始将集群用于流量服务之前具有准确的集群视图。 在讨论集群热身时，集群 “变为可用” 意味着: 对于新加入的集群，在集群热身前，集群对于 Envoy 的其余部分是不存在的。即引用集群的 HTTP 路由将导致 404 或 503（取决于配置）。 对于更新后的集群，旧集群将继续存在并服务流量。当新集群被加热后，它将与旧集群进行原子交换，从而不会发生流量中断。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 14:59:00 "},"intro/arch_overview/service_discovery.html":{"url":"intro/arch_overview/service_discovery.html","title":"服务发现","keywords":"","body":"Service discovery When an upstream cluster is defined in the configuration, Envoy needs to know how to resolve the members of the cluster. This is known as service discovery. Supported service discovery types Static Static is the simplest service discovery type. The configuration explicitly specifies the resolved network name (IP address/port, unix domain socket, etc.) of each upstream host. Strict DNS When using strict DNS service discovery, Envoy will continuously and asynchronously resolve the specified DNS targets. Each returned IP address in the DNS result will be considered an explicit host in the upstream cluster. This means that if the query returns three IP addresses, Envoy will assume the cluster has three hosts, and all three should be load balanced to. If a host is removed from the result Envoy assumes it no longer exists and will drain traffic from any existing connection pools. Note that Envoy never synchronously resolves DNS in the forwarding path. At the expense of eventual consistency, there is never a worry of blocking on a long running DNS query. Logical DNS Logical DNS uses a similar asynchronous resolution mechanism to strict DNS. However, instead of strictly taking the results of the DNS query and assuming that they comprise the entire upstream cluster, a logical DNS cluster only uses the first IP address returned when a new connection needs to be initiated. Thus, a single logical connection pool may contain physical connections to a variety of different upstream hosts. Connections are never drained. This service discovery type is optimal for large scale web services that must be accessed via DNS. Such services typically use round robin DNS to return many different IP addresses. Typically a different result is returned for each query. If strict DNS were used in this scenario, Envoy would assume that the cluster’s members were changing during every resolution interval which would lead to draining connection pools, connection cycling, etc. Instead, with logical DNS, connections stay alive until they get cycled. When interacting with large scale web services, this is the best of all possible worlds: asynchronous/eventually consistent DNS resolution, long lived connections, and zero blocking in the forwarding path. Original destination Original destination cluster can be used when incoming connections are redirected to Envoy either via an iptables REDIRECT or TPROXY target or with Proxy Protocol. In these cases requests routed to an original destination cluster are forwarded to upstream hosts as addressed by the redirection metadata, without any explicit host configuration or upstream host discovery. Connections to upstream hosts are pooled and unused hosts are flushed out when they have been idle longer thancleanup_interval_ms, which defaults to 5000ms. If the original destination address is is not available, no upstream connection is opened. Original destination service discovery must be used with the original destination load balancer. Service discovery service (SDS) The service discovery service is a generic REST based API used by Envoy to fetch cluster members. Lyft provides a reference implementation via the Python discovery service. That implementation uses AWS DynamoDB as the backing store, however the API is simple enough that it could easily be implemented on top of a variety of different backing stores. For each SDS cluster, Envoy will periodically fetch the cluster members from the discovery service. SDS is the preferred service discovery mechanism for a few reasons: Envoy has explicit knowledge of each upstream host (vs. routing through a DNS resolved load balancer) and can make more intelligent load balancing decisions. Extra attributes carried in the discovery API response for each host inform Envoy of the host’s load balancing weight, canary status, zone, etc. These additional attributes are used globally by the Envoy mesh during load balancing, statistic gathering, etc. Generally active health checking is used in conjunction with the eventually consistent service discovery service data to making load balancing and routing decisions. This is discussed further in the following section. On eventually consistent service discovery Many existing RPC systems treat service discovery as a fully consistent process. To this end, they use fully consistent leader election backing stores such as Zookeeper, etcd, Consul, etc. Our experience has been that operating these backing stores at scale is painful. Envoy was designed from the beginning with the idea that service discovery does not require full consistency. Instead, Envoy assumes that hosts come and go from the mesh in an eventually consistent way. Our recommended way of deploying a service to service Envoy mesh configuration uses eventually consistent service discovery along with active health checking (Envoy explicitly health checking upstream cluster members) to determine cluster health. This paradigm has a number of benefits: All health decisions are fully distributed. Thus, network partitions are gracefully handled (whether the application gracefully handles the partition is a different story). When health checking is configured for an upstream cluster, Envoy uses a 2x2 matrix to determine whether to route to a host: Discovery Status HC OK HC Failed Discovered Route Don’t Route Absent Route Don’t Route / Delete Host discovered / health check OK Envoy will route to the target host. Host absent / health check OK: Envoy will route to the target host. This is very important since the design assumes that the discovery service can fail at any time. If a host continues to pass health check even after becoming absent from the discovery data, Envoy will still route. Although it would be impossible to add new hosts in this scenario, existing hosts will continue to operate normally. When the discovery service is operating normally again the data will eventually re-converge. Host discovered / health check FAIL Envoy will not route to the target host. Health check data is assumed to be more accurate than discovery data. Host absent / health check FAIL Envoy will not route and will delete the target host. This is the only state in which Envoy will purge host data. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 15:59:54 "},"intro/arch_overview/health_checking.html":{"url":"intro/arch_overview/health_checking.html","title":"健康检查","keywords":"","body":"健康检查 Active health checking can be configured on a per upstream cluster basis. As described in the service discovery section, active health checking and the SDS service discovery type go hand in hand. However, there are other scenarios where active health checking is desired even when using the other service discovery types. Envoy supports three different types of health checking along with various settings (check interval, failures required before marking a host unhealthy, successes required before marking a host healthy, etc.): HTTP: During HTTP health checking Envoy will send an HTTP request to the upstream host. It expects a 200 response if the host is healthy. The upstream host can return 503 if it wants to immediately notify downstream hosts to no longer forward traffic to it. L3/L4: During L3/L4 health checking, Envoy will send a configurable byte buffer to the upstream host. It expects the byte buffer to be echoed in the response if the host is to be considered healthy. Envoy also supports connect only L3/L4 health checking. Redis: Envoy will send a Redis PING command and expect a PONG response. The upstream Redis server can respond with anything other than PONG to cause an immediate active health check failure. Optionally, Envoy can perform EXISTS on a user-specified key. If the key does not exist it is considered a passing healthcheck. This allows the user to mark a Redis instance for maintenance by setting the specified key to any value and waiting for traffic to drain. Seeredis_key. 被动健康检查 Envoy also supports passive health checking via outlier detection. 链接池交互 See here for more information. HTTP 健康检查过滤器 When an Envoy mesh is deployed with active health checking between clusters, a large amount of health checking traffic can be generated. Envoy includes an HTTP health checking filter that can be installed in a configured HTTP listener. This filter is capable of a few different modes of operation: No pass through: In this mode, the health check request is never passed to the local service. Envoy will respond with a 200 or a 503 depending on the current draining state of the server. No pass through, computed from upstream cluster health: In this mode, the health checking filter will return a 200 or a 503 depending on whether at least a specified percentage of the servers are healthy in one or more upstream clusters. (If the Envoy server is in a draining state, though, it will respond with a 503 regardless of the upstream cluster health.) Pass through: In this mode, Envoy will pass every health check request to the local service. The service is expected to return a 200 or a 503 depending on its health state. Pass through with caching: In this mode, Envoy will pass health check requests to the local service, but then cache the result for some period of time. Subsequent health check requests will return the cached value up to the cache time. When the cache time is reached, the next health check request will be passed to the local service. This is the recommended mode of operation when operating a large mesh. Envoy uses persistent connections for health checking traffic and health check requests have very little cost to Envoy itself. Thus, this mode of operation yields an eventually consistent view of the health state of each upstream host without overwhelming the local service with a large number of health check requests. Further reading: Health check filter configuration. /healthcheck/fail admin endpoint. /healthcheck/ok admin endpoint. 主动健康检查快速失败 When using active health checking along with passive health checking (outlier detection), it is common to use a long health checking interval to avoid a large amount of active health checking traffic. In this case, it is still useful to be able to quickly drain an upstream host when using the /healthcheck/fail admin endpoint. To support this, the router filter will respond to the x-envoy-immediate-health-check-fail header. If this header is set by an upstream host, Envoy will immediately mark the host as being failed for active health check. Note that this only occurs if the host’s cluster has active health checking configured. The health checking filter will automatically set this header if Envoy has been marked as failed via the /healthcheck/fail admin endpoint. 健康检查身份 Just verifying that an upstream host responds to a particular health check URL does not necessarily mean that the upstream host is valid. For example, when using eventually consistent service discovery in a cloud auto scaling or container environment, it’s possible for a host to go away and then come back with the same IP address, but as a different host type. One solution to this problem is having a different HTTP health checking URL for every service type. The downside of that approach is that overall configuration becomes more complicated as every health check URL is fully custom. The Envoy HTTP health checker supports the service_name option. If this option is set, the health checker additionally compares the value of the x-envoy-upstream-healthchecked-cluster response header to service_name. If the values do not match, the health check does not pass. The upstream health check filter appends x-envoy-upstream-healthchecked-cluster to the response headers. The appended value is determined by the --service-cluster command line option. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 16:02:42 "},"intro/arch_overview/connection_pooling.html":{"url":"intro/arch_overview/connection_pooling.html","title":"连接池","keywords":"","body":"连接池 对于 HTTP 流量，Envoy 支持在底层协议（HTTP/1.1 或 HTTP/2）之上的抽象连接池。过滤器代码不需要知道底层协议是否支持真正的复用。 在实践中，底层实现具有以下高级属性： HTTP/1.1 HTTP/1.1 连接池根据需要获取上游主机的连接（取决于断路限制）。当连接变得可用时，请求被绑定到连接，这可能是因为连接完成先前请求的处理，或者因为新的连接已经准备好可以接收第一次请求。HTTP/1.1 连接池不使用流水线，因此如果上游连接被切断，只有一个下游请求必须重置。 HTTP/2 HTTP/2连接池获取到上游主机的单个连接。所有请求都通过此连接复用。如果收到 GOAWAY 帧，或者连接达到最大流限制，连接池将创建新的连接并且耗尽现有连接。 HTTP/2 是首选的通信协议，因为连接很少会被切断。 健康检查交互 如果 Envoy 配置有主动或被动健康检查，则代表从健康状态转换为不健康状态的主机将关闭所有连接池连接。如果主机重新进入负载均衡轮换，它将创建新的连接，这将最大化解决流量不佳（由于 ECMP 路由或其他原因）的机会。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 15:13:32 "},"intro/arch_overview/load_balancing.html":{"url":"intro/arch_overview/load_balancing.html","title":"负载均衡","keywords":"","body":"负载均衡 当过滤器需要获取到上游集群中主机的连接时，cluster manager 将使用负载均衡策略来确定选择哪个主机。负载均衡策略是可拔插的，并且在配置中以每个上游集群为基础进行指定。请注意，如果没有为集群配置活动的运行状况检查策略，则所有上游集群成员都认为是健康的。 支持的负载均衡器 Round robin 这是一个简单的策略，每个健康的上游主机按循环顺序选择。如果将权重分配给本地的端点，则使用加权 round robin（循环）调度，其中较高权重的端点将更频繁地出现在循环中以实现有效权重。 加权最少请求 请求最少的负载均衡器使用 O(1) 算法选择两个随机健康主机，并选择主动请求较少的主机（研究表明这种方法几乎与 O(N) 全扫描一样好）。如果集群中的任何主机的负载均衡权重大于1，则负载均衡器将转换为随机选择主机并使用该主机时间的模式。该算法对于负载测试来说简单且足够。它不应该用于需要真正的加权最小请求的地方（通常请求持续时间可变且较长）。我们可能会在将来添加一个真正的全扫描加权最小请求变体来涵盖此用例。 Ring hash Ring/modulo 哈希负载均衡器实现对上游主机的一致性哈希。该算法基于将所有主机映射到一个环上，使得从主机集添加或移除主机的更改仅影响 1/N 个请求。这种技术通常也被称为“ketama”哈希。一致的哈希负载均衡器只有在使用指定哈希值的协议路由时才有效。最小环大小控制环中每个主机的复制因子。例如，如果最小环大小为 1024 并且有 16 个主机，则每个主机将被复制 64 次。环哈希负载均衡器当前不支持加权。 当使用基于优先级的负载均衡时，优先级也通过哈希选择，因此当后端集合稳定时，选定的端点仍将保持一致。 注意 环哈希负载均衡器不支持所在地加权负载均衡。 Maglev Maglev（磁悬浮）负载均衡器对上游主机实施一致性的哈希。它使用本文第 3.4 节中描述的算法，固定表大小为65537（参见同一论文的第5.3节）。Maglev 可以用作环哈希负载均衡器的替代品，可以在任何需要一致性哈希的地方使用。就像环哈希负载均衡器一样，只有在使用指定哈希值的协议路由时，一致性哈希负载均衡器才有效。 一般来说，与环形散列（“ketama”）算法相比，Maglev 具有快得多的查表编译时间以及主机选择时间（当使用 256K 条目的大环时大约分别为 10 倍和 5 倍）。Maglev 的缺点是它不像环哈希那样稳定。当主机被移除时，更多的键将移动位置（模拟显示键将移动大约两倍）。据说，对于包括 Redis 在内的许多应用程序来说，Maglev 很可能是环形哈希替代品的一大优势。高级读者可以使用这个 benchmark 来比较具有不同参数的环形哈希与 Maglev。 随机 随机负载均衡器选择一个随机的健康主机。如果没有配置健康检查策略，则随机负载均衡器通常比 round robin 更好。随机选择可以避免主机出现故障后对集合中的主机造成偏见。 原始目的地 这是一种专用的负载均衡器，只能与原始目的集群一起使用。上游主机是根据下游连接元数据选择的，即连接被打开到与连接重定向到 envoy 之前传入与连接的目标地址相同的地址。新的目标由负载均衡器按需添加到集群，并且集群会定期清除集群中未使用的主机。原始目标集群不能使用其他负载均衡类型。 恐慌阈值 在负载均衡期间，Envoy 通常只会考虑上游集群中健康的主机。但是，如果集群中健康主机的百分比变得过低，envoy 将忽视所有主机中的健康状况和均衡。这被称为恐慌阈值。缺省恐慌阈值是 50％。这可以通过运行时以及集群配置进行配置。恐慌阈值用于避免在负载增加时主机故障导致整个集群中级联故障的情况。 请注意，恐慌阈值是有优先级的。这意味着如果单个优先级中健康节点的百分比低于阈值，该优先级将进入恐慌模式。一般而言，不鼓励将恐慌阈值与优先级结合使用，因为当有足够多的节点的状态不健康触发恐慌阈值时，大部分流量应该已经溢出到下一个优先级。 优先级划分 在负载均衡期间，Envoy 通常只考虑配置为最高优先级的主机。对于每个 EDS LocalityLbEndpoints，还可以指定一个可选的优先级。当最高优先级（P=0）的端点健康时，所有流量将以该优先级落在端点上。由于最高优先级的端点变得不健康，因此流量将开始慢慢降低优先级。 目前，假定每个优先级级别都由因子 1.4（硬编码）过度配置的。因此，如果 80％ 的端点是健康的，那么优先级仍然被认为是健康的，因为 80*1.4>100。当健康端点的数量下降到 72％ 以下时，优先级的健康状况低于100。此时，相当于到 P=0 健康状态的流量的百分比将转到 P=0，剩余流量将流向 P=1。 假设一个简单的设置有 2 个优先级，P=1 100％ 健康。 P=0 健康端点 到 P=0 的流量百分比 到 P=1 的流量百分比 100% 100% 0% 72% 100% 0% 71% 99% 1% 50% 70% 30% 25% 35% 65% 0% 0% 100% 如果 P=1 变得不健康，它将继续从 P=0 溢出负载，直到健康 P=0 + P=1 的总和低于 100 为止。此时，健康状态将被放大到 100％ 的“有效”健康状态。 P=0 健康端点 P=1 健康端点 Traffic to P=0 到 P=1 的流量 100% 100% 100% 0% 72% 72% 100% 0% 71% 71% 99% 1% 50% 50% 70% 30% 25% 100% 35% 65% 25% 25% 50% 50% 随着更多的优先级被添加，每个级别消耗等于其“缩放”有效健康的负载，所以如果 P=0 + P=1 的组合健康度小于 100，则 P=2 将仅接收流量。 P=0 健康端点 P=1 健康端点 P=2 健康端点 到 P=0 的流量 到 P=1 的流量 到 P=2 的流量 100% 100% 100% 100% 0% 0% 72% 72% 100% 100% 0% 0% 71% 71% 100% 99% 1% 0% 50% 50% 100% 70% 30% 0% 25% 100% 100% 35% 65% 0% 25% 25% 100% 25% 25% 50% 在伪代码中加和： load to P_0 = min(100, health(P_0) * 100 / total_health) health(P_X) = 140 * healthy_P_X_backends / total_P_X_backends total_health = min(100, Σ(health(P_0)...health(P_X)) load to P_X = 100 - Σ(percent_load(P_0)..percent_load(P_X-1)) Zone 感知路由 我们使用以下术语： 始发/上游集群：Envoy 将来自始发集群的请求路由到上游集群中。 本地 zone：包含始发和上游集群中主机子集的同一区域。 Zone 感知路由：尽量将请求路由到本地 zone 的上游集群主机上。 当始发和上游集群中的主机部署不同 zone 时，Envoy 执行 zone 感知路由。在执行 zone 感知路由之前有几个先决条件： 始发和上游集群都不处于恐慌模式。 Zone 感知路由已启用。 始发集群与上游集群具有相同数量的 zone。 上游集群有足够多的主机。浏览此处获取更多信息。 Zone 感知路由的目的是尽可能多地向上游集群的本地 zone 中发送流量，同时大致保持上游集群中的所有主机拥有相同的（取决于负载均衡策略）的每秒请求数量。 只要上游集群中每台主机的请求数量保持大致相同，Envoy 就会尝试尽可能多地将流量推送到本地上游区域。Envoy 路由到本地 zone 还是执行跨 zone 路由，这取决于本地 zone 中始发集群和上游集群中健康主机的百分比。关于始发和上游集群之间的本地 zone 百分比关系有以下两种情况： 始发集群的本地 zone 百分比大于上游集群中本地 zone 的百分比。在这种情况下，我们无法将来自始发集群本地 zone 的所有请求路由到上游集群的本地 zone，因为这会导致所有上游主机请求不均衡。相反，Envoy 计算可以直接路由到上游集群的本地 zone 的请求的百分比。其余的请求被路由到跨 zone。特定 zone 根据 zone 的剩余容量（该 zone 将获得一些本地 zone 流量并且可能具有 Envoy 可用于跨 zone 业务量的额外容量）来选择。 始发集群本地 zone 百分比小于上游集群中的本地 zone 百分比。在这种情况下，上游集群的本地 zone 可以获取来自始发集群本地 zone 的所有请求，并且还有一定空间允许来自集群中其他 zone 的流量（如果有必要的话）。 请注意，使用多个优先级时，zone 感知路由当前仅支持 P=0。 所在地加权负载均衡 另一种用于确定如何在不同 zone 和地理位置之间分配权重的方式是使用LocalityLbEndpoints消息中通过 EDS 提供的显式权重。这种方式与上述 zone 感知路由相互排斥，因为在所在地感知 LB 的情况下，我们通过管理服务器来提供所在地加权，而不是在 zone 感知路由中使用的 Envoy 侧启发式的方式。 当所有端点健康时，使用加权循环 round-robin 来挑选本地节点，其中地点权重用于加权。当某地的某些端点不健康时，我们通过调整地点的权重来反映这一点。与优先级一样，我们设置了一个过度提供因子（目前硬编码为 1.4），这意味着当一个地区只有少数端点不健康时，我们不会进行任何权重调整。 假设一个简单的设置，包含 2 个地点 X 和 Y，其中 X 的所在地权重为 1，Y 的所在地权重为 2，L=Y 100％ 健康。 L=X 健康端点 到 L=X 的流量百分比 到 L=Y 的流量百分比 100% 33% 67% 70% 33% 67% 69% 32% 68% 50% 26% 74% 25% 15% 85% 0% 0% 100% 在伪代码中加和： health(L_X) = 140 * healthy_X_backends / total_X_backends effective_weight(L_X) = locality_weight_X * min(100, health(L_X)) load to L_X = effective_weight(L_X) / Σ_c(effective_weight(L_c)) 请注意，在挑选优先级之后进行所在地加权选取。负载均衡器遵循以下步骤： 挑选优先级别。 从（1）中选择优先级别的所在地（如本节所述）。 从（2）中选择使用集群指定的负载均衡器所在地范围内的端点。 通过在集群配置中设置locality_weighted_lb_config并通过load_balancing_weight在LocalityLbEndpoints中提供权重来配置所在地加权负载均衡。 此功能与负载均衡器子集设置不兼容，因为将个别子集的所在地级权重与显而易见的权重进行协调并不容易。 负载均衡器子集 根据附加在主机上的元数据将上游集群中的主机划分为子集，可以这样来配置 Envoy。然后，路由可以指定主机必须匹配的元数据，有了这些元数据负载均衡器才能选择路由，也可以选择回退到预定义的主机集（包括任何主机）。 子集使用集群指定的负载均衡器策略。原始目的地策略不能与子集一起使用，因为上游主机预先不知道这些策略。子集与 zone 感知路由兼容，但请注意，子集的使用可能很容易违反上述最小主机条件。 如果已配置的子集路由未指定元数据或没有匹配元数据的子集存在，则子集负载均衡器将启动其回退策略。默认策略是NO_ENDPOINT，在这种情况下，请求失败，就好像该集群没有主机一样。相反，ANY_ENDPOINT后备策略会在集群中的所有主机上进行负载均衡，而不考虑主机元数据。最后，DEFAULT_SUBSET会导致回退到与特定元数据集匹配的主机之间进行负载均衡。 子集必须被预定义才能让子集负载均衡器有效地选择正确的主机子集。每个定义都是一组密钥，可以转换为零个或多个子集。从概念上讲，具有定义中所有键的元数据值的每个主机都会添加到其键-值对特定的子集。如果所有主机都不拥有密钥，那么定义不会产生子集。可以提供多个定义，并且如果单个主机与多个定义匹配，则可以在多个子集中出现。 在路由期间，路由的元数据匹配配置将用于查找特定的子集。如果存在具有路由指定的确切密钥和值的子集，则该子集将用于负载均衡。否则，使用回退策略。因此，集群的子集配置必须包含一个与给定路由具有相同密钥的定义才能实现子集负载均衡。 此功能只能在 V2 配置 API 中使用。此外，主机元数据仅在集群使用 EDS 发现类型时支持。子集负载均衡的主机元数据必须放在过滤器名称 \"envoy.lb\" 下。同样，路由元数据匹配条件使用 \"envoy.lb\" 过滤器名称。主机元数据可以是分层的（例如，顶级密钥的值可以是结构化值或列表），但子集负载均衡器仅比较顶级密钥和值。因此，在使用结构化值时，如果主机的元数据中出现相同的结构化值，则路由的匹配条件会匹配。 示例 我们将使用所有值都是字符串的简单元数据。假设定义了以下主机并将其与集群关联： 主机 元数据 host1 v: 1.0, stage: prod host2 v: 1.0, stage: prod host3 v: 1.1, stage: canary host4 v: 1.2-pre, stage: dev 集群可以像这样启用子集负载均衡： --- name: cluster-name type: EDS eds_cluster_config: eds_config: path: '.../eds.conf' connect_timeout: seconds: 10 lb_policy: LEAST_REQUEST lb_subset_config: fallback_policy: DEFAULT_SUBSET default_subset: stage: prod subset_selectors: - keys: - v - stage - keys: - stage 下表描述了一些路由及其对集群的应用结果。通常，匹配标准将与匹配请求的特定方面的路由一起使用，例如路径或 header 信息。 匹配标准 负载均衡位于 原因 stage: canary host3 所选主机的子集 v: 1.2-pre, stage: dev host4 所选主机的子集 v: 1.0 host1, host2 回退：没有仅具有 “v” 的子集选择器 other: x host1, host2 回退：没有 “other” 子集选择器 (none) host1, host2 回退：未请求子集 元数据匹配标准也可以在路由的加权集群上指定。来自选定加权集群的元数据匹配条件将与路由中的条件合并，并覆盖该原路由器中的条件： 路由匹配条件 加权集群匹配条件 最终匹配条件 stage: canary stage: prod stage: prod v: 1.0 stage: prod v: 1.0, stage: prod v: 1.0, stage: prod stage: canary v: 1.0, stage: canary v: 1.0, stage: prod v: 1.1, stage: canary v: 1.1, stage: canary (none) v: 1.0 v: 1.0 v: 1.0 (none) v: 1.0 具有元数据的示例主机 具有主机元数据的EDS LbEndpoint： --- endpoint: address: socket_address: protocol: TCP address: 127.0.0.1 port_value: 8888 metadata: filter_metadata: envoy.lb: version: '1.0' stage: 'prod' 具有元数据匹配标准的示例路由 具有元数据匹配标准的 RDS Route： --- match: prefix: / route: cluster: cluster-name metadata_match: filter_metadata: envoy.lb: version: '1.0' stage: 'prod' Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:54:59 "},"intro/arch_overview/outlier.html":{"url":"intro/arch_overview/outlier.html","title":"异常点检测","keywords":"","body":"异常点检测 异常点检测和驱逐是用来动态确定上游集群中是否有主机的表现不同于其他主机的过程，并将它们从健康负载均衡集中移除。性能可能沿着不同的轴，例如连续失败、一时的成功率下降、一时的延迟等。异常检测是被动健康检查的一种形式。Envoy 还支持主动健康检查。被动和主动健康检查既可以一起也可以独立使用，它们共同形成整体上游健康检查解决方案的基础。 驱逐算法 这取决于异常点检测的类型，驱逐或者以内联（例如在连续的 5xx 的情况下）或以指定的间隔（例如在定期成功率的情况下）运行。驱逐算法的工作原理如下： 主机被确定为异常点。 如果没有主机被驱逐，Envoy 会立即驱逐主机。否则，它会检查以确保驱逐主机的数量低于允许的阈值（通过 outlier_detection.max_ejection_percent 设置指定）。如果驱逐的主机数量超过阈值，则主机不会被驱逐。 主机被驱逐需要几毫秒。主机被驱逐意味着主机被标记为不健康，并且在负载均衡期间不会被使用，除非负载均衡器处于恐慌情形中。毫秒数等于 outlier_detection.base_ejection_time_msvalue 乘以主机被驱逐的次数。这会导致主机主机被弹出需要越来越长的时间（如果它们继续失败）。 被驱逐的主机将在弹出时间满足后自动重新投入使用。通常，异常检测与主动健康检查一起用于全面的健康检查解决方案。 检测类型 Envoy 支持以下的异常点检测类型： 连续的 5xx 如果上游主机返回一些连续的 5xx，它将被驱逐。请注意，在这种情况下，5xx 意味着实际的 5xx 响应代码，或者会导致 HTTP 路由器代表上游返回一个事件（重置、连接失败等）的事件。主机被弹出时所需的连续 5xx 数量由 outlier_detection.consecutive_5xx 值控制。 连续的网关故障 如果上游主机返回一些连续的“网关错误”（502、503 或 504 状态码），它将被驱逐。请注意，这包括会导致 HTTP 路由器代表上游返回其中一个状态码的事件（重置、连接失败等）。主机被驱逐所需的连续网关故障的数量由 outlier_detection.consecutive_gateway_failure 值控制。 成功率 基于成功率的异常值驱逐聚合来自集群中每个主机的成功率数据。然后以给定的间隔基于统计异常值检测来驱逐主机。如果主机在聚合时间间隔内的请求量小于 outlier_detection.success_rate_request_volume 值，则无法为主机计算成功率异常点驱逐。此外，如果某个时间间隔内所需的最小请求量的主机数小于 outlier_detection.success_rate_minimum_hosts 值，则不会对集群执行检测。 驱逐事件记录 Envoy 可以选择生成异常点驱逐事件日志。这在日常操作中非常有用，因为全局统计信息不能提供关于哪些主机正在被驱逐以及被驱逐的原因。日志使用 JSON 格式，每行一个对象： { \"time\": \"...\", \"secs_since_last_action\": \"...\", \"cluster\": \"...\", \"upstream_url\": \"...\", \"action\": \"...\", \"type\": \"...\", \"num_ejections\": \"...\", \"enforced\": \"...\", \"host_success_rate\": \"...\", \"cluster_success_rate_average\": \"...\", \"cluster_success_rate_ejection_threshold\": \"...\" } time 事件发生的时间。 secs_since_last_action 自上次动作（被驱逐或未被驱逐）发生时到现的时间（以秒为单位）。由于在第一次驱逐之前没有动作，所以该值将为 -1。 cluster 拥有被驱逐主机的 集群。 upstream_url 被驱逐主机的 URL，例如 tcp://1.2.3.4:80。 action 发生的动作。如果主机被驱逐，则为 eject；如果主机恢复服务，则为 uneject。 type 如果 action 为 eject，type 指定发生的驱逐类型。当前类型可以是 5xx、GatewayFailure 或 SuccessRate。 num_ejections 如果 action 为 eject，则指定主机被驱逐的次数（对于 Envoy 而言是本地的，并且如果主机由于任何原因从上游集群移除然后被重新添加则重置该值）。 enforced 如果 action 为 eject，则指定驱逐是否被强制执行。true 表示主机被驱逐。false 表示事件已记录，但主机未被实际驱逐。 host_success_rate 如果 action 为 eject，指定发生驱逐事件时主机在 0-100 范围内的成功率。 cluster_success_rate_average 如果 action 为 eject，指定发生驱逐事件时集群中主机在 0-100 范围内的平均成功率。 cluster_success_rate_ejection_threshold 如果 action 为 eject 并且 type 为 SuccessRate，指定发生驱逐事件时的成功率驱逐阈值。 参考配置 集群管理器全局配置 单集群配置 运行时设置 统计参考 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 15:04:59 "},"intro/arch_overview/circuit_breaking.html":{"url":"intro/arch_overview/circuit_breaking.html","title":"断路","keywords":"","body":"断路 断路（circuit breaking）是分布式系统的关键组成部分。尽快失败和向下游施加反向压力大体上会得到更好的效果。Envoy 网格的主要优点之一就是 Envoy 在网络级别强制实现断路限制，而不必独立配置和编写每个应用程序。Envoy 支持各种类型的完全分布式（非协调的）断路： 集群最大连接数：Envoy 将为上游集群中的所有主机建立的最大连接数。实际上，这仅适用于 HTTP/1.1集群，因为 HTTP/2 使用到每个主机的单个连接。 集群最大挂起请求数：在等待就绪连接池连接时将排队的最大请求数。实际上，这仅适用于 HTTP/1.1 集群，因为 HTTP/2 连接池不会排队请求。HTTP/2 请求会立即复用。如果该断路器溢出，则集群的upstream_rq_pending_overflowcounter 计数器将增加。 集群最大请求数：在任何给定时间内，集群中所有主机可以处理的最大请求数。实际上，这适用于仅 HTTP/2 集群，因为 HTTP/1.1 集群由最大连接断路器控制。如果该断路器溢出，集群的 upstream_rq_pending_overflow 计数器将递增。 集群最大活动重试次数：在任何给定时间内，集群中所有主机可以执行的最大重试次数。一般而言，我们建议积极进行断路重试，以便做零星故障重试而整体重试数量又不会爆炸式增加和导致大规模级联故障。如果该断路器溢出，集群的 upstream_rq_retry_overflow 计数器将递增。 断路限制可以根据每个上游集群和优先级进行配置和跟踪。这使得分布式系统的不同组件可以独立调整并具有不同的限制。 请注意，在 HTTP 请求的情况下，断路会导致路由器设置 x-envoy-overloaded 头。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 14:23:05 "},"intro/arch_overview/global_rate_limiting.html":{"url":"intro/arch_overview/global_rate_limiting.html","title":"全局速率限制","keywords":"","body":"全局速率限制 Although distributed circuit breaking is generally extremely effective in controlling throughput in distributed systems, there are times when it is not very effective and global rate limiting is desired. The most common case is when a large number of hosts are forwarding to a small number of hosts and the average request latency is low (e.g., connections/requests to a database server). If the target hosts become backed up, the downstream hosts will overwhelm the upstream cluster. In this scenario it is extremely difficult to configure a tight enough circuit breaking limit on each downstream host such that the system will operate normally during typical request patterns but still prevent cascading failure when the system starts to fail. Global rate limiting is a good solution for this case. Envoy integrates directly with a global gRPC rate limiting service. Although any service that implements the defined RPC/IDL protocol can be used, Lyft provides a reference implementationwritten in Go which uses a Redis backend. Envoy’s rate limit integration has the following features: Network level rate limit filter: Envoy will call the rate limit service for every new connection on the listener where the filter is installed. The configuration specifies a specific domain and descriptor set to rate limit on. This has the ultimate effect of rate limiting the connections per second that transit the listener. Configuration reference. HTTP level rate limit filter: Envoy will call the rate limit service for every new request on the listener where the filter is installed and where the route table specifies that the global rate limit service should be called. All requests to the target upstream cluster as well as all requests from the originating cluster to the target cluster can be rate limited. Configuration reference Rate limit service configuration. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 16:18:03 "},"intro/arch_overview/ssl.html":{"url":"intro/arch_overview/ssl.html","title":"TLS","keywords":"","body":"TLS Envoy supports both TLS termination in listeners as well as TLS origination when making connections to upstream clusters. Support is sufficient for Envoy to perform standard edge proxy duties for modern web services as well as to initiate connections with external services that have advanced TLS requirements (TLS1.2, SNI, etc.). Envoy supports the following TLS features: Configurable ciphers: Each TLS listener and client can specify the ciphers that it supports. Client certificates: Upstream/client connections can present a client certificate in addition to server certificate verification. Certificate verification and pinning: Certificate verification options include basic chain verification, subject name verification, and hash pinning. Certificate revocation: Envoy can check peer certificates against a certificate revocation list (CRL) if one is provided. ALPN: TLS listeners support ALPN. The HTTP connection manager uses this information (in addition to protocol inference) to determine whether a client is speaking HTTP/1.1 or HTTP/2. SNI: SNI is supported for both server (listener) and client (upstream) connections. Session resumption: Server connections support resuming previous sessions via TLS session tickets (see RFC 5077). Resumption can be performed across hot restarts and between parallel Envoy instances (typically useful in a front proxy configuration). 底层实现 Currently Envoy is written to use BoringSSL as the TLS provider. 启用认证验证 Certificate verification of both upstream and downstream connections is not enabled unless the validation context specifies one or more trusted authority certificates. Example configuration static_resources: listeners: - name: listener_0 address: { socket_address: { address: 127.0.0.1, port_value: 10000 } } filter_chains: - filters: - name: envoy.http_connection_manager # ... tls_context: common_tls_context: validation_context: trusted_ca: filename: /usr/local/my-client-ca.crt clusters: - name: some_service connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: 127.0.0.2, port_value: 1234 }}] tls_context: common_tls_context: validation_context: trusted_ca: filename: /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt is the default path for the system CA bundle on Debian systems. This makes Envoy verify the server identity of 127.0.0.2:1234 in the same way as e.g. cURL does on standard Debian installations. Common paths for system CA bundles on Linux and BSD are /etc/ssl/certs/ca-certificates.crt (Debian/Ubuntu/Gentoo etc.) /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem (CentOS/RHEL 7) /etc/pki/tls/certs/ca-bundle.crt (Fedora/RHEL 6) /etc/ssl/ca-bundle.pem (OpenSUSE) /usr/local/etc/ssl/cert.pem (FreeBSD) /etc/ssl/cert.pem (OpenBSD) See the reference for UpstreamTlsContexts and DownstreamTlsContexts for other TLS options. 身份验证过滤器 Envoy provides a network filter that performs TLS client authentication via principals fetched from a REST VPN service. This filter matches the presented client certificate hash against the principal list to determine whether the connection should be allowed or not. Optional IP white listing can also be configured. This functionality can be used to build edge proxy VPN support for web infrastructure. Client TLS authentication filter configuration reference. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 16:25:23 "},"intro/arch_overview/statistics.html":{"url":"intro/arch_overview/statistics.html","title":"统计","keywords":"","body":"统计 Envoy 的主要目标之一是使网络可以理解。Envoy 根据不同的配置收集了大量的统计数据。一般来说，统计数据分为以下三类： Downstream：下游统计涉及传入的连接/请求。 它们由侦听器、HTTP 连接管理器、TCP 代理过滤器等生成。 Upstream：上游统计涉及传出连接/请求。它们由连接池、路由器过滤器、TCP 代理过滤器等生成 Server：服务器统计信息描述了 Envoy 服务器实例的工作方式。服务器正常运行时间或分配内存量等统计信息在此处分类。 单个代理场景通常涉及下游和上游统计信息。 这两种类型可用于获取该特定网络跃点的详细图。来自整个网格的统计数据给出了每一跳和整体网络健康状况的非常详细的图。 发布的统计数据在操作指南中详细记录。 在 v1 API中，Envoy 仅支持 statsd 作为统计输出格式。支持 TCP 和 UDP statsd。 截至 v2 API，Envoy 有能力支持自定义、可插拔接收器。Envoy 中包含了一些标准接收器实现。 有些接收器也支持使用标签/维度发布统计信息。 在 Envoy 和整个文档中，统计信息由规范的字符串表示来标识。 这些字符串的动态部分被剥离成为标签。 用户可以通过Tag Specifier配置配置此行为。 特使发出三种类型的值作为统计数据： Counter（计数器）：无符号整数只会增加而不会减少。 例如，总请求。 Gauge（量表）：无符号整数，既有增加也有减少。 例如，当前有效的请求。 Histogram（直方图）：作为值流的一部分的无符号整数，然后由收集器进行汇总以最终生成汇总百分点值。 例如，上游请求时间。 在内部，计数器和计量器被分批并定期冲洗以提高性能。直方图会在收到时写入。 注意：以前称为定时器的内容已成为直方图，因为这两种表示法之间的唯一区别就是单位。 v1 API 参考 v2 API 参考 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 11:09:39 "},"intro/arch_overview/runtime.html":{"url":"intro/arch_overview/runtime.html","title":"运行时配置","keywords":"","body":"运行时配置 Envoy 支持“运行时”配置（也称为“功能标志”和“决策者”）。配置设置可以更改，这将影响运算而无需重新启动 Envoy 或更改主配置。当前支持的实现使用文件系统树。Envoy 监视配置目录中的符号链接交换，并在发生这种情况时重新加载树。这种类型的系统通常在大型分布式系统中部署。其他实现也并不难。支持的运行时配置设置记录在操作指南的相关章节。使用默认的运行时值和“null”提供者，Envoy 也能正确运行，因此不需要存在一个用于运行 Envoy 的系统。 运行时配置。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 15:12:08 "},"intro/arch_overview/tracing.html":{"url":"intro/arch_overview/tracing.html","title":"追踪","keywords":"","body":"追踪 概览 Distributed tracing allows developers to obtain visualizations of call flows in large service oriented architectures. It can be invaluable in understanding serialization, parallelism, and sources of latency. Envoy supports three features related to system wide tracing: Request ID generation: Envoy will generate UUIDs when needed and populate the x-request-idHTTP header. Applications can forward the x-request-id header for unified logging as well as tracing. External trace service integration: Envoy supports pluggable external trace visualization providers. Currently Envoy supports LightStep, Zipkin or any Zipkin compatible backends (e.g. Jaeger). However, support for other tracing providers would not be difficult to add. Client trace ID joining: The x-client-trace-id header can be used to join untrusted request IDs to the trusted internal x-request-id. 如何初始化追踪 The HTTP connection manager that handles the request must have the tracing object set. There are several ways tracing can be initiated: By an external client via the x-client-trace-id header. By an internal service via the x-envoy-force-trace header. Randomly sampled via the random_sampling runtime setting. The router filter is also capable of creating a child span for egress calls via the start_child_spanoption. 追踪上下文传播 Envoy provides the capability for reporting tracing information regarding communications between services in the mesh. However, to be able to correlate the pieces of tracing information generated by the various proxies within a call flow, the services must propagate certain trace context between the inbound and outbound requests. Whichever tracing provider is being used, the service should propagate the x-request-id to enable logging across the invoked services to be correlated. The tracing providers also require additional context, to enable the parent/child relationships between the spans (logical units of work) to be understood. This can be achieved by using the LightStep (via OpenTracing API) or Zipkin tracer directly within the service itself, to extract the trace context from the inbound request and inject it into any subsequent outbound requests. This approach would also enable the service to create additional spans, describing work being done internally within the service, that may be useful when examining the end-to-end trace. Alternatively the trace context can be manually propagated by the service: When using the LightStep tracer, Envoy relies on the service to propagate the x-ot-span-contextHTTP header while sending HTTP requests to other services. When using the Zipkin tracer, Envoy relies on the service to propagate the B3 HTTP headers ( x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, and x-b3-flags). The x-b3-sampledheader can also be supplied by an external client to either enable or disable tracing for a particular request. 每条追踪中包含哪些数据 An end-to-end trace is comprised of one or more spans. A span represents a logical unit of work that has a start time and duration and can contain metadata associated with it. Each span generated by Envoy contains the following data: Originating service cluster set via --service-cluster. Start time and duration of the request. Originating host set via --service-node. Downstream cluster set via the x-envoy-downstream-service-cluster header. HTTP URL. HTTP method. HTTP response code. Tracing system-specific metadata. The span also includes a name (or operation) which by default is defined as the host of the invoked service. However this can be customized using a Decorator on the route. The name can also be overridden using the x-envoy-decorator-operation header. Envoy automatically sends spans to tracing collectors. Depending on the tracing collector, multiple spans are stitched together using common information such as the globally unique request ID x-request-id (LightStep) or the trace ID configuration (Zipkin). See v1 API reference v2 API reference for more information on how to setup tracing in Envoy. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 15:09:10 "},"intro/arch_overview/tcp_proxy.html":{"url":"intro/arch_overview/tcp_proxy.html","title":"TCP 代理","keywords":"","body":"TCP 代理 Envoy 本质上是一个 L3/L4 服务器，实现 L3/L4 的代理功能是一件轻而易举的事情。TCP 代理过滤器可以在下游的客户端与上游的集群间实现最基本的 1:1 网络连接代理功能。 这可以用来做一个安全通道的替代品，或者与其他的过滤器聚合如 MongoDB 过滤器或速率限制过滤器。 TCP 代理过滤器会被上游集群全局资源管理器中使用的连接限制约束。 TCP 代理过滤器需要和上游集群的资源管理器协商是否能在不逾越集群最大连接数的前提下，建立一个新连接，如果答案为否则 TCP 代理将不可以建立新连接。 TCP 代理过滤器参考配置。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 14:41:29 "},"intro/arch_overview/access_logging.html":{"url":"intro/arch_overview/access_logging.html","title":"访问记录","keywords":"","body":"访问记录 HTTP 连接管理与 tcp 代理支持可扩展的访问记录，并拥有以下特性: 可按每个 HTTP 连接管理或 tcp 代理记录任何数量的访问记录。 异步 IO 架构。访问记录将永远不会阻塞主要的网络处理线程。 可定制化的访问记录格式,可使用预制定的字段,更可使用任意的 HTTP request 以及 response。 可定制化的访问记录过滤器,可允许不同类型的请求以及回复写入至不同的访问记录。 访问记录 配置。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 10:20:10 "},"intro/arch_overview/mongo.html":{"url":"intro/arch_overview/mongo.html","title":"MongoDB","keywords":"","body":"MongoDB Envoy 支持网络层的 MongoDB 嗅探过滤器,并拥有如下特性： MongoDB wire 格式的 BSON 转换器。 考虑周全的 MongoDB 查询/操作统计分析报告，包括了时间以及路由集群中的散列/多次获取次数。 查询记录。 通过 $comment 参数做每个调用点的统计分析报告。 故障注入。 MongoDB 过滤器是表现 Envoy 扩展性以及核心抽象能力的典范案例。在 Lyft 我们将这个过滤器应用在所有的应用以及数据库中。 它提供了对应用程序平台和正在使用的特定 MongoDB 驱动程序不可知的重要数据源。 MongoDB 代理过滤器参考配置。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 15:49:53 "},"intro/arch_overview/dynamo.html":{"url":"intro/arch_overview/dynamo.html","title":"DynamoDB","keywords":"","body":"DynamoDB Envoy supports an HTTP level DynamoDB sniffing filter with the following features: DynamoDB API request/response parser. DynamoDB per operation/per table/per partition and operation statistics. Failure type statistics for 4xx responses, parsed from response JSON, e.g., ProvisionedThroughputExceededException. Batch operation partial failure statistics. The DynamoDB filter is a good example of Envoy’s extensibility and core abstractions at the HTTP layer. At Lyft we use this filter for all application communication with DynamoDB. It provides an invaluable source of data agnostic to the application platform and specific AWS SDK in use. DynamoDB filter configuration. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 16:33:08 "},"intro/arch_overview/redis.html":{"url":"intro/arch_overview/redis.html","title":"Redis","keywords":"","body":"Redis Envoy 可以作为 Redis 的代理，在集群的实例间对命令进行分区。在这种模式下，Envoy 的目标是保持可用性和分区容错的一致性。这是将 Envoy 和 Redis 集群 进行比较时的重点。Envoy 被设计为尽力而为的缓存，这意味着它不会尝试协调不一致的数据或保持全局一致的集群成员关系视图。 Redis 项目中提供了与 Redis 分区相关的全面参考。请参阅 ”Partitioning: how to split data among multiple Redis instances“。 Envoy Redis 的特点: Redis 协议 编解码器 基于散列的分区 Ketama 一致性哈希算法 详细的命令统计 主动和被动的健康检查 有计划的未来增强功能: 额外的时间统计 断路器 对分散的命令进行请求折叠 复制 内置的重试功能 跟踪 哈希标记 配置 有关过滤器配置的详细信息，请参阅 Redis 代理过滤器配置参考。 相应的集群定义应该配置为 环哈希负载均衡。 如果需要进行主动健康检查，则应该对集群配置使用 Redis 健康检查。 如果需要被动健康检查，还需要配置 异常检测。 为了进行被动健康检查，需要将连接超时，命令超时和连接关闭都映射到 5xx 响应，而来自 Redis 的所有其他响应都视为成功。 支持的命令 在协议层面，支持流水线，但不支持 MULTI(事务块)。应该尽可能的使用流水线以获得最佳性能。 在命令层面，Envoy 仅支持可能的散列到服务器的命令。PING 命令是唯一的例外，Envoy 将立即使用 PONG 作为回复。对 PING 命令使用参数是不被支持的。所有其他支持的命令都必须包含一个键。受支持的命令在功能上与原始的 Redis 命令相同，除非出现了故障情况。 有关每个命令用法的详细信息，请参阅官方的 Redis 命令参考。 Command Group PING Connection DEL Generic DUMP Generic EXISTS Generic EXPIRE Generic EXPIREAT Generic PERSIST Generic PEXPIRE Generic PEXPIREAT Generic PTTL Generic RESTORE Generic TOUCH Generic TTL Generic TYPE Generic UNLINK Generic GEOADD Geo GEODIST Geo GEOHASH Geo GEOPOS Geo GEORADIUS_RO Geo GEORADIUSBYMEMBER_RO Geo HDEL Hash HEXISTS Hash HGET Hash HGETALL Hash HINCRBY Hash HINCRBYFLOAT Hash HKEYS Hash HLEN Hash HMGET Hash HMSET Hash HSCAN Hash HSET Hash HSETNX Hash HSTRLEN Hash HVALS Hash LINDEX List LINSERT List LLEN List LPOP List LPUSH List LPUSHX List LRANGE List LREM List LSET List LTRIM List RPOP List RPUSH List RPUSHX List EVAL Scripting EVALSHA Scripting SADD Set SCARD Set SISMEMBER Set SMEMBERS Set SPOP Set SRANDMEMBER Set SREM Set SSCAN Set ZADD Sorted Set ZCARD Sorted Set ZCOUNT Sorted Set ZINCRBY Sorted Set ZLEXCOUNT Sorted Set ZRANGE Sorted Set ZRANGEBYLEX Sorted Set ZRANGEBYSCORE Sorted Set ZRANK Sorted Set ZREM Sorted Set ZREMRANGEBYLEX Sorted Set ZREMRANGEBYRANK Sorted Set ZREMRANGEBYSCORE Sorted Set ZREVRANGE Sorted Set ZREVRANGEBYLEX Sorted Set ZREVRANGEBYSCORE Sorted Set ZREVRANK Sorted Set ZSCAN Sorted Set ZSCORE Sorted Set APPEND String BITCOUNT String BITFIELD String BITPOS String DECR String DECRBY String GET String GETBIT String GETRANGE String GETSET String INCR String INCRBY String INCRBYFLOAT String MGET String MSET String PSETEX String SET String SETBIT String SETEX String SETNX String SETRANGE String STRLEN String 失败模式 如果 Redis 抛出了错误，我们会将这个错误作为响应传递给命令。Envoy 将来自 Redis 的响应与错误数据类型视为正常响应，并将它传递给调用者。 Envoy 也会产生自己的错误来响应客户端。 错误 含义 没有上游主机 环哈希负载均衡器在为键的选择的位置上没有可用的主机 上游主机错误 后端没有在超时期限内进行响应或关闭连接 无效的请求 由于数据类型或长度的原因，命令在命令拆分器的第一阶段被拒绝 不支持的命令 该命令未被 Envoy 识别，因此不能被服务，因为它不能被哈希到后端的服务器 返回了多个错误 分段的命令将会组合多个响应(例如 DEL 命令)，将返回接收到的错误总数 上游协议错误 分段命令收到意外的数据类型或后端响应的数据不符合 Redis 协议 命令参数数量错误 Envoy 中的命令参数数量检查未通过 在 MGET 命令中，每个无法被获取的键都会产生一个错误响应。例如，如果我们获取五个键的指并且其中有两个键出现了后端响应超时的错误，我们将会获得每个值得错误响应信息。 $ redis-cli MGET a b c d e 1) \"alpha\" 2) \"bravo\" 3) (error) upstream failure 4) (error) upstream failure 5) \"echo\" Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 10:21:03 "},"intro/arch_overview/hot_restart.html":{"url":"intro/arch_overview/hot_restart.html","title":"热重启","keywords":"","body":"热重启 易于使用是 Envoy 的一个重要的使命。除了健壮性统计以及本地的管理接口，Envoy 还可以 “热” 或者说是 “活” 重启自己。 这意味着 Envoy 可以在不需丢失任何连接的情况下完全地重载自己 (包括代码与配置)。 热重启功能实现如下的一些架构： 统计以及部分的锁将被保存在共享内存区。这也就是意味着这些标准即使在重启时还可以在进程间保持持久化。 两个活跃进程间通讯时，会在unix domain socket的基础上使用基础的 RPC 协议。 新进程首先将所有自身的初始化进程完成（包括载入配置、初始化服务发现和健康检查步骤等等），然后再去请求旧的进程取得侦听 socket 的拷贝。 新进程开始进行侦听并通知旧的线材开始删除动作。 在删除的旧进程的时候，旧进程会尝试优雅地关闭现有的连接。可以怎么去做到这点呢？这取决于配置过滤器。可以使用 --drain-time-s参数进行配置删除时间，如果传的时间参数越大，则删除动作会越发激进。 在删除完成后，新的 Envoy 进程将告知旧的 Envoy 进程将自己关掉。这个时间可以通过 --parent-shutdown-time-s参数进行配置。 Envoy 的热重启支持时经过精心设计的，重启机制可以在新 Envoy 进程与旧 Envoy 进程在不同容器运行时发挥良好作用。 进程间通讯只依赖于 unix domain sockets 。 在源码分发包里，你可以找到一个使用 Python 写的重启/父进程的案例。父进程在标准的流程控制工具中大有用武之地，例如monit、runit等等。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-19 16:39:04 "},"intro/arch_overview/dynamic_configuration.html":{"url":"intro/arch_overview/dynamic_configuration.html","title":"动态配置","keywords":"","body":"动态配置 Envoy 的架构使得使用不同类型的配置管理方法成为可能。部署中采用的方法将取决于实现者的需求。简单部署可以通过全静态配置来实现。更复杂的部署可以递增地添加更复杂的动态配置，缺点是实现者必须提供一个或多个基于外部 REST 配置的提供者 API。本文档概述了当前可用的选项。 顶级配置参考 参考配置 Envoy v2 API 概述. 全静态 在全静态配置中，实现者提供一组监听器 (和过滤器链)、 集群和可选的 HTTP 路由配置。动态主机发现仅能通过基 于DNS 的服务发现。配置重载必须通过内置的热重启机制进行。 虽然简单，但可以使用静态配置和优雅的热重启来创建相当复杂的部署。 仅 SDS/EDS 服务发现服务（SDS）API 提供更高级的机制，Envoy 可以通过该机制发现上游集群中的成员。SDS 已在 v2 API 中重命名为 Endpoint Discovery Service（EDS）。 在静态配置之上，SDS 允许 Envoy 部署规避 DNS 的局限性（响应中的最大记录等），并使用更多可用于负载均衡和路由的信息（例如，金丝雀状态、区域等）。 SDS/EDS 和 CDS 集群发现服务 (CDS) API 是 Envoy 的一种机制，在路由期间可以用来发现使用的上游集群。Envoy 将优雅地添加、更新和删除由 API 指定的集群。该API允许实现者构建拓扑，在其中 Envoy 在初始配置时不需要知道所有上游群集。通常，在与 CDS（但没有路由发现服务）一起进行 HTTP 路由时，实现者将利用路由器的能力将请求转发到在 HTTP 请求头中指定的集群。 尽管可以通过指定全静态集群来使用不带 SDS/EDS 的 CDS，但我们仍建议为通过 CDS 指定的集群使用 SDS/EDS API。 在内部，更新集群定义时，操作是优雅的。但是，所有现有的连接池都将被排空并重新连接。SDS/EDS 不受此限制。当通过 SDS/EDS 添加和删除主机时，集群中的现有主机不受影响。 SDS/EDS、CDS 和 RDS 路由发现服务 (RDS) API 是 Envoy 的一种机制，可以在运行时发现用于HTTP连接管理器过滤器的整个路由配置。路由配置将优雅地交换，而不会影响现有的请求。这个 API 与 SDS/EDS 和 CDS 一起使用时，允许实现者构建复杂的路由拓扑（流量转移、蓝绿部署等），除了获取新的 Envoy 二进制文件外，不需要重启 Envoy。 SDS/EDS、CDS、RDS 和 LDS 监听器发现服务 (LDS) 是 Envoy 的一种机制，可以在运行时发现整个监听器。这包括所有的过滤器堆栈，直到并包括带有内嵌到 RDS 的应用的 HTTP 过滤器。将 LDS 添加到混和中，几乎可以动态配置 Envoy 的每个方面。只有非常少见的配置更改（管理员、追踪驱动程序等）或二进制更新时才需要热启动。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 16:57:28 "},"intro/arch_overview/init.html":{"url":"intro/arch_overview/init.html","title":"初始化","keywords":"","body":"初始化 Envoy 在启动时的初始化是很复杂的。本章将在高级别解释这个过程是如何工作的。以下会在任何监听器启动监听并接收新连接前发生。 启动期间，集群管理器 会首先进行多阶段初始化，首先初始化静态/ DNS 集群，然后是预定义的 SDS 集群. 然后，如果适用，它会初始化 CDS ， 等待响应（或失败）， 并为 CDS 提供的集群执行相同主/次初始化。 如果集群使用 主动健康检查 ，Envoy也会执行单个主动HC轮次。 集群管理器初始化完成后， RDS 和 LDS 将初始化（如果适用）。在LDS / RDS请求至少有一次响应（或失败）之后，服务器才开始接受连接。 如果 LDS 本身返回需要 RDS 响应的监听器，则Envoy会进一步等待，直到收到 RDS 响应（或失败）。请注意，这个过程发生在未来的每个通过 LDS 添加的监听器上，并且被称为 监听器热身。 在先前所有的步骤发生之后，监听器开始接受新的连接。该流程可确保在热启动期间新流程完全能够在旧流程排除之前接受并处理新连接。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 13:29:18 "},"intro/arch_overview/draining.html":{"url":"intro/arch_overview/draining.html","title":"排除","keywords":"","body":"排除 排除是 Envoy 相应各种事件的要求试图优雅地关闭连接的过程。排除发生在以下时机： 服务器通过 healthcheck/fail 管理端点手动设置健康状况检查失败。有关更多信息，请参阅 健康检查过滤器 架构概述。 服务器 热重启。 通过 LDS 修改或者移除单个监听器。 每个 已配置监听器 有 drain_type 设置，用来控制排除何时发生。目前支持的值有： default 对于所有上述三种情况（管理排除，热重启和 LDS 更新/删除），Envoy 将排除监听器。这是默认设置。 modify_only Envoy 只会响应上述第二和第三种情况（热重启和 LDS 更新/删除）而排除监听器。如果 Envoy 同时托管 ingress 和 egress 监听器，此设置很有用。需要在 egress 监听器上设置 modify_only，以便在尝试执行受控关闭，依赖 ingress 监听器排除来执行全服务器排除时，它们只在修改期间排除。 请注意，虽然排除是每监听器的概念，但它必须在网络过滤器级别被支持。目前支持优雅排除的过滤器只有 HTTP 连接管理器, Redis, 和 Mongo。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 16:29:50 "},"intro/arch_overview/scripting.html":{"url":"intro/arch_overview/scripting.html","title":"脚本","keywords":"","body":"脚本 Envoy试验性的支持 Lua 脚本作为专用 HTTP 过滤器 的一部分。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 14:58:23 "},"intro/deployment_types/deployment_types.html":{"url":"intro/deployment_types/deployment_types.html","title":"部署类型","keywords":"","body":"部署类型 Envoy 有多种使用场景，其中更多情况下会作为 mesh 被部署在基础设施的不同主机上。本章节将按照复杂性上升的顺序讨论 envoy 的三种推荐部署类型。 仅服务之间 Service to service egress listener Service to service ingress listener Optional external service egress listeners Discovery service integration Configuration template 服务见外加前端代理 Configuration template 服务间、前端代理、双向代理 Configuration template Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 16:22:02 "},"intro/deployment_types/service_to_service.html":{"url":"intro/deployment_types/service_to_service.html","title":"仅服务之间","keywords":"","body":"仅服务之间 The above diagram shows the simplest Envoy deployment which uses Envoy as a communication bus for all traffic internal to a service oriented architecture (SOA). In this scenario, Envoy exposes several listeners that are used for local origin traffic as well as service to service traffic. 服务间 egress listener This is the port used by applications to talk to other services in the infrastructure. For example,http://localhost:9001. HTTP and gRPC requests use the HTTP/1.1 host header or the HTTP/2:authority header to indicate which remote cluster the request is destined for. Envoy handles service discovery, load balancing, rate limiting, etc. depending on the details in the configuration. Services only need to know about the local Envoy and do not need to concern themselves with network topology, whether they are running in development or production, etc. This listener supports both HTTP/1.1 or HTTP/2 depending on the capabilities of the application. 服务间 ingress listener This is the port used by remote Envoys when they want to talk to the local Envoy. For example,http://localhost:9211. Incoming requests are routed to the local service on the configured port(s). Multiple application ports may be involved depending on application or load balancing needs (for example if the service needs both an HTTP port and a gRPC port). The local Envoy performs buffering, circuit breaking, etc. as needed. Our default configurations use HTTP/2 for all Envoy to Envoy communication, regardless of whether the application uses HTTP/1.1 or HTTP/2 when egressing out of a local Envoy. HTTP/2 provides better performance via long lived connections and explicit reset notifications. 可选外部服务 egress listener Generally, an explicit egress port is used for each external service that a local service wants to talk to. This is done because some external service SDKs do not easily support overriding the hostheader to allow for standard HTTP reverse proxy behavior. For example, http://localhost:9250might be allocated for connections destined for DynamoDB. Instead of using host routing for some external services and dedicated local port routing for others, we recommend being consistent and using local port routing for all external services. 服务发现集成 The recommended service to service configuration uses an external discovery service for all cluster lookups. This provides Envoy with the most detailed information possible for use when performing load balancing, statistics gathering, etc. 配置模板 The source distribution includes an example service to service configuration that is very similar to the version that Lyft runs in production. See here for more information. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 21:38:33 "},"intro/deployment_types/front_proxy.html":{"url":"intro/deployment_types/front_proxy.html","title":"服务之间外加前端代理","keywords":"","body":"服务间外加前端代理 The above diagram shows the service to service configuration sitting behind an Envoy cluster used as an HTTP L7 edge reverse proxy. The reverse proxy provides the following features: Terminates TLS. Supports both HTTP/1.1 and HTTP/2. Full HTTP L7 routing support. Talks to the service to service Envoy clusters via the standard ingress port and using the discovery service for host lookup. Thus, the front Envoy hosts work identically to any other Envoy host, other than the fact that they do not run collocated with another service. This means that are operated in the same way and emit the same statistics. 配置模板 The source distribution includes an example front proxy configuration that is very similar to the version that Lyft runs in production. See here for more information. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 21:38:22 "},"intro/deployment_types/double_proxy.html":{"url":"intro/deployment_types/double_proxy.html","title":"服务间、前端代理、双向代理","keywords":"","body":"服务间、前端代理、双向代理 The above diagram shows the front proxy configuration alongside another Envoy cluster running as a double proxy. The idea behind the double proxy is that it is more efficient to terminate TLS and client connections as close as possible to the user (shorter round trip times for the TLS handshake, faster TCP CWND expansion, less chance for packet loss, etc.). Connections that terminate in the double proxy are then multiplexed onto long lived HTTP/2 connections running in the main data center. In the above diagram, the front Envoy proxy running in region 1 authenticates itself with the front Envoy proxy running in region 2 via TLS mutual authentication and pinned certificates. This allows the front Envoy instances running in region 2 to trust elements of the incoming requests that ordinarily would not be trustable (such as the x-forwarded-for HTTP header). Configuration template The source distribution includes an example double proxy configuration that is very similar to the version that Lyft runs in production. See here for more information. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 21:38:41 "},"intro/comparison.html":{"url":"intro/comparison.html","title":"与类似系统比较","keywords":"","body":"与类似系统比较 一言概之，我们相信 Envoy 有一套独有特色的优秀的功能集合，以支撑现代面向服务的架构。 以下我们将以 Envoy 与类似的其余系统进行比对。 虽然在某些特定领域(例如边缘代理，软负载均衡，服务消息传送层)，Envoy 未如以下的某些系统提供更加完备的支持，但在完整比对后，没有其他任何一个系统 能提供一套例如Envoy一般拥有完备功能，自包含，以及高性能的解决方案。 NOTE: 以下的许多项目还处在一个开发活跃期，我们所描述的信息也许会与项目最新的状态脱节，如果您发现这些情况，请立即告知我们，我们将会进行修正。 nginx nginx是一个经典的现代web服务器。 它支持静态内容展现， HTTP L7反向代理 负载均衡，HTTP/2，以及其他的许多特性。 作为一个边缘反向代理，nginx提供了远远多于 Envoy 的功能特性， 但我们认为大多数的现代面向服务的架构其实不需要用到那么多的特性。 而 Envoy 在下列边缘代理的特性做得比 nginx 更为出色: 完备的HTTP/2 透明代理支持。 Envoy支持HTTP/2 包括上游连接以及下游连接在内的双向通信。 而 nginx 仅仅支持HTTP/2 下游连接。 免费的高级负载功能。 而在 nginx 的世界，只有付费的 nginx plus 服务器才能提供类同于 Envoy 的高级负载功能。 可以在每一个服务节点的边界运行同样一套软件来处理事务。 在许多架构体系中，需要使用 nginx 与 haproxy 的混合部署架构。 相比之下，一个独立的代理解决方案会更有利于后续的运维维护。 haproxy haproxy 是一个经典的现代软负载均衡服务器。它提供基本的 HTTP 反向代理功能。 而 Envoy 在下列负载均衡的特性做得比 haproxy 更为出色： HTTP/2 支持。 可插拔架构。 与远程服务发现服务的整合。 与远程全局限速服务的整合。 提供大量的更为细致的统计分析。 AWS ELB Amazon 的 ELB 为在 Amazon EC2 上运行的程序提供服务发现以及负载均衡服务。而 Envoy 在下列负载均衡以及服务发现的特性做得比 ELB 更为出色: 统计与日志 (CloudWatch 的统计有延迟，并且在一些细节上严重缺乏，而 Amazon 的日志只能在 S3 以特定格式获取) 稳定性(使用 ELB 时不时会遇到不稳定事故，并且难以进行 debug 排查问题) 更为高级的负载均衡以及节点间的直连功能。 Envoy mesh 在进行硬件弹性伸缩时并不需要额外的网络跳转。 负载均衡可以根据区域，金丝雀状态的回馈提供更好的决策以及收集到许多有意思的统计分析结果。 而且负载均衡还支持例如重试这样的高级特性。 AWS最近发布了一个 application load balancer 产品。 这个产品增加了 HTTP/2 支持，可以将 HTTP/2 与基本的 HTTP L7 请求一般流转到不同的后端集群。 相比Envoy，这个产品所能提供的功能偏少，且性能与稳定性还未知，但不容置疑的是AWS将会在这个领域持续地进行研发。 SmartStack SmartStack 这个方案特别有意思，它在 haproxy 的基础上提供了更多的服务发现以及健康检查支持。 抽象地说，SmartStack 在大多数目标上与 Envoy 保持一致 (与进程无关的架构，对应用平台的不可知性等)。 而Envoy在下列负载均衡以及服务发现的特性做得比 SmartStack 更为出色: 上述所提及的做的比haproxy好的所有特性。 整合服务发现与积极的健康检查。 Envoy 将所有的功能都在一个高性能的方案内完整提供。 Finagle Finagle 是 Twitter 基于 Scala/JVM 开发的服务通讯库。 在Twitter 以及其他公司的基于 JVM 的架构中普遍使用。 它提供了服务发现，负载均衡，过滤器等 Envoy 也具有的功能。 而Envoy 在下列负载均衡以及服务发现的特性做得比 Finagle 更为出色: 最终一致的服务发现，基于在分布式的积极健康检查基础上实现 在各个维度上都有更优越的性能表现(内存占用率，CPU使用率，P99 时延) 与进程无关的架构，对应用平台的不可知性的架构。 Envoy与不同的应用技术栈都可一起工作。 proxygen 和 wangle proxygen是 Facebook 使用C+11 开发的高性能 HTTP 代理库， 是基于一个类同 Finagle 的 C++ 库 wangle 进行开发。 在代码实现层面，Envoy使用了类同的技术去实现一个高性能的的HTTP 库/代理。 除此之外，两个项目不具备可比性。 Envoy是一个完备的自包含服务，提供了大量的功能点。相比之下，proxygen 仅仅是一个库，各个项目还需要基于它继续构建功能。 gRPC gRPC是一个由 google 研发的跨平台的消息传送系统。 它使用 IDL 去描述 RPC 库，然后基于 IDL 为各种不同的编程语言生成不同的应用运行时。它底层基于HTTP/2去实现传输层。gRPC 似乎有一个终极的目标，在将来去实现许多 Enovy 的功能点(例如负载均衡)，但在我们写就这篇文档的时候，还有不少的运行时库还不成熟并且集中在序列以及反序列化上。我们认为 gRPC 是 Envoy 的搭档，而不是竞争者。 Envoy如何与gRPC进行整合，您可以查看此链接。 linkerd linkerd是一个独立的，开源的RPC路由代理，它基于 Netty 与 Finagle(Scala/JVM) 研发。 linkerd 提供了许多 Finagle 的特性，包括对延时敏感的负载均衡，连接池，断路器，重试预算，截至线，追踪，细粒度的植入， 以及对请求的流量路由层。linkerd提供一个可插拔的服务发现层 (Consul以及Zookeeper为此提供标准支持，就如 Marathon 以及Kubernetes 的API)。 linkerd的内存消耗以及CPU的要求都远远高于Envoy。 对比Enovy，linkerd仅提供了一个最小可用的配置语言， 且不支持热加载，而是用动态配置以及服务抽象的方式变相地 提供类似的功能。 linkerd支持HTTP/1。1， Thrift， ThriftMux， HTTP/2 (experimental) 和 gRPC (experimental)。 nghttp2 nghttp2 是一个包含了不同事物的项目。 首先，它包含了一个库 (nghttp2)，用来实现HTTP/2协议。Envoy使用这个库(在这个库上做了一层很浅的封装)来做HTTP/2的支持。 这个项目还包含了一个非常有用的压力测试工具(h2load) 以及一个反向代理 (nghttpx)。 如果要做个比对， 在nghttp2 所实现的众多功能点中，我们觉得 Envoy更类似于 nghttpx ， nghttpx是一个透明的 HTTP/1 HTTP/2 反向代理， 支持 TLS 终止， 支持gRPC代理。 我们认为 nghttpx 是一个表现不同代理功能点的杰出范例，而并不是一个健壮的生产可用的解决方案。Envoy的产品焦点更多地放在可监控性，操作的敏捷性，以及高级的负载均衡功能。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 18:15:43 "},"intro/getting_help.html":{"url":"intro/getting_help.html","title":"获取帮助","keywords":"","body":"获取帮助 我们非常有兴致地正在建立一个围绕 Envoy 产品的技术社区。如果您在使用 Envoy 上需要帮助或者是您想为社区做出贡献，请尝试联系我们。 请查看 联系信息。 报告安全性缺陷 请查看 安全联系信息。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 18:15:43 "},"intro/version_history.html":{"url":"intro/version_history.html","title":"历史版本","keywords":"","body":"历史版本 1.7.0 (Pending) access log: ability to log response trailers access log: ability to format START_TIME access log: added DYNAMIC_METADATA access log formatter. access log: added HeaderFilter to filter logs based on request headers admin: added GET /config_dump for dumping the current configuration and associated xDS version information (if applicable). admin: added GET /stats/prometheus as an alternative endpoint for getting stats in prometheus format. admin: added /runtime_modify endpoint to add or change runtime values admin: mutations must be sent as POSTs, rather than GETs. Mutations include:POST /cpuprofiler, POST /healthcheck/fail, POST /healthcheck/ok, POST /logging, POST /quitquitquit, POST /reset_counters,POST /runtime_modify?key1=value1&key2=value2&keyN=valueN, admin: removed /routes endpoint; route configs can now be found at the /config_dump endpoint. buffer filter: the buffer filter can be optionally disabled or overridden with route-local configuration. cli: added –config-yaml flag to the Envoy binary. When set its value is interpreted as a yaml representation of the bootstrap config and overrides –config-path. cluster: Add option to close tcp_proxy upstream connections when health checks fail. cluster: Add option to drain connections from hosts after they are removed from service discovery, regardless of health status. cluster: fixed bug preventing the deletion of all endpoints in a priority health check: added ability to set additional HTTP headers for HTTP health check. health check: added support for EDS delivered endpoint health status. health check: added interval overrides for health state transitions from healthy to unhealthy, unhealthy to healthy and for subsequent checks on unhealthy hosts. health check http filter: added generic header matching to trigger health check response. Deprecated the endpoint option. health check: added support for custom health check. http: filters can now optionally support virtual host, route, and weighted cluster local configuration. http: added the ability to pass DNS type Subject Alternative Names of the client certificate in the x-forwarded-client-cert header. listeners: added tcp_fast_open_queue_length option. load balancing: added weighted round robin support. The round robin scheduler now respects endpoint weights and also has improved fidelity across picks. load balancer: Locality weighted load balancing is now supported. load balancer: ability to configure zone aware load balancer settings through the API logger: added the ability to optionally set the log format via the --log-format option. logger: all logging levels can be configured at run-time: trace debug info warning error critical. sockets: added capture transport socket extension to support recording plain text traffic and PCAP generation. sockets: added IP_FREEBIND socket option support for listeners and upstream connections viacluster manager wide and cluster specific options. sockets: added IP_TRANSPARENT socket option support for listeners. sockets: added SO_KEEPALIVE socket option for upstream connections per cluster. stats: added support for histograms. stats: added option to configure the statsd prefix tls: removed support for legacy SHA-2 CBC cipher suites. tracing: the sampling decision is now delegated to the tracers, allowing the tracer to decide when and if to use it. For example, if the x-b3-sampled header is supplied with the client request, its value will override any sampling decision made by the Envoy proxy. websocket: support configuring idle_timeout and max_connect_attempts. 1.6.0 (March 20, 2018) access log: added DOWNSTREAM_REMOTE_ADDRESS, DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT, and DOWNSTREAM_LOCAL_ADDRESS access log formatters. DOWNSTREAM_ADDRESS access log formatter has been deprecated. access log: added less than or equal (LE) comparison filter. access log: added configuration to runtime filter to set default sampling rate, divisor, and whether to use independent randomness or not. admin: added /runtime admin endpoint to read the current runtime values. build: added support for building Envoy with exported symbols. This change allows scripts loaded with the Lua filter to load shared object libraries such as those installed via LuaRocks. config: added support for sending error details as grpc.rpc.Status in DiscoveryRequest. config: added support for inline delivery of TLS certificates and private keys. config: added restrictions for the backing config sources of xDS resources. For filesystem based xDS the file must exist at configuration time. For cluster based xDS the backing cluster must be statically defined and be of non-EDS type. grpc: the Google gRPC C++ library client is now supported as specified in the gRPC services overview and GrpcService. grpc-json: Added support for inline descriptors. health check: added gRPC health check based on grpc.health.v1.Health service. health check: added ability to set host header value for http health check. health check: extended the health check filter to support computation of the health check response based on the percentage of healthy servers in upstream clusters. health check: added setting for no-traffic interval. http : added idle timeout for upstream http connections. http: added support for proxying 100-Continue responses. http: added the ability to pass a URL encoded PEM encoded peer certificate in the x-forwarded-client-cert header. http: added support for trusting additional hops in the x-forwarded-for request header. http: added support for incoming HTTP/1.0. hot restart: added SIGTERM propagation to children to hot-restarter.py, which enables using it as a parent of containers. ip tagging: added HTTP IP Tagging filter. listeners: added support for listening for both IPv4 and IPv6 when binding to ::. listeners: added support for listening on UNIX domain sockets. listeners: added support for abstract unix domain sockets on Linux. The abstract namespace can be used by prepending ‘@’ to a socket path. load balancer: added cluster configuration for healthy panic threshold percentage. load balancer: added Maglev consistent hash load balancer. load balancer: added support for LocalityLbEndpoints priorities. lua: added headers replace() API. lua: extended to support metadata object API. redis: added local PING support to the Redis filter. redis: added GEORADIUS_RO and GEORADIUSBYMEMBER_RO to the Redis command splitter whitelist. router: added DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT, DOWNSTREAM_LOCAL_ADDRESS, DOWNSTREAM_LOCAL_ADDRESS_WITHOUT_PORT, PROTOCOL, and UPSTREAM_METADATA header formatters. The CLIENT_IP header formatter has been deprecated. router: added gateway-error retry-on policy. router: added support for route matching based on URL query string parameters. router: added support for more granular weighted cluster routing by allowing the total_weightto be specified in configuration. router: added support for custom request/response headers with mixed static and dynamic values. router: added support for direct responses. I.e., sending a preconfigured HTTP response without proxying anywhere. router: added support for HTTPS redirects on specific routes. router: added support for prefix_rewrite for redirects. router: added support for stripping the query string for redirects. router: added support for downstream request/upstream response header manipulation in weighted cluster. router: added support for range based header matching for request routing. squash: added support for the Squash microservices debugger. Allows debugging an incoming request to a microservice in the mesh. stats: added metrics service API implementation. stats: added native DogStatsd support. stats: added support for fixed stats tag values which will be added to all metrics. tcp proxy: added support for specifying a metadata matcher for upstream clusters in the tcp filter. tcp proxy: improved TCP proxy to correctly proxy TCP half-close. tcp proxy: added idle timeout. tcp proxy: access logs now bring an IP address without a port when using DOWNSTREAM_ADDRESS. Use DOWNSTREAM_REMOTE_ADDRESS instead. tracing: added support for dynamically loading an OpenTracing tracer. tracing: when using the Zipkin tracer, it is now possible for clients to specify the sampling decision (using the x-b3-sampled header) and have the decision propagated through to subsequently invoked services. tracing: when using the Zipkin tracer, it is no longer necessary to propagate the x-ot-span-context header. See more on trace context propagation here. transport sockets: added transport socket interface to allow custom implementations of transport sockets. A transport socket provides read and write logic with buffer encryption and decryption (if applicable). The existing TLS implementation has been refactored with the interface. upstream: added support for specifying an alternate stats name while emitting stats for clusters. Many small bug fixes and performance improvements not listed. 1.5.0 (December 4, 2017) access log: added fields for UPSTREAM_LOCAL_ADDRESS and DOWNSTREAM_ADDRESS. admin: added JSON output for stats admin endpoint. admin: added basic Prometheus output for stats admin endpoint. Histograms are not currently output. admin: added version_info to the /clusters admin endpoint. config: the v2 API is now considered production ready. config: added --v2-config-only CLI flag. cors: added CORS filter. health check: added x-envoy-immediate-health-check-fail header support. health check: added reuse_connection option. http: added per-listener stats. http: end-to-end HTTP flow control is now complete across both connections, streams, and filters. load balancer: added subset load balancer. load balancer: added ring size and hash configuration options. This used to be configurable via runtime. The runtime configuration was deleted without deprecation as we are fairly certain no one is using it. log: added the ability to optionally log to a file instead of stderr via the --log-path option. listeners: added drain_type option. lua: added experimental Lua filter. mongo filter: added fault injection. mongo filter: added “drain close” support. outlier detection: added HTTP gateway failure type. See DEPRECATED.md for outlier detection stats deprecations in this release. redis: the redis proxy filter is now considered production ready. redis: added “drain close” functionality. router: added x-envoy-overloaded support. router: added regex route matching. router: added custom request headers for upstream requests. router: added downstream IP hashing for HTTP ketama routing. router: added cookie hashing. router: added start_child_span option to create child span for egress calls. router: added optional upstream logs. router: added complete custom append/override/remove support of request/response headers. router: added support to specify response code during redirect. router: added configuration to return either a 404 or 503 if the upstream cluster does not exist. runtime: added comment capability. server: change default log level (-l) to info. stats: maximum stat/name sizes and maximum number of stats are now variable via the--max-obj-name-len and --max-stats options. tcp proxy: added access logging. tcp proxy: added configurable connect retries. tcp proxy: enable use of outlier detector. tls: added SNI support. tls: added support for specifying TLS session ticket keys. tls: allow configuration of the min and max TLS protocol versions. tracing: added custom trace span decorators. Many small bug fixes and performance improvements not listed. 1.4.0 (August 24, 2017) macOS is now supported. (A few features are missing such as hot restart and original destination routing). YAML is now directly supported for config files. Added /routes admin endpoint. End-to-end flow control is now supported for TCP proxy, HTTP/1, and HTTP/2. HTTP flow control that includes filter buffering is incomplete and will be implemented in 1.5.0. Log verbosity compile time flag added. Hot restart compile time flag added. Original destination cluster and load balancer added. WebSocket is now supported. Virtual cluster priorities have been hard removed without deprecation as we are reasonably sure no one is using this feature. Route validate_clusters option added. x-envoy-downstream-service-node header added. x-forwarded-client-cert header added. Initial HTTP/1 forward proxy support for absolute URLs has been added. HTTP/2 codec settings are now configurable. gRPC/JSON transcoder filter added. gRPC web filter added. Configurable timeout for the rate limit service call in the network and HTTP rate limit filters. x-envoy-retry-grpc-on header added. LDS API added. TLS require_client_certificate option added. Configuration check tool added. JSON schema check tool added. Config validation mode added via the --mode option. --local-address-ip-version option added. IPv6 support is now complete. UDP statsd_ip_address option added. Per-cluster DNS resolvers added. Fault filter enhancements and fixes. Several features are deprecated as of the 1.4.0 release. They will be removed at the beginning of the 1.5.0 release cycle. We explicitly call out that the HttpFilterConfigFactory filter API has been deprecated in favor of NamedHttpFilterConfigFactory. Many small bug fixes and performance improvements not listed. 1.3.0 (May 17, 2017) As of this release, we now have an official breaking change policy. Note that there are numerous breaking configuration changes in this release. They are not listed here. Future releases will adhere to the policy and have clear documentation on deprecations and changes. Bazel is now the canonical build system (replacing CMake). There have been a huge number of changes to the development/build/test flow. See /bazel/README.md and /ci/README.md for more information. Outlier detection has been expanded to include success rate variance, and all parameters are now configurable in both runtime and in the JSON configuration. TCP level listener and cluster connections now have configurable receive buffer limits at which point connection level back pressure is applied. Full end to end flow control will be available in a future release. Redis health checking has been added as an active health check type. Full Redis support will be documented/supported in 1.4.0. TCP health checking now supports a “connect only” mode that only checks if the remote server can be connected to without writing/reading any data. BoringSSL is now the only supported TLS provider. The default cipher suites and ECDH curves have been updated with more modern defaults for both listener and cluster connections. The header value match rate limit action has been expanded to include an expect matchparameter. Route level HTTP rate limit configurations now do not inherit the virtual host level configurations by default. The include_vh_rate_limits to inherit the virtual host level options if desired. HTTP routes can now add request headers on a per route and per virtual host basis via therequest_headers_to_add option. The example configurations have been refreshed to demonstrate the latest features. per_try_timeout_ms can now be configured in a route’s retry policy in addition to via the x-envoy-upstream-rq-per-try-timeout-ms HTTP header. HTTP virtual host matching now includes support for prefix wildcard domains (e.g., *.lyft.com). The default for tracing random sampling has been changed to 100% and is still configurable inruntime. HTTP tracing configuration has been extended to allow tags to be populated from arbitrary HTTP headers. The HTTP rate limit filter can now be applied to internal, external, or all requests via the request_type option. Listener binding now requires specifying an address field. This can be used to bind a listener to both a specific address as well as a port. The MongoDB filter now emits a stat for queries that do not have $maxTimeMS set. The MongoDB filter now emits logs that are fully valid JSON. The CPU profiler output path is now configurable. A watchdog system has been added that can kill the server if a deadlock is detected. A route table checking tool has been added that can be used to test route tables before use. We have added an example repo that shows how to compile/link a custom filter. Added additional cluster wide information related to outlier detection to the /clusters admin endpoint. Multiple SANs can now be verified via the verify_subject_alt_name setting. Additionally, URI type SANs can be verified. HTTP filters can now be passed opaque configuration specified on a per route basis. By default Envoy now has a built in crash handler that will print a back trace. This behavior can be disabled if desired via the --define=signal_trace=disabled Bazel option. Zipkin has been added as a supported tracing provider. Numerous small changes and fixes not listed here. 1.2.0 (March 7, 2017) Cluster discovery service (CDS) API. Outlier detection (passive health checking). Envoy configuration is now checked against a JSON schema. Ring hash consistent load balancer, as well as HTTP consistent hash routing based on a policy. Vastly enhanced global rate limit configuration via the HTTP rate limiting filter. HTTP routing to a cluster retrieved from a header. Weighted cluster HTTP routing. Auto host rewrite during HTTP routing. Regex header matching during HTTP routing. HTTP access log runtime filter. LightStep tracer parent/child span association. Route discovery service (RDS) API. HTTP router x-envoy-upstream-rq-timeout-alt-response header support. use_original_dst and bind_to_port listener options (useful for iptables based transparent proxy support). TCP proxy filter route table support. Configurable stats flush interval. Various third party library upgrades, including using BoringSSL as the default SSL provider. No longer maintain closed HTTP/2 streams for priority calculations. Leads to substantial memory savings for large meshes. Numerous small changes and fixes not listed here. 1.1.0 (November 30, 2016) Switch from Jannson to RapidJSON for our JSON library (allowing for a configuration schema in 1.2.0). Upgrade recommended version of various other libraries. Configurable DNS refresh rate for DNS service discovery types. Upstream circuit breaker configuration can be overridden via runtime. Zone aware routing support. Generic header matching routing rule. HTTP/2 graceful connection draining (double GOAWAY). DynamoDB filter per shard statistics (pre-release AWS feature). Initial release of the fault injection HTTP filter. HTTP rate limit filter enhancements (note that the configuration for HTTP rate limiting is going to be overhauled in 1.2.0). Added refused-stream retry policy. Multiple priority queues for upstream clusters (configurable on a per route basis, with separate connection pools, circuit breakers, etc.). Added max connection circuit breaking to the TCP proxy filter. Added CLI options for setting the logging file flush interval as well as the drain/shutdown time during hot restart. A very large number of performance enhancements for core HTTP/TCP proxy flows as well as a few new configuration flags to allow disabling expensive features if they are not needed (specifically request ID generation and dynamic response code stats). Support Mongo 3.2 in the Mongo sniffing filter. Lots of other small fixes and enhancements not listed. 1.0.0 (September 12, 2016) Initial open source release. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 22:06:40 "},"start/start.html":{"url":"start/start.html","title":"入门指南","keywords":"","body":"入门指南 本章节会介绍非常简单的配置，并提供一些示例。 Envoy 目前不提供单独的预先编译好的二进制文件，但提供了 Docker 镜像。这是开始使用 Envoy 的最快方式。如果您希望在 Docker 容器外使用 Envoy，则需要构建它。 这些示例使用 v2 Envoy API，但仅使用 API 的静态配置功能，这对于简单的需求非常有用。 更复杂的需求是由动态配置来支持的。 快速开始运行简单示例 根据 Envoy 存储库中的文件运行这些命令。下面的部分给出了配置文件和执行步骤更详细的解释。 configs/google_com_proxy.v2.yaml 中提供了一个非常简单的可用于验证基于纯 HTTP 代理的 Envoy 配置。 这不表示实际的 Envoy 部署： $ docker pull envoyproxy/envoy:latest $ docker run --rm -d -p 10000:10000 envoyproxy/envoy:latest $ curl -v localhost:10000 使用的 Docker 镜像将包含最新版本的 Envoy 和一个基本的 Envoy 配置。此基本配置告诉 Envoy 将入站请求路由到 *.google.com。 简单的配置 Envoy 通过 YAML 文件中传入的参数来进行配置。 admin message 是 administration 服务必须的配置。address 键指定监听地址，下面的例子监听地址是 0.0.0.0:9901。 admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources 包含 Envoy 启动时静态配置的所有内容，而不是 Envoy 在运行时动态配置的资源。v2 API Overview 描述了这一点。 static_resources: listeners 规范 listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { host_rewrite: www.google.com, cluster: service_google } http_filters: - name: envoy.router clusters 规范 clusters: - name: service_google connect_timeout: 0.25s type: LOGICAL_DNS # Comment out the following line to test on v6 networks dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: google.com, port_value: 443 }}] tls_context: { sni: www.google.com } 使用 Envoy Docker 镜像 创建一个简单的 Dockerfile 来执行 Envoy，假定 envoy.yaml（如上所述）位于本地目录中。您可以参考命令行选项。 FROM envoyproxy/envoy:latest COPY envoy.yaml /etc/envoy/envoy.yaml 使用以下命令构建您配置的 Docker 镜像： $ docker build -t envoy:v1 现在您可以执行它： $ docker run -d --name envoy -p 9901:9901 -p 10000:10000 envoy:v1 最后测试使用： $ curl -v localhost:10000 如果您想通过 docker-compose 使用 envoy，则可以使用 volume 覆盖提供的配置文件。 Sandbox 我们使用 Docker Compose 创建了许多 sandbox ，这些 sandbox 设置了不同的环境来测试 Envoy 的功能并显示示例配置。 当我们觉得人们更有兴趣时，将添加和展示更多不同特征的 sandbox。 以下 sandbox 可用： Front Proxy Zipkin Tracing Jaeger Tracing Jaeger Native Tracing gRPC Bridge 其他用例 除代理本身之外， Envoy 还被几个特定用例捆绑为开源发行版的一部分。 Envoy as an API Gateway in Kubernetes Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 13:29:48 "},"start/sandboxes/front_proxy.html":{"url":"start/sandboxes/front_proxy.html","title":"前端代理","keywords":"","body":"前端代理 为了帮助大家了解如何使用 Envoy 作为前端代理，我们发布了一个 docker compose 沙箱，该沙箱中部署了一个前端 envoy 以及与服务 envoy 搭配的一组服务（简单的沙箱应用）。这三个容器将被部署在名为 envoymesh 的虚拟网络中。 下面是使用 docker compose 部署的架构图： 所有传入的请求都通过前端 envoy 进行路由，该 envoy 充当位于 envoymesh 网络边缘的反向代理。通过docker compose 将端口 80 映射到 8000 端口（请参阅 /examples/front-proxy/docker-compose.yml）。此外，请注意，由前端 envoy 由到服务容器的所有流量实际上路由到服务 envoy（在 /examples/front-proxy/front-envoy.yaml 中设置的路由）。反过来，服务 envoy 通过回环地址（/examples/front-proxy/service-envoy.yaml 中的路由设置）将请求路由到 flask 应用程序。此设置说明了运行服务 envoy 与您的服务搭配的优势：所有请求都由服务 envoy 处理，并有效地路由到您的服务。 运行 Sandbox 以下文档通过按照上图中所述组织的 envoy 集群的设置运行。 步骤 1：安装 Docker 确保您已经安装了最新版本的 docker、docker-compose 和 docker-machine。 安装这些软件最简单的方式是使用 Docker Toolbox。 步骤 2：配置 Docker Machine 首先创建一个容纳容器的新机器： $ docker-machine create --driver virtualbox default $ eval $(docker-machine env default) 步骤 3：克隆 Envoy repo，启动所有的容器 如果您还没有克隆 envoy repo，执行 git clone git@github.com:envoyproxy/envoy 或者 git clone https://github.com/envoyproxy/envoy.git 来克隆。 $ pwd envoy/examples/front-proxy $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- example_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp example_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp 步骤 4：测试 Envoy 的路由能力 您现在可以通过前端 envoy 向两项服务发送请求。 向 service1： $ curl -v $(docker-machine ip default):8000/service/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 向 service2： $ curl -v $(docker-machine ip default):8000/service/2 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/2 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 请注意，每个请求在发送给前端 envoy 时已正确路由到相应的应用程序。 步骤 5：测试 Envoy 的负载均衡能力 Now let’s scale up our service1 nodes to demonstrate the clustering abilities of envoy.: 现在扩展我们的 service1 节点来演示 envoy 的集群能力： $ docker-compose scale service1=3 Creating and starting example_service1_2 ... done Creating and starting example_service1_3 ... done 现在，如果我们多次向 service1 发送请求，前端 envoy 将通过循环轮询三台 service1 机器来负载均衡请求： $ curl -v $(docker-machine ip default):8000/service/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > GET /service/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 步骤 6：进入容器并 curl 服务 In addition of using curl from your host machine, you can also enter the containers themselves and curl from inside them. To enter a container you can use docker-compose exec /bin/bash. For example we can enter the front-envoy container, and curl for services locally: 除了使用主机上的 curl 外，您还可以进入容器并从容器里面 curl。要进入容器，可以使用 docker-compose exec /bin/bash。例如，我们可以进入前端 envoy 容器，并在本地 curl 服务： $ docker-compose exec front-envoy /bin/bash root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: 85ac151715c6 resolvedhostname: 172.19.0.3 root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: 20da22cfc955 resolvedhostname: 172.19.0.5 root@81288499f9d7:/# curl localhost:80/service/1 Hello from behind Envoy (service 1)! hostname: f26027f1ce28 resolvedhostname: 172.19.0.6 root@81288499f9d7:/# curl localhost:80/service/2 Hello from behind Envoy (service 2)! hostname: 92f4a3737bbc resolvedhostname: 172.19.0.2 步骤7：进入容器并 curl admin 当 envoy 运行时，它也会将 admin 连接到所需的端口。在示例配置 admin 绑定到 8001 端口。我们可以 curl 它获得有用的信息。例如，您可以 curl /server_info 来获取正在运行的 envoy 版本的信息。此外，你可以 curl /stats 来获得统计数据。例如在 frontenvoy 里面我们可以得到： $ docker-compose exec front-envoy /bin/bash root@e654c2c83277:/# curl localhost:8001/server_info envoy 10e00b/RELEASE live 142 142 0 root@e654c2c83277:/# curl localhost:8001/stats cluster.service1.external.upstream_rq_200: 7 ... cluster.service1.membership_change: 2 cluster.service1.membership_total: 3 ... cluster.service1.upstream_cx_http2_total: 3 ... cluster.service1.upstream_rq_total: 7 ... cluster.service2.external.upstream_rq_200: 2 ... cluster.service2.membership_change: 1 cluster.service2.membership_total: 1 ... cluster.service2.upstream_cx_http2_total: 1 ... cluster.service2.upstream_rq_total: 2 ... 请注意，我们可以获取上游集群的成员数量，它们实现的请求数量，有关 http 入口的信息以及大量其他有用的统计信息。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-16 22:33:38 "},"start/sandboxes/zipkin_tracing.html":{"url":"start/sandboxes/zipkin_tracing.html","title":"Zipkin 追踪","keywords":"","body":"Zipkin 追踪 Zipkin 追踪 standbox 使用Zipkin 作为追踪提供者演示Envoy的请求追踪 功能。此沙箱与上述前端代理体系结构非常相似，但有一点不同：在返回响应之前，service1 会对 service2 进行 API 调用。 这三个容器将被部署在名为envoymesh的虚拟网络中。 所有传入的请求都通过前端 envoy 进行路由，该envoy 充当位于 envoymesh 网络边缘的反向代理。 端口80通过 docker compose 映射到端口8000（参见/examples/zipkin-tracing/docker-compose.yml）。请注意，所有 envoy 都配置为收集请求跟踪（例如 http_connection_manager/config/tracing 中的设置/examples/zipkin-tracing/front-envoy-zipkin.yaml）并设置为将 Zipkin 追踪器生成的跨度传播到 Zipkin 集群中（跟踪驱动程序设置/examples/zipkin-tracing/front-envoy-zipkin.yaml）。 在将请求路由到相应的服务 envoy 或应用程序之前，Envoy 将负责为跟踪生成适当的跨度（父/子/共享上下文跨度）。 在高层次上，每个跨度记录上游API调用的延迟以及将跨度与其他相关跨度（例如跟踪ID）关联所需的信息。 从 Envoy 进行跟踪的最重要的好处之一是，它将负责将跟踪传播到 Zipkin 服务群集。 但是，为了充分利用跟踪，应用程序必须传播 Envoy 生成的跟踪标头，同时调用其他服务。 在我们提供的沙箱中，简单的应用程序（请参阅/examples/front-proxy/service.py）作为 service1 传播跟踪头，同时对 service2 进行出站呼叫。 运行 Sandbox 以下文档通过按照上图中所述组织的 envoy 集群的设置运行。 第1步：构建 sandbox 要构建这个沙盒示例，并启动示例应用程序，请运行以下命令： $ pwd envoy/examples/zipkin-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- zipkintracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp zipkintracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp zipkintracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp 第2步：生成一些负载 您现在可以通过前台特使向 service1 发送请求，如下所示： $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 第3步：在 Zipkin UI 中查看追踪 使用您的浏览器访问 http://localhost:9411。 你应该看到 Zipkin 仪表板。 如果这个 IP 地址不正确，你可以运行：$ docker-machine ip default来找到正确的地址。, 将服务设置为“前台代理”，并将开始时间设置为在测试开始前几分钟（步骤2）并按回车。你应该看到前端代理的痕迹。 单击一个跟踪来探索从前端代理到service1到service2的请求所采用的路径，以及每个跃点产生的延迟。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 23:43:49 "},"start/sandboxes/jaeger_tracing.html":{"url":"start/sandboxes/jaeger_tracing.html","title":"Jaeger 追踪","keywords":"","body":"Jaeger 追踪 Jaeger 追踪沙箱展示了 Envoy 的 请求追踪 能力，它使用 Jaeger 作为追踪的提供者。此沙箱与上述前端代理体系结构非常相似，但有一点不同：在返回响应之前，service1 会对 service2 进行一次 API 调用。这三个容器将被部署在名为 envoymesh 的虚拟网络中。（注意：沙箱只能在 x86-64 上运行）。 所有传入的请求都通过前端 envoy 进行路由，该 envoy 充当位于 envoymesh 网络边缘的反向代理。端口 80 被 docker compose 映射到端口 8000（参见/examples/jaeger-native-tracing/docker-compose.yml）。请注意，所有的 envoy 都被配置为收集请求跟踪信息（例如，/examples/jaeger-tracing/front-envoy-jaeger.yaml 中配置的 http_connection_manager/config/tracing），并将 Jaeger 追踪器生成的 span 传播到 Jaeger 集群中（跟踪驱动在 /examples/jaeger-tracing/front-envoy-jaeger.yaml 中配置）。 在将请求路由到相应的服务 envoy 或应用之前，Envoy 将负责为追踪生成恰当的 span（父/子上下文 span）。在高层次上，每个 span 将记录上游 API 调用的延迟以及将该 span 与其他相关 span 进行关联所需的信息（例如 trace ID）。 Envoy 追踪最重要的好处之一是它将负责将追踪行为传播到 Jaeger 服务集群中。然而，为了充分利用追踪机制，在对其余服务进行请求时，应用必须传播 Envoy 生成的追踪 header。在我们提供的沙箱中，一个简单的 flask 应用（请参阅追踪函数 /examples/front-proxy/service.py）将作为 service1，在对外请求 service2 时传播追踪 header。 运行沙箱 以下文档按照上图描述对一个 envoy 集群的配置过程进行了演练。 步骤1：建立沙箱 要构建这个沙箱示例并启动示例应用程序，请运行以下命令： $ pwd envoy/examples/jaeger-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- jaegertracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp 步骤2：生成一些负载 您现在可以通过前端 envoy（front-envoy）向 service1 发送请求，如下所示： $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 步骤3：在 Jaeger UI 中查看追踪 在浏览器中打开 http://localhost:16686。您应该可以看到 Jaeger 仪表盘。设置服务为 “front-proxy” 并点击 ‘Find Traces’。您应该可以看到从 front-proxy 发起的追踪信息。请单击一个追踪来查看从 front-proxy 到 service1 再到 service2 的请求路径，以及每一跳产生的延迟。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 15:51:50 "},"start/sandboxes/jaeger_native_tracing.html":{"url":"start/sandboxes/jaeger_native_tracing.html","title":"Jaeger 原生追踪","keywords":"","body":"Jaeger 原生追踪 Jaeger 追踪沙箱展示了 Envoy 的 请求追踪 能力，它使用 Jaeger 作为追踪的提供者，并且使用 Jaeger 的原生 C++ 客户端 作为插件。使用 Jaeger 及其原生客户端替代 Envoy 内置的 Zipkin 客户端有以下优势： Jaeger 可使追踪传播（trace propagation）与其他服务一起工作而无需进行配置 更改。 可以使用各种不同的 采样策略，包括可以从 Jaeger 后端集中控制的概率采样策略和远程采样策略。 Spans 以更高效的二进制编码发送到采集器。 此沙箱与上述前端代理体系结构非常相似，但有一点不同：在返回响应之前，service1 会对 service2 进行一次 API 调用。这三个容器将被部署在名为 envoymesh 的虚拟网络中。（注意：沙箱只能在 x86-64 上运行）。 所有传入的请求都通过前端 envoy 进行路由，该 envoy 充当位于 envoymesh 网络边缘的反向代理。端口 80 被 docker compose 映射到端口 8000（参见/examples/jaeger-native-tracing/docker-compose.yml）。请注意，所有的 envoy 都被配置为收集请求跟踪信息（例如，/examples/jaeger-native-tracing/front-envoy-jaeger.yaml 中配置的 http_connection_manager/config/tracing），并将 Jaeger 追踪器生成的 span 传播到 Jaeger 集群中（跟踪驱动在 /examples/jaeger-native-tracing/front-envoy-jaeger.yaml 中配置）。 在将请求路由到相应的服务 envoy 或应用之前，Envoy 将负责为追踪生成恰当的 span（父/子上下文 span）。在高层次上，每个 span 将记录上游 API 调用的延迟以及将该 span 与其他相关 span 进行关联所需的信息（例如 trace ID）。 Envoy 追踪最重要的好处之一是它将负责将追踪行为传播到 Jaeger 服务集群中。然而，为了充分利用追踪机制，在对其余服务进行请求时，应用必须传播 Envoy 生成的追踪 header。在我们提供的沙箱中，一个简单的 flask 应用（请参阅追踪函数 /examples/front-proxy/service.py）将作为 service1，在对外请求 service2 时传播追踪 header。 运行沙箱 以下文档按照上图描述对一个 envoy 集群的配置过程进行了演练。 步骤1：建立沙箱 要构建这个沙箱示例并启动示例应用程序，请运行以下命令： $ pwd envoy/examples/jaeger-native-tracing $ docker-compose up --build -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- jaegertracing_service1_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_service2_1 /bin/sh -c /usr/local/bin/ ... Up 80/tcp jaegertracing_front-envoy_1 /bin/sh -c /usr/local/bin/ ... Up 0.0.0.0:8000->80/tcp, 0.0.0.0:8001->8001/tcp 步骤2：生成一些负载 您现在可以通过前端 envoy（front-envoy）向 service1 发送请求，如下所示： $ curl -v $(docker-machine ip default):8000/trace/1 * Trying 192.168.99.100... * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) > GET /trace/1 HTTP/1.1 > Host: 192.168.99.100:8000 > User-Agent: curl/7.43.0 > Accept: */* > 步骤3：在 Jaeger UI 中查看追踪 在浏览器中打开 http://localhost:16686。您应该可以看到 Jaeger 仪表盘。设置服务为 “front-proxy” 并点击 ‘Find Traces’。您应该可以看到从 front-proxy 发起的追踪信息。请单击一个追踪来查看从 front-proxy 到 service1 再到 service2 的请求路径，以及每一跳产生的延迟。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 15:51:00 "},"start/distro/ambassador.html":{"url":"start/distro/ambassador.html","title":"Envoy 作为 Kubernetes 的 API 网关","keywords":"","body":"Envoy 作为 Kubernetes 的 API 网关 使用 Ambassador 的一个常见场景是将其部署为 Kubernetes 的 edge 服务（ API 网关）。Ambassador 是开源 Envoy 的分布式版本，专门为 kubernetes 设计的。 本例将介绍如何通过 Ambassador 在 Kubernetes 上部署 Ambassador 。 部署 Ambassador Ambassador 的设置是通过 kubernetes 部署的。为了在 kubernetes 安装 Ambassador/Envoy，如果你的集群启动了 RBAC： kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-rbac.yaml 如果您没启动 RBAC： kubectl apply -f https://www.getambassador.io/yaml/ambassador/ambassador-no-rbac.yaml 上面的 YAML 将会为 Ambassador 创建 kubernetes 部署，包含 readiness 和 liveness 检查。默认，将会创建3个 Ambassador 实例。每一个 Ambassador 实例包含一个 Envoy 代理以及一个 Ambassador 控制器。 我们现在需要创建一个 Kubernetes 服务来指向 Ambassador 的部署，我们将使用 LoadBalancer 服务。如果你的集群不支持 LoadBalancer 服务，你需要改成 NodePort 或者 ClusterIP。 --- apiVersion: v1 kind: Service metadata: labels: service: ambassador name: ambassador spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: service: ambassador 将上面的 YAML 文件保存成ambassador-svc.yaml文件。然后将这个服务部署到 kubernetes： kubectl apply -f ambassador-svc.yaml 这时候 Envoy 和 Ambassador 控制器已经在你的集群上运行。 配置 Ambassador Ambassador 使用 Kubernetes 注解来添加或删除配置。这个示例 YAML 将添加一条到 Google 的路由，类似于入门指南中的基本配置示例。 --- apiVersion: v1 kind: Service metadata: name: google annotations: getambassador.io/config: | --- apiVersion: ambassador/v0 kind: Mapping name: google_mapping prefix: /google/ service: https://google.com:443 host_rewrite: www.google.com spec: type: ClusterIP clusterIP: None 保存上面的文件，命名为 google.yaml。然后运行: kubectl apply -f google.yaml Ambassador 将发现您的 Kubernetes 注解的更改，并添加到 Envoy 的路由。注意，我们在这个例子中使用了一个虚拟服务;通常，您会将注解与真正的 Kubernetes 服务关联起来。 测试映射 您可以通过获得 Ambassador 服务的外部 IP 地址来测试这个映射，然后通过curl发送请求： $ kubectl get svc ambassador NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE ambassador 10.19.241.98 35.225.154.81 80:32491/TCP 15m $ curl -v 35.225.154.81/google/ 更多 Ambassador 在上公开了多个 Envoy 的特性映射，比如 CORS 、加权循环调度算法、gRPC、TLS 和超时设定。要了解更多信息，请阅读配置文档。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 19:50:36 "},"install/building.html":{"url":"install/building.html","title":"构建","keywords":"","body":"构建 Envoy 构建系统使用了 Bazel 。为了简化初始构建和快速启动，我们提供了一个基于 Ubuntu 16的 docker 容器，它内部包含了所需的所有东西用来来构建和静态链接 envoy，详参 ci/README.md 为了手动创建，遵循 bazel/README.md 的说明。 要求 Envoy 最初是在 Ubuntu 14 LTS 上开发和部署的。它应该适用于任何最近的 Linux，包括 Ubuntu 16 LTS。 构建 Envoy 有以下要求: GCC 5+ (对 C++14 支持)。 这些预先构建的第三方依赖。 这些 Bazel native 的依赖。 请阅读 CI 和 Bazel 的文档。获取有关执行手动构建的更多信息。 预构建的二进制文件 在每一个 master 提交中，我们创建了一组轻量级 Docker 镜像，其中包含 Envoy 的二进制文件。我们在发布官方版本时也会使用发布版本号来标记 docker 镜像。 envoyproxy/envoy: 发布在 Ubuntu Xenial 基础之上带有标记的二进制文件 envoyproxy/envoy-alpine: 发布带有在 glibc 基础之上标记的二进制文件 envoyproxy/envoy-alpine-debug: 发布带有在 glibc 基础之上 debug 标记的二进制文件 我们将考虑在帮助 CI、包等方面创建额外的二进制类型，如果需要的话，请在 GitHub 上打开一个 issue。 修改 Envoy 如果你对修改 Envoy 和测试你的改变感兴趣，那么一种方法就是使用 Docker。本指南将介绍构建您自己的 Envoy 二进制文件的过程，并将二进制文件放入一个 Ubuntu 容器中。 构建 Envoy Docker 镜像 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 10:34:31 "},"install/ref_configs.html":{"url":"install/ref_configs.html","title":"参考配置","keywords":"","body":"参考配置 源代码分发包里为以下三种 Envoy 的主要部署类型准备了一整套的配置案例模版： 服务间 前端代理 双向代理 这一套的配置案例模版主要用来展现 Envoy 在复杂部署下的全部能力。并不是所有的功能点都能适用于所有的应用场景。 详细文档可查看配置参考。 配置生成器 Envoy 的配置开始变得越发复杂。 在 Lyft 我们用 jinja 模版让生产以及管理配置的工作变得轻松一些。 源代码分发包里便包含了其中一个版本的配置生成器，这个版本的配置生成器非常接近我们在 Lyft 使用的版本。 我们同时为上述的三种场景都准备了相应的范例配置模版。 脚本生成器: configs/configgen.py 服务间模版: configs/envoy_service_to_service.template.json 前端代理模版: configs/envoy_front_proxy.template.json 双向代理模版: configs/envoy_double_proxy.template.json 可以从 repo 的根目录运行以下命令以生成相关的范例配置： mkdir -p generated/configs bazel build //configs:example_configs tar xvf $PWD/bazel-genfiles/configs/example_configs.tar -C generated/configs 上面的命令将生成三个完全可扩展配置，而配置中所使用到的其中某些变量被定义在 configgen.py 里。可以查看 configgen.py 里的注释以学习如何让不同的扩展工作。 关于范例配置，在此分享一些笔记： 一个假定运行在 discovery.yourcompany.net 上的服务发现实例。 yourcompany.net 的 DNS 做了许多配置。 可在配置模版中查找基于其实现的各种实例。 为 LightStep 而配置的追踪。 为了禁止或启用 Zipkin http://zipkin.io 追踪，而删除或改变相应的追踪配置 。 用于演示如何使用全局速率限制服务的示例配置。 可以通过删除速率限制配置以禁用此服务。 为服务间参考配置而设定的路由发现服务，此服务假定运行在 rds.yourcompany.net 上。 为服务间参考配置而设定的集群发现服务，此服务假定运行在 cds.yourcompany.net 上。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 14:11:29 "},"install/tools/config_load_check_tool.html":{"url":"install/tools/config_load_check_tool.html","title":"配置负载检查工具","keywords":"","body":"配置负载检查工具 配置负载检查工具检查 JSON 格式的配置文件是否使用有效的 JSON 编写，并符合 Envoy JSON 模式。 该工具利用test / config_test / config_test.cc中的配置测试。该测试加载 JSON 配置文件并使用它运行服务器配置初始化。 输入 该工具需要一个指向保存 JSON Envoy 配置文件的根目录的 PATH 变量。该工具将以递归方式遍历文件系统树，并对每个找到的文件运行配置测试。请记住，该工具将尝试加载路径中找到的所有文件。 输出 该工具使用它当前正在测试的配置初始化服务器配置时，将输出 Enovy 日志。 如果存在 JSON 文件格式错误或不符合Envoy JSON模式的配置文件，则该工具将以状态 EXIT_FAILURE 退出。 如果该工具成功加载找到的所有配置文件，它将以状态EXIT_SUCCESS退出。 构建 我们可以使用 Bazel 在本地构建该工具。 bazel build //test/tools/config_load_check:config_load_check_tool 运行 该工具将需要如上所述的 PATH 变量。 bazel-bin/test/tools/config_load_check/config_load_check_tool PATH Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-20 22:28:10 "},"install/tools/route_table_check_tool.html":{"url":"install/tools/route_table_check_tool.html","title":"路由表检查工具","keywords":"","body":"路由表检查工具 路由表检查工具检查路由返回的路由参数是否符合预期。该工具还可以用于检查路径重定向、路径重写或主机重写是否符合预期。 输入 该工具期望两个输入 JSON 文件：一个路由配置 JSON 文件。在一个工具配置 JSON 文件中找到了路由配置 JSON 文件架构。配置 JSON 文件模板的工具在 config 中。工具配置输入文件指定 URL （由权限和路径组成）以及期望的路由参数值。其他参数（如附加标头）是可选的。 输出 如果任何测试用例与预期的路由参数值不匹配，那么程序将以状态 EXIT_FAILURE 退出 。--detail 选项打印出每个测试的详细信息。第一行表示测试名称。如果测试失败，则打印失败的测试用例的详细信息。第一个字段是预期路由参数值。第二个字段是实际路由参数值。第三个字段表示比较的参数。在下面的例子中，test_2 和 test_5 失败了而其他测试通过了。在失败的测试用例中，会打印冲突细节。 Test_1 Test_2 default other virtual_host_name Test_3 Test_4 Test_5 locations ats cluster_name Test_6 目前不支持使用有效运行时值进行测试，这可能会在以后的工作中添加。 构建 工具可以在本地使用 Bazel 构建。bazel build //test/tools/router_check:router_check_tool 运行 该工具接受两个输入 json 文件和一个可选的命令行参数 --details 。 命令行参数的预期顺序是：1. 路由配置 json 文件;2. 工具配置 json 文件;3. 可选项 。 bazel-bin/test/tools/router_check/router_check_tool router_config.json tool_config.json bazel-bin/test/tools/router_check/router_check_tool router_config.json tool_config.json --details 测试 bash shell 脚本测试可以使用 bazel 运行。该测试比较了使用不同路由和工具配置 json 文件的路由。配置文件可以在 test/tools/router_check/test/config/… 找到。bazel test //test/tools/router_check/...。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-19 23:35:53 "},"install/tools/schema_validator_check_tool.html":{"url":"install/tools/schema_validator_check_tool.html","title":"Schema 验证器检查工具","keywords":"","body":"Schema 验证器检查工具 Schema 验证器工具验证被传入的 JSON 符合配置中的某个 schema。为验证整个配置，请参考配置负载检查工具。当前，仅路由配置 schema 验证被支持。 输入 工具期望两个输入：检查传入的 JSON 所用的 schema 类型。对于路由配置 验证被支持的类型是 :route。JSON 所在的路径。 输出 如果 JSON 符合 schema，工具将以状态 EXIT_SUCCESS 退出。如果 JSON 不符合 schema，会输出一条错误消息告知不符合 schema 的细节。工具将以 EXIT_FAILURE 状态退出。 构建 工具可以在本地使用 Bazel 构建。bazel build //test/tools/schema_validator:schema_validator_tool 运行 工具采用上面描述的一条路径。bazel-bin/test/tools/schema_validator/schema_validator_tool --schema-type SCHEMA_TYPE --json-path PATH Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 10:40:56 "},"configuration/overview/v1_overview.html":{"url":"configuration/overview/v1_overview.html","title":"v1 API 概览","keywords":"","body":"v1 API 概览 Attention The v1 configuration/API is now considered legacy and the deprecation schedule has been announced. Please upgrade and use the v2 configuration/API. The Envoy configuration format is written in JSON and is validated against a JSON schema. The schema can be found in source/common/json/config_schemas.cc. The main configuration for the server is contained within the listeners and cluster manager sections. The other top level elements specify miscellaneous configuration. YAML support is also provided as a syntactic convenience for hand-written configurations. Envoy will internally convert YAML to JSON if a file path ends with .yaml. In the rest of the configuration documentation, we refer exclusively to JSON. Envoy expects unambiguous YAML scalars, so if a cluster name (which should be a string) is called true, it should be written in the configuration YAML as “true”. The same applies to integer and floating point values (e.g. 1 vs. 1.0 vs. “1.0”). { \"listeners\": [], \"lds\": \"{...}\", \"admin\": \"{...}\", \"cluster_manager\": \"{...}\", \"flags_path\": \"...\", \"statsd_udp_ip_address\": \"...\", \"statsd_tcp_cluster_name\": \"...\", \"stats_flush_interval_ms\": \"...\", \"watchdog_miss_timeout_ms\": \"...\", \"watchdog_megamiss_timeout_ms\": \"...\", \"watchdog_kill_timeout_ms\": \"...\", \"watchdog_multikill_timeout_ms\": \"...\", \"tracing\": \"{...}\", \"rate_limit_service\": \"{...}\", \"runtime\": \"{...}\", } listeners (required, array) An array of listeners that will be instantiated by the server. A single Envoy process can contain any number of listeners. lds (optional, object) Configuration for the Listener Discovery Service (LDS). If not specified only static listeners are loaded. admin (required, object) Configuration for the local administration HTTP server. cluster_manager (required, object) Configuration for the cluster manager which owns all upstream clusters within the server. flags_path (optional, string) The file system path to search for startup flag files. statsd_udp_ip_address (optional, string) The UDP address of a running statsd compliant listener. If specified, statisticswill be flushed to this address. IPv4 addresses should have format host:port (ex: 127.0.0.1:855). IPv6 addresses should have URL format [host]:port (ex: [::1]:855). statsd_tcp_cluster_name (optional, string) The name of a cluster manager cluster that is running a TCP statsd compliant listener. If specified, Envoy will connect to this cluster to flush statistics. stats_flush_interval_ms (optional, integer) The time in milliseconds between flushes to configured stats sinks. For performance reasons Envoy latches counters and only flushes counters and gauges at a periodic interval. If not specified the default is 5000ms (5 seconds). watchdog_miss_timeout_ms (optional, integer) The time in milliseconds after which Envoy counts a nonresponsive thread in the “server.watchdog_miss” statistic. If not specified the default is 200ms. watchdog_megamiss_timeout_ms (optional, integer) The time in milliseconds after which Envoy counts a nonresponsive thread in the “server.watchdog_mega_miss” statistic. If not specified the default is 1000ms. watchdog_kill_timeout_ms (optional, integer) If a watched thread has been nonresponsive for this many milliseconds assume a programming error and kill the entire Envoy process. Set to 0 to disable kill behavior. If not specified the default is 0 (disabled). watchdog_multikill_timeout_ms (optional, integer) If at least two watched threads have been nonresponsive for at least this many milliseconds assume a true deadlock and kill the entire Envoy process. Set to 0 to disable this behavior. If not specified the default is 0 (disabled). tracing (optional, object) Configuration for an external tracing provider. If not specified, no tracing will be performed. rate_limit_service (optional, object) Configuration for an external rate limit service provider. If not specified, any calls to the rate limit service will immediately return success. runtime (optional, object) Configuration for the runtime configuration provider. If not specified, a “null” provider will be used which will result in all defaults being used. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 18:03:41 "},"configuration/overview/v2_overview.html":{"url":"configuration/overview/v2_overview.html","title":"v2 API 概览","keywords":"","body":"v2 API 概览 The Envoy v2 APIs are defined as proto3 Protocol Buffers in the data plane API repository. They evolve the existing v1 APIs and concepts to support: Streaming delivery of xDS API updates via gRPC. This reduces resource requirements and can lower the update latency. A new REST-JSON API in which the JSON/YAML formats are derived mechanically via the proto3 canonical JSON mapping. Delivery of updates via the filesystem, REST-JSON or gRPC endpoints. Advanced load balancing through an extended endpoint assignment API and load and resource utilization reporting to management servers. Stronger consistency and ordering properties when needed. The v2 APIs still maintain a baseline eventual consistency model. See the xDS protocol description for further details on aspects of v2 message exchange between Envoy and the management server. Bootstrap 配置 To use the v2 API, it’s necessary to supply a bootstrap configuration file. This provides static server configuration and configures Envoy to access dynamic configuration if needed. As with the v1 JSON/YAML configuration, this is supplied on the command-line via the -c flag, i.e.: ./envoy -c .{json,yaml,pb,pb_text} --v2-config-only where the extension reflects the underlying v2 config representation. The --v2-config-only flag is not strictly required as Envoy will attempt to autodetect the config file version, but this option provides an enhanced debug experience when configuration parsing fails. The Bootstrap message is the root of the configuration. A key concept in the Bootstrap message is the distinction between static and dynamic resouces. Resources such as a Listener or Cluster may be supplied either statically in static_resources or have an xDS service such as LDS or CDSconfigured in dynamic_resources. 示例 Below we will use YAML representation of the config protos and a running example of a service proxying HTTP from 127.0.0.1:10000 to 127.0.0.2:1234. 静态 A minimal fully static bootstrap config is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 127.0.0.1, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } http_filters: - name: envoy.router clusters: - name: some_service connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN hosts: [{ socket_address: { address: 127.0.0.2, port_value: 1234 }}] 除了动态 EDS 大部分静态 A bootstrap config that continues from the above example with dynamic endpoint discovery via anEDS gRPC management server listening on 127.0.0.3:5678 is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 127.0.0.1, port_value: 10000 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } http_filters: - name: envoy.router clusters: - name: some_service connect_timeout: 0.25s lb_policy: ROUND_ROBIN type: EDS eds_cluster_config: eds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] - name: xds_cluster connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN http2_protocol_options: {} hosts: [{ socket_address: { address: 127.0.0.3, port_value: 5678 }}] Notice above that xds_cluster is defined to point Envoy at the management server. Even in an otherwise completely dynamic configurations, some static resources need to be defined to point Envoy at its xDS management server(s). In the above example, the EDS management server could then return a proto encoding of a DiscoveryResponse: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.ClusterLoadAssignment cluster_name: some_service endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 127.0.0.2 port_value: 1234 The versioning and type URL scheme that appear above are explained in more detail in the streaming gRPC subscription protocol documentation. 动态 A fully dynamic bootstrap configuration, in which all resources other than those belonging to the management server are discovered via xDS is provided below: admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } dynamic_resources: lds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] cds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] static_resources: clusters: - name: xds_cluster connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN http2_protocol_options: {} hosts: [{ socket_address: { address: 127.0.0.3, port_value: 5678 }}] The management server could respond to LDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.Listener name: listener_0 address: socket_address: address: 127.0.0.1 port_value: 10000 filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http codec_type: AUTO rds: route_config_name: local_route config_source: api_config_source: api_type: GRPC cluster_names: [xds_cluster] http_filters: - name: envoy.router The management server could respond to RDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.RouteConfiguration name: local_route virtual_hosts: - name: local_service domains: [\"*\"] routes: - match: { prefix: \"/\" } route: { cluster: some_service } The management server could respond to CDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.Cluster name: some_service connect_timeout: 0.25s lb_policy: ROUND_ROBIN type: EDS eds_cluster_config: eds_config: api_config_source: api_type: GRPC cluster_names: [xds_cluster] The management server could respond to EDS requests with: version_info: \"0\" resources: - \"@type\": type.googleapis.com/envoy.api.v2.ClusterLoadAssignment cluster_name: some_service endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 127.0.0.2 port_value: 1234 管理服务器 A v2 xDS management server will implement the below endpoints as required for gRPC and/or REST serving. In both streaming gRPC and REST-JSON cases, a DiscoveryRequest is sent and aDiscoveryResponse received following the xDS protocol. gRPC streaming 端点 POST /envoy.api.v2.ClusterDiscoveryService/StreamClusters See cds.proto for the service definition. This is used by Envoy as a client when cds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /envoy.api.v2.EndpointDiscoveryService/StreamEndpoints See eds.proto for the service definition. This is used by Envoy as a client when eds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the eds_cluster_config field of the Cluster config. POST ``/envoy.api.v2.ListenerDiscoveryService/StreamListeners See lds.proto for the service definition. This is used by Envoy as a client when lds_config: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /envoy.api.v2.RouteDiscoveryService/StreamRoutes See rds.proto for the service definition. This is used by Envoy as a client when route_config_name: some_route_name config_source: api_config_source: api_type: GRPC cluster_names: [some_xds_cluster] is set in the rds field of the HttpConnectionManager config. REST 端点 POST /v2/discovery:clusters See cds.proto for the service definition. This is used by Envoy as a client when cds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /v2/discovery:endpoints See eds.proto for the service definition. This is used by Envoy as a client when eds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the eds_cluster_config field of the Cluster config. POST /v2/discovery:listeners See lds.proto for the service definition. This is used by Envoy as a client when lds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the dynamic_resources of the Bootstrap config. POST /v2/discovery:routes See rds.proto for the service definition. This is used by Envoy as a client when route_config_name: some_route_name config_source: api_config_source: api_type: REST cluster_names: [some_xds_cluster] is set in the rds field of the HttpConnectionManager config. 聚合发现服务 While Envoy fundamentally employs an eventual consistency model, ADS provides an opportunity to sequence API update pushes and ensure affinity of a single management server for an Envoy node for API updates. ADS allows one or more APIs and their resources to be delivered on a single, bidirectional gRPC stream by the management server. Without this, some APIs such as RDS and EDS may require the management of multiple streams and connections to distinct management servers. ADS will allow for hitless updates of configuration by appropriate sequencing. For example, suppose foo.com was mappped to cluster X. We wish to change the mapping in the route table to point foo.com at cluster Y. In order to do this, a CDS/EDS update must first be delivered containing both clusters X and Y. Without ADS, the CDS/EDS/RDS streams may point at distinct management servers, or when on the same management server at distinct gRPC streams/connections that require coordination. The EDS resource requests may be split across two distinct streams, one for X and one for Y. ADS allows these to be coalesced to a single stream to a single management server, avoiding the need for distributed synchronization to correctly sequence the update. With ADS, the management server would deliver the CDS, EDS and then RDS updates on a single stream. ADS is only available for gRPC streaming (not REST) and is described more fully in this document. The gRPC endpoint is: POST /envoy.api.v2.AggregatedDiscoveryService/StreamAggregatedResources See discovery.proto for the service definition. This is used by Envoy as a client when ads_config: api_type: GRPC cluster_names: [some_ads_cluster] is set in the dynamic_resources of the Bootstrap config. When this is set, any of the configuration sources above can be set to use the ADS channel. For example, a LDS config could be changed from lds_config: api_config_source: api_type: REST cluster_names: [some_xds_cluster] to lds_config: {ads: {}} with the effect that the LDS stream will be directed to some_ads_cluster over the shared ADS channel. 管理服务器不可达 When Envoy instance looses connectivity with the management server, Envoy will latch on to the previous configuration while actively retrying in the background to reestablish the connection with the management server. Envoy debug logs the fact that it is not able to establish a connection with the management server every time it attempts a connection. upstream_cx_connect_fail a cluster level statistic of the cluster pointing to management server provides a signal for monitoring this behavior. 状态 All features described in the v2 API reference are implemented unless otherwise noted. In the v2 API reference and the v2 API repository, all protos are frozen unless they are tagged as draft or experimental. Here, frozen means that we will not break wire format compatibility. Frozen protos may be further extended, e.g. by adding new fields, in a manner that does not break backwards compatibility. Fields in the above protos may be later deprecated, subject to thebreaking change policy, when their related functionality is no longer required. While frozen APIs have their wire format compatibility preserved, we reserve the right to change proto namespaces, file locations and nesting relationships, which may cause breaking code changes. We will aim to minimize the churn here. Protos tagged draft, meaning that they are near finalized, are likely to be at least partially implemented in Envoy but may have wire format breaking changes made prior to freezing. Protos tagged experimental, have the same caveats as draft protos and may have have major changes made prior to Envoy implementation and freezing. The current open v2 API issues are tracked here. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 18:02:43 "},"configuration/listeners/stats.html":{"url":"configuration/listeners/stats.html","title":"统计","keywords":"","body":"统计 监听器 每个监听器的统计树根在 listener.. ，有如下统计: 名称 类似 描述 downstream_cx_total Counter 连接总数 downstream_cx_destroy Counter 销毁连接总数 downstream_cx_active Gauge 活动连接总数 downstream_cx_length_ms Histogram 连接长度，单位毫秒 ssl.connection_error Counter TLS 连接错误总数，不包括证书认证失败 ssl.handshake Counter TLS连接握手成功总数 ssl.session_reused Counter TLS会话恢复成功总数 ssl.no_certificate Counter 不带客户端证书的TLS连接成功总数 ssl.fail_no_sni_match Counter 因为缺少SNI匹配而被拒绝的TLS连接总数 ssl.fail_verify_no_cert Counter 因为缺少客户端证书而失败的TLS连接总数 ssl.fail_verify_error Counter CA认证失败的TLS连接总数 ssl.fail_verify_san Counter SAN认证失败的TLS连接总数 ssl.fail_verify_cert_hash Counter 认证pinning认证失败的TLS连接总数 ssl.cipher. Counter 使用 的TLS连接总数 监听器管理器 监听器管理器的统计树根在 listener_manager. ，有以下统计。所有 stats 名称中的 : 字符被替换为 _。 名称 类型 描述 listener_added Counter 添加的监听器总数（不管是通过静态配置还是 LDS） listener_modified Counter 修改过的监听器总数（通过LDS） listener_removed Counter 删除过的监听器总数（通过LDS） listener_create_success Counter 成功添加到 workers 的监听器对象总数 listener_create_failure Counter 添加到 workers 失败的监听器对象总数 total_listeners_warming Gauge 当前热身中的监听器数量 total_listeners_active Gauge 当前活动中的监听器数量 total_listeners_draining Gauge 当前排除中的监听器数量 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 16:54:56 "},"configuration/listeners/runtime.html":{"url":"configuration/listeners/runtime.html","title":"运行时","keywords":"","body":"运行时 监听器支持下列运行时设置: ssl.alt_alpn 使用配置的 alt_alpn 协议字符串的请求百分比。默认为0。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 16:54:56 "},"configuration/listeners/lds.html":{"url":"configuration/listeners/lds.html","title":"监听器发现服务（LDS）","keywords":"","body":"监听器发现服务（LDS） 监听器发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取监听器。Envoy 将协调 API 响应，并根据需要添加，修改或删除已知的监听器。 监听器更新的语义如下： 每个监听器必须有一个独特的名字。如果没有提供名称，Envoy 将创建一个 UUID。要动态更新的监听器，管理服务必须提供监听器的唯一名称。 当一个监听器被添加，在参与连接处理之前，会先进入“预热”阶段。例如，如果监听器引用 RDS 配置，那么在监听器迁移到 “active” 之前，将会解析并提取该配置。 监听器一旦创建，实际上就会保持不变。因此，更新监听器时，会创建一个全新的监听器（使用相同的侦听套接字）。新增加的监听者都会通过上面所描述的相同“预热”过程。 当更新或删除监听器时，旧的监听器将被置于 “draining（逐出）” 状态，就像整个服务重新启动时一样。监听器移除之后，该监听器所拥有的连接，经过一段时间优雅地关闭（如果可能的话）剩余的连接。逐出时间通过 --drain-time-s 选项设置。 注意 任何在 Envoy 配置中静态定义的监听器都不能通过 LDS API 进行修改或删除。 配置 v1 LDS API v2 LDS API 统计 LDS 的统计树是以 listener_manager.lds 为根，统计如下： 名称 类型 描述 config_reload Counter 由于不同的配置更新，导致配置API调用总数 update_attempt Counter LDS配置API调用重试总数 update_success Counter LDS配置API调用成功总数 update_failure Counter LDS配置API调用失败总数（网络或模式错误） version Gauge 上次成功调用的内容哈希值 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 23:41:14 "},"configuration/listener_filters/original_dst_filter.html":{"url":"configuration/listener_filters/original_dst_filter.html","title":"原始目的地","keywords":"","body":"原始目的地 原始目的地监听器过滤器读取 SO_ORIGINAL_DST 套接字属性，这个属性在连接被重定向时设置。重定向可以通过iptables REDIRECT target，或者 iptables REDIRECT target，或者 iptables TPROXY target 实现，结合使用监听器的 transparent 属性设置. Envoy 中的后续处理将恢复后的目标地址视为连接的本地地址，而不是监听器正在监听的地址。此外， 原始目标集群 可用于将 HTTP 请求或 TCP 连接转发到恢复后的目标地址。 v2 API 参考 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:45:24 "},"configuration/listener_filters/tls_inspector.html":{"url":"configuration/listener_filters/tls_inspector.html","title":"TLS 检查器","keywords":"","body":"TLS 检查器 TLS 检查器监听器筛选器允许检测传输是 TLS 还是明文，如果是 TLS，它将检测来自客户端的 服务器名称指示。这可以用来通过 FilterChainMatch 的 sni_domains 来选择 过滤器链。 SNI v2 API 参考 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:50:17 "},"configuration/network_filters/client_ssl_auth_filter.html":{"url":"configuration/network_filters/client_ssl_auth_filter.html","title":"客户端 TLS 身份验证","keywords":"","body":"客户端 TLS 身份验证 客户端 TLS 认证过滤器架构概览 v1 API 参考 v2 API 参考 统计 每个配置的客户端 TLS 认证过滤器均有以 auth.clientssl.. 开头的统计信息，其统计信息如下所示： 名称 类型 描述 update_success Counter 主体更新成功总次数 update_failure Counter 主体更新失败总次数 auth_no_ssl Counter 无 TLS 而被忽略的连接总次数 auth_ip_white_list Counter 由于 IP 白名单而被允许的连接总次数 auth_digest_match Counter 由于证书匹配而被允许的连接总次数 auth_digest_no_match Counter 由于证书不匹配而被拒绝的连接总次数 total_principals Gauge 已加载主体总数 REST API GET /v1/certs/list/approved 认证过滤器将每隔一段刷新时间调用一次这个API，来获取当前获得批准的证书/主体列表。预期的 JSON 响应如下所示： { \"certificates\": [] } certificates (required, array) 为批准的证书/主体列表。 每个证书对象定义为： { \"fingerprint_sha256\": \"...\", } fingerprint_sha256 (required, string) 为批准的客户端证书的 SHA256 hash 值。Envoy 会将此 hash 与所提供的客户端证书进行匹配，以确定是否存在摘要匹配。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 19:41:36 "},"configuration/network_filters/echo_filter.html":{"url":"configuration/network_filters/echo_filter.html","title":"Echo","keywords":"","body":"回写 回写是一个简单的网络过滤器，主要用于演示网络级别过滤器 API。 安装此过滤器后，它会将所有接收到的数据回写（写入）回连接的下游客户端。 v1 API reference v2 API reference Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 16:37:04 "},"configuration/network_filters/mongo_proxy_filter.html":{"url":"configuration/network_filters/mongo_proxy_filter.html","title":"Mongo 代理","keywords":"","body":"Mongo 代理 MongoDB 架构概述 v1 API 参考 v2 API 参考 故障注入 Mongo 代理过滤器支持故障注入。可以查看 V1 以及 V2 的 API 参考文档以了解如何进行相关配置。 统计 每一个配置的 MongoDB 代理过滤器的统计信息都以 mongo.. 开头，其统计信息如下： 名称 类型 描述 decoding_error Counter MongoDB 协议解码错误的数量 delay_injected Counter 延迟被注入的次数 op_get_more Counter OP_GET_MORE 消息的数量 op_insert Counter OP_INSERT 消息的数量 op_kill_cursors Counter OP_KILL_CURSORS 消息的数量 op_query Counter OP_QUERY 消息的数量 op_query_tailable_cursor Counter 设置了 tailable cursor flag 的 OP_QUERY 消息的数量 op_query_no_cursor_timeout Counter 没有设置 cursor timeout flag 的 OP_QUERY 消息的数量 op_query_await_data Counter 设置了 await data flag 的 OP_QUERY 消息的数量 op_query_exhaust Counter 设置了 exhaust flag 的 OP_QUERY 消息的数量 op_query_no_max_time Counter 没有设置 maxTimeMS 的查询数量 op_query_scatter_get Counter 分散获取查询的数量 op_query_multi_get Counter 多重查询的次数 op_query_active Gauge 活跃查询的数量 op_reply Counter OP_REPLY 消息的数量 op_reply_cursor_not_found Counter 设置了 cursor not found flag 的 OP_REPLY 消息的数量 op_reply_query_failure Counter 设置了 query failure flag 的 OP_REPLY 消息的数量 op_reply_valid_cursor Counter 拥有有效的游标的 OP_REPLY 消息的数量 cx_destroy_local_with_active_rq Counter 拥有一个活跃查询并被本地破坏的连接总数 cx_destroy_remote_with_active_rq Counter 拥有一个活跃查询并被远程破坏的连接总数 cx_drain_close Counter 在服务器关闭期间，在回复边界被优雅关闭的连接总数 分散获取查询 任何不使用 _id 作为查询参数的查询，Envoy 将其定义为一个分散获取查询 。Envoy 同时在最顶级文档以及 _id 的 $query 中查找。 多重查询 任何使用 _id 作为查询参数的查询，且 _id 不是一个标量值（如文档或数组）， Envoy 定义其为 多重查询。 Envoy 同时在最顶级文档以及 _id 的 $query 中查找。 $comment 解析 如果一个查询具有顶级的 $comment 字段（通常在 $query 字段的基础上添加），Envoy 会将其解析为 JSON 格式并查找以下结构： { \"callingFunction\": \"...\" } callingFunction (required, string) 执行查询的函数。 在可用的情况下，这个函数将会用来做 按调用站 查询统计。 按命令统计 MongoDB 过滤器将在 mongo..cmd.. 命名空间为命令收集相应的统计信息。 名称 类型 描述 total Counter 命令数量 reply_num_docs Histogram 回复中的文档数量 reply_size Histogram 回复的字节大小 reply_time_ms Histogram 命令时间（毫秒） 按集合查询统计 MongoDB 过滤器将在 mongo..collection..query. 命名空间为查询收集相应的统计信息。 名称 类型 描述 total Counter 查询数量 scatter_get Counter 分散获取查询数量 multi_get Counter 多重查询梳理 reply_num_docs Histogram 回复中的文档数量 reply_size Histogram 回复的字节大小 reply_time_ms Histogram 查询时间（毫秒） 按集合与现场查询统计 如果应用程序在 $comment 字段中提供调用函数 ，Envoy 将相应生成按调用站点为维度的统计信息。 这些统计信息与按集合统计相匹配，可在 mongo..collection..callsite..query. 命名空间中找到相关信息。 运行时 Mongo 代理过滤器支持如下运行时设置： mongo.connection_logging_enabled 启用日志记录的连接百分比。 默认为100。 这将只允许将指定百分比的连接做日志记录, 但这些连接上的所有信息将会做日志记录。 mongo.proxy_enabled 启用代理的连接百分比。默认为100。 mongo.logging_enabled 启用日志记录的消息的百分比。 默认值为100。如果小于100，部分查询可能会在无回复的情况下被记录。 mongo.mongo.drain_close_enabled 当服务器正在被删除或尝试做强制关闭时，将被关闭的连接百分比。默认为100。 mongo.fault.fixed_delay.percent 当没有活跃故障时，一个合格的 MongoDB 操作受到注入故障影响的可能性。 默认为配置中指定的 percent 。 mongo.fault.fixed_delay.duration_ms 以毫秒为单位的延迟时间。默认为配置中指定的 duration_ms。 访问日志格式 访问日志格式不可定制，并具有以下布局： {\"time\": \"...\", \"message\": \"...\", \"upstream_host\": \"...\"} time 解析完整信息所需的系统时间，精确度至毫秒。 message 文本扩展的消息。 消息是否完全可扩展取决于上下文。 为避免日志超大，有时会提供摘要数据。 upstream_host 连接正在被代理的上游主机, 在过滤器配合 TCP 代理过滤器时，此字段将被填充。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 19:29:44 "},"configuration/network_filters/rate_limit_filter.html":{"url":"configuration/network_filters/rate_limit_filter.html","title":"速率限制","keywords":"","body":"速率限制 全局速率限制架构概述 v1 API 参考 v2 API 参考 统计 所有配置的的速率限制过滤器都有以 ratelimit.. 开头的统计，以提供以下的统计报告： 名称 类型 描述 total Counter 所有发给速率限制服务的请求数 error Counter 所有联系速率限制服务的错误数 over_limit Counter 所有由速率限制服务回复的超过速率限制的响应数 ok Counter 所有由速率限制服务回复的未超过速率限制的响应数 cx_closed Counter 所有因超过速率限制相应而被关闭的连接数 active Gauge 所有发给速率限制服务的活跃请求数 运行时 网络级别速率限制过滤器支持以下运行时配置： ratelimit.tcp_filter_enabled 将调用速率限制服务的连接百分比。缺省值为100。 ratelimit.tcp_filter_enforcing 将调用速率限制服务并强制执行决定的连接百分比。缺省值为100。这可以在完全执行结果之前，测试将会发生什么。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:53:20 "},"configuration/network_filters/redis_proxy_filter.html":{"url":"configuration/network_filters/redis_proxy_filter.html","title":"Redis 代理","keywords":"","body":"Redis 代理 Redis 架构概述 v1 接口文档 v2 接口文档 统计 每个已配置的 Redis 代理过滤器都有以 redis.. 开头的统计，并提供如下的统计报告： Name Type Description downstream_cx_active Gauge 活跃的连接总数 downstream_cx_protocol_error Counter 协议错误总次数 downstream_cx_rx_bytes_buffered Gauge 当前接收并缓存的总字节数 downstream_cx_rx_bytes_total Counter 接收的总字节数 downstream_cx_total Counter 连接总数 downstream_cx_tx_bytes_buffered Gauge 当前发送并缓存的总字节数 downstream_cx_tx_bytes_total Counter 发送的总字节数 downstream_cx_drain_close Counter 因连接耗尽而关闭的连接总数 downstream_rq_active Gauge 活跃的请求总数 downstream_rq_total Counter 请求总数 分离器统计 Redis 过滤器将采集命令分离器的统计信息，并存放到以 redis..splitter 开头的统计中，提供如下的统计报告： Name Type Description invalid_request Counter 参数个数不正确的请求数 unsupported_command Counter 命令分离器无法识别的命令数 每个命令的统计 Redis 过滤器将收集每个命令的统计信息，并存放到以 redis..command. 开头的命名空间中，提供如下的统计报告： Name Type Description total Counter 命令数 运行时 Redis 代理过滤器支持如下的运行时设置： redis.drain_close_enabled 当服务器因为连接耗尽而尝试关闭连接的百分比。 默认值是 100。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 12:10:34 "},"configuration/network_filters/tcp_proxy_filter.html":{"url":"configuration/network_filters/tcp_proxy_filter.html","title":"TCP 代理","keywords":"","body":"TCP 代理 TCP 代理 架构概述 v1 接口文档 v2 接口文档 统计 在适当的情况下，TCP 代理会发出自己下游以及上游集群的统计信息。下游的统计信息都有以 tcp. 开头的统计，并提供如下的统计报告： 名称 类型 描述 downstream_cx_total Counter 过滤器处理的连接总数 downstream_cx_no_route Counter 没有找到匹配路由或没有找到路由集群的连接总数 downstream_cx_tx_bytes_total Counter 写入下游连接的总字节数 downstream_cx_tx_bytes_buffered Gauge 当前缓存到下游连接的总字节数 downstream_cx_rx_bytes_total Counter 从下游连接读取的总字节数 downstream_cx_rx_bytes_buffered Gauge 当前从下游连接缓存的总字节数 downstream_flow_control_paused_reading_total Counter 流量控制从下游暂停读取的总次数 downstream_flow_control_resumed_reading_total Counter 流量控制从下游恢复读取的总次数 idle_timeout Counter 由于空闲连接超时而关闭的连接总数 upstream_flush_total Counter 在下游连接关闭后继续刷新上游数据的连接总数 upstream_flush_active Gauge 在下游连接关闭后，当前继续刷新上游数据的连接总数 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 14:30:54 "},"configuration/http_conn_man/http_conn_man.html":{"url":"configuration/http_conn_man/http_conn_man.html","title":"HTTP 连接管理器","keywords":"","body":"HTTP 连接管理器 HTTP 链接管理器架构概览 HTTP 协议架构概览 v1 API 参考 v2 API 参考 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 12:32:20 "},"configuration/http_conn_man/route_matching.html":{"url":"configuration/http_conn_man/route_matching.html","title":"路由匹配","keywords":"","body":"路由匹配 注意 本节为 v1 API编写，但概念也适用于 v2 API。 在未来的版本中将以 v2 API为目标重写为。 当 Envoy 匹配路由时，它使用如下步骤： HTTP 请求的 host 或者 :authority 头匹配到 虚拟主机。 按顺序检查虚拟主机中的每个 路由条目 。如果匹配，则使用该路由而不再进一步检查其他路由。 独立地，按顺序检查虚拟主机中的每个 虚拟集群 。如果匹配，则使用该虚拟集群而不再进一步检查其他虚拟集群。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 10:07:56 "},"configuration/http_conn_man/traffic_splitting.html":{"url":"configuration/http_conn_man/traffic_splitting.html","title":"流量转移/拆分","keywords":"","body":"流量转移拆分 注意 本节为 v1 API 编写，但概念也适用于 v2 API。 在未来的版本中将以 v2 API 为目标重写为。 在两个上游之间转移流量 跨多个上游拆分流量 Envoy 的路由器可以跨两个或更多上游集群地将流量拆分到虚拟主机中的路由。有两个常见的用例。 版本升级：路由的流量逐渐从一个集群优雅地转移到另一个集群。 流量转移部分更详细地描述了这个场景。 A/B 测试或多变量测试：同时测试两个或更多版本的相同服务。流向路由的流量必须在运行同一服务的不同版本的集群之间进行拆分。 流量拆分部分更详细地描述了这种情况。 在两个上游之间转移流量 路由配置中的运行时对象判断选择特定路由（以及它的集群）的可能性（译者注：可以理解为百分比）。通过使用运行时配置，虚拟主机中到特定路由的流量可逐渐从一个集群转移到另一个集群。考虑以下示例配置，其中在 envoy 配置文件中声明了名为 helloworld 的服务的两个版本 helloworld_v1 和 helloworld_v2。 { \"route_config\": { \"virtual_hosts\": [ { \"name\": \"helloworld\", \"domains\": [\"*\"], \"routes\": [ { \"prefix\": \"/\", \"cluster\": \"helloworld_v1\", \"runtime\": { \"key\": \"routing.traffic_shift.helloworld\", \"default\": 50 } }, { \"prefix\": \"/\", \"cluster\": \"helloworld_v2\", } ] } ] } } Envoy 使用 first match 策略来匹配路由。如果路由具有运行时对象，则会根据运行时值另外匹配请求（如果未指定值，则为默认值）。因此，通过在上述示例中背靠背地放置路由并在第一个路由中指定运行时对象，可以通过更改运行时值来完成流量转移。以下是完成任务所需的大致操作顺序。 在开始时，将 routing.traffic_shift.helloworld 设置为 100, 因此所有到 helloworld 虚拟主机的请求都将匹配 v1 路由并由 helloworld_v1 集群提供服务。 为了开始将流量转移到 helloworld_v2 集群, 设置 routing.traffic_shift.helloworld 为 0 . 例如设置为 90 时，有1个不会与 v1 路由匹配，然后会落入 v2 路由。 逐渐减少 routing.traffic_shift.helloworld 中设置的值，以便大部分请求与 v2 路由匹配。 当 routing.traffic_shift.helloworld 设置为 0 时, 到 helloworld 虚拟主机的请求都不会匹配 v1 路由。现在，所有流量都会流向 v2 路由，并由 helloworld_v2 集群提供服务。 跨多个上游拆分流量 再次考虑 helloworld 示例，现在有三个版本（v1、v2和v3）而不是两个。要在三个版本间平均分配流量（即33％、33％、34％），可以使用 weighted_clusters选项指定每个上游集群的权重。 与前面的例子不同，单个路由条目就足够了。路由中的 weighted_clusters 配置块可用于指定多个上游集群以及权重，权证则表示要发送到每个上游集群的流量百分比。 { \"route_config\": { \"virtual_hosts\": [ { \"name\": \"helloworld\", \"domains\": [\"*\"], \"routes\": [ { \"prefix\": \"/\", \"weighted_clusters\": { \"runtime_key_prefix\" : \"routing.traffic_split.helloworld\", \"clusters\" : [ { \"name\" : \"helloworld_v1\", \"weight\" : 33 }, { \"name\" : \"helloworld_v2\", \"weight\" : 33 }, { \"name\" : \"helloworld_v3\", \"weight\" : 34 } ] } } ] } ] } } 默认情况下，权重必须总和精确为100。在 V2 API 中，总权重默认为100，但可以修改以允许更精细的粒度。 可以使用以下运行时变量动态调整分配给每个集群的权重：routing.traffic_split.helloworld.helloworld_v1，routing.traffic_split.helloworld.helloworld_v2 和 routing.traffic_split.helloworld.helloworld_v3。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 10:37:44 "},"configuration/http_conn_man/headers.html":{"url":"configuration/http_conn_man/headers.html","title":"HTTP header 操作","keywords":"","body":"HTTP header manipulation The HTTP connection manager manipulates several HTTP headers both during decoding (when the request is being received) as well as during encoding (when the response is being sent). user-agent server x-client-trace-id x-envoy-downstream-service-cluster x-envoy-downstream-service-node x-envoy-external-address x-envoy-force-trace x-envoy-internal x-forwarded-client-cert x-forwarded-for x-forwarded-proto x-request-id x-ot-span-context x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags Custom request/response headers See https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:20:12 "},"configuration/http_conn_man/header_sanitizing.html":{"url":"configuration/http_conn_man/header_sanitizing.html","title":"HTTP header sanitizing","keywords":"","body":"HTTP 标头清理 出于安全原因考虑，Envoy 将根据请求是内部请求还是外部请求来 “清理” 各种传入的 HTTP 标头。 清理行为取决于标头，并可能会导致添加、删除或更改。最终，一个请求被认定为内部或是外部请求都是由 x-forwarded-for 标头决定（请仔细阅读链接部分，因为 Envoy 填充标头的动作是非常复杂的，并取决于 use_remote_address 设置）。 Envoy 可能会清理以下标头: x-envoy-decorator-operation x-envoy-downstream-service-cluster x-envoy-downstream-service-node x-envoy-expected-rq-timeout-ms x-envoy-external-address x-envoy-force-trace x-envoy-internal x-envoy-ip-tags x-envoy-max-retries x-envoy-retry-grpc-on x-envoy-retry-on x-envoy-upstream-alt-stat-name x-envoy-upstream-rq-per-try-timeout-ms x-envoy-upstream-rq-timeout-alt-response x-envoy-upstream-rq-timeout-ms x-forwarded-client-cert x-forwarded-for x-forwarded-proto x-request-id Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 15:55:23 "},"configuration/http_conn_man/stats.html":{"url":"configuration/http_conn_man/stats.html","title":"统计","keywords":"","body":"统计 每个连接管理器都有一个以 http.. 为根的统计树，其统计信息如下： 名称 类型 描述 downstream_cx_total Counter 连接总数 downstream_cx_ssl_total Counter TLS 连接总数 downstream_cx_http1_total Counter HTTP/1.1 连接总数 downstream_cx_websocket_total Counter WebSocket 连接总数 downstream_cx_http2_total Counter HTTP/2 连接总数 downstream_cx_destroy Counter 被破坏的连接总数 downstream_cx_destroy_remote Counter 由于远程关闭，而导致被破坏的连接总数 downstream_cx_destroy_local Counter 由于本地关闭，而导致被破坏的连接总数 downstream_cx_destroy_active_rq Counter 由于超过一个活跃请求，而导致被破坏的连接总数 downstream_cx_destroy_local_active_rq Counter 由于超过一个活跃请求，而导致被本地破坏的连接总数 downstream_cx_destroy_remote_active_rq Counter 由于超过一个活跃请求，而导致被远程破坏的连接总数 downstream_cx_active Gauge 活跃连接总数 downstream_cx_ssl_active Gauge 活跃 TLS 连接总数 downstream_cx_http1_active Gauge 活跃 HTTP/1.1 连接总数 downstream_cx_websocket_active Gauge 活跃 WebSocket 连接总数 downstream_cx_http2_active Gauge 活跃 HTTP/2 连接总数 downstream_cx_protocol_error Counter 协议错误总数 downstream_cx_length_ms Histogram 连接长度毫秒 downstream_cx_rx_bytes_total Counter 收到的总字节数 downstream_cx_rx_bytes_buffered Gauge 当前收到并缓存的总字节数 downstream_cx_tx_bytes_total Counter 发出的总字节数 downstream_cx_tx_bytes_buffered Gauge 当前发出并缓存的总字节数 downstream_cx_drain_close Counter 由于删除，而导致被关闭的连接总数 downstream_cx_idle_timeout Counter 由于空闲超时，而导致被关闭的连接总数 downstream_flow_control_paused_reading_total Counter 由于流量控制，而导致被禁止的总读取次数 downstream_flow_control_resumed_reading_total Counter 由于流量控制，而导致在连接上启用的总读取次数 downstream_rq_total Counter 请求总数 downstream_rq_http1_total Counter HTTP/1.1 总请求总数 downstream_rq_http2_total Counter HTTP/2 请求总数 downstream_rq_active Gauge 活跃请求总数 downstream_rq_response_before_rq_complete Counter 在请求完成之前发送的总响应数 downstream_rq_rx_reset Counter 收到的请求重置总数 downstream_rq_tx_reset Counter 发出的请求重置总数 downstream_rq_non_relative_path Counter 带有非相对 HTTP 路径的请求总数 downstream_rq_too_large Counter 由于缓存过大正文，而导致 413 响应的请求总数 downstream_rq_1xx Counter 1xx 响应总数 downstream_rq_2xx Counter 2xx 响应总数 downstream_rq_3xx Counter 3xx 响应总数 downstream_rq_4xx Counter 4xx 响应总数 downstream_rq_5xx Counter 5xx 响应总数 downstream_rq_ws_on_non_ws_route Counter 由于非 WebSocket 路由而被拒绝的 WebSocket 升级请求总数 downstream_rq_time Histogram 请求时间(毫秒) rs_too_large Counter 由于缓存过大正文，而导致的错误响应总数 以 user agent 维度进行统计 以 user agent 维度进行的统计信息都以 http..user_agent.. 开头。 目前 Envoy 匹配 iOS (ios) 以及 Android (android) 的 user agent ，并产生以下统计信息： 名称 类型 描述 downstream_cx_total Counter 连接总数 downstream_cx_destroy_remote_active_rq Counter 由于超过一个活跃请求，而导致被远程破坏的连接总数 downstream_rq_total Counter 请求总数 以监听器维度进行统计 以监听器维度进行的统计信息都以 listener..http.. 开头，并有以下统计信息： 名称 类型 描述 downstream_rq_1xx Counter 1xx 响应总数 downstream_rq_2xx Counter 2xx 响应总数 downstream_rq_3xx Counter 3xx 响应总数 downstream_rq_4xx Counter 4xx 响应总数 downstream_rq_5xx Counter 5xx 响应总数 以编解码器维度进行统计 每一个编解码器都有进行按编解码器维度进行统计的能力。目前只有 http2 有编解码器统计信息。 Http2 编解码器统计 所有的 http2 统计信息都以 http2. 开头 名称 类型 描述 header_overflow Counter 由于头部大于 Envoy :: Http :: Http2 :: ConnectionImpl :: StreamImpl :: MAX_HEADER_SIZE（63k）而重置的连接总数 headers_cb_no_stream Counter 在没有关联流的情况下进行头部回调的错误总数。 由于尚未确诊的 bug，这会导致一些意外事件的发送 rx_messaging_error Counter 因为违背 HTTP/2 spec 中的 section 8 而导致的无效接受帧总数。 这将导致 tx_reset rx_reset Counter Envoy 收到的重置流帧的总数 too_many_header_frames Counter 由于接收太多头帧而导致 HTTP2 连接重置的总次数。 Envoy 支持代理最多一个 100-Continue 头部帧 ,一个 non-100 响应代码头部帧和一个拥有尾部的帧 trailers Counter 由下游请求所看到的尾部总数 tx_reset Counter Envoy 发送的重置流帧总数 追踪统计 追踪统计信息是在做出追踪决定时发出的。所有追踪统计信息都以 http..tracing. 开头，并带有以下统计信息： 名称 类型 描述 random_sampling Counter 通过随机抽样可追踪决策的总数 service_forced Counter 通过服务器运行时标识 tracing.global_enabled 的可追踪决策的总数 client_enabled Counter 通过请求头部 x-envoy-force-trace 设定的可追踪决策的总数 not_traceable Counter 通过 request id 的不可追踪的决策总数 health_check Counter 通过健康检查的不可追踪的决策总数 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 14:55:32 "},"configuration/http_conn_man/runtime.html":{"url":"configuration/http_conn_man/runtime.html","title":"运行时","keywords":"","body":"运行时 HTTP 连接管理支持以下运行时设定: http_connection_manager.represent_ipv4_remote_address_as_ipv4_mapped_ipv6 将其 IPv4 地址映射到 IPv6 的远程地址的请求的百分比。默认为0。 需要将 use_remote_address 开启。 详情可参看 represent_ipv4_remote_address_as_ipv4_mapped_ipv6。 tracing.client_enabled 如设置 x-client-trace-id 头部，将被强行追踪的请求的百分比。默认为100。 tracing.global_enabled 在应用所有其他检查后追踪的请求的百分比（强制追踪，采样等）。默认为100。 tracing.random_sampling 将被随机跟踪的请求的百分比。 请查阅此处以获得更多信息。该运行时间控制在0-10000范围内指定，默认值为10000。 因此，可以按0.01％的增量指定跟踪采样。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:50:55 "},"configuration/http_conn_man/rds.html":{"url":"configuration/http_conn_man/rds.html","title":"路由发现服务（RDS）","keywords":"","body":"路由发现服务（RDS） 路由发现服务（RDS）的API在 Envoy 里面是一个可选 API，用于动态获取路由配置。路由配置包括 HTTP 头部修改，虚拟主机以及每个虚拟主机中包含的单个路由规则。每个 HTTP连接管理器 都可以通过 API 独立地获取自身的路由配置。 v1 API reference v2 API reference 统计 RDS 的统计树以 http..rds..*.为根，route_config_name名称中的任何:字符在统计树中被替换为_。统计树包含以下统计信息： 名称 类型 描述 config_reload 计数器 加载配置不同导致重新调用API的总次数 update_attempt 计数器 调用API获取资源重试总数 update_success 计数器 调用API获取资源成功总数 update_failure 计数器 调用API获取资源失败总数（因网络、句法错误） version 测量 最后一次API获取资源成功的内容HASH Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 11:02:26 "},"configuration/http_filters/buffer_filter.html":{"url":"configuration/http_filters/buffer_filter.html","title":"Buffer","keywords":"","body":"缓冲区 缓冲区过滤器用于停止过滤器迭代并等待完全被缓冲的完整请求。 这可以在不同场景下发挥作用，包括确保应用程序不必去处理不完整请求以及高网络延迟。 v1 API 参考 v2 API 参考 单路由配置 通过在虚拟主机、路由或加权集群上提供 BufferPerRoute 配置， 可达到在单路由的基础上重写或禁用缓冲区过滤器。 统计 缓冲区过滤器在 http..buffer. 命名空间输出统计信息。 统计信息前缀 来自所拥有的 HTTP 连接管理器。 名称 类型 描述 rq_timeout Counter 因超时等待完整请求的总请求数 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 14:30:54 "},"configuration/http_filters/cors_filter.html":{"url":"configuration/http_filters/cors_filter.html","title":"CORS","keywords":"","body":"CORS 这是一个基于路由或虚拟主机设置处理跨源资源共享请求的过滤器。请参阅下面的页面以了解更多信息。 https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS https://www.w3.org/TR/cors/ v1 API 参考 v2 API 参考 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 14:37:49 "},"configuration/http_filters/dynamodb_filter.html":{"url":"configuration/http_filters/dynamodb_filter.html","title":"DynamoDB","keywords":"","body":"DynamoDB DynamoDB 架构概述 v1 API 参考 v2 API 参考 统计 DynamoDB 过滤器在 http..dynamodb. 命名空间输出统计信息。统计前缀 来自所拥有的 HTTP 连接管理器。 可以在 http..dynamodb.operation.. 命名空间找到按操作为维度的统计信息。 名称 类型 描述 upstream_rq_total Counter 以 命名的请求总数 upstream_rq_time Histogram 在 上耗费的时间 upstream_rq_total_xxx Counter 以 命名的请求总数，以响应代码为维度统计(503、2xx等) upstream_rq_time_xxx Histogram 在 上耗费的时间，以响应代码为维度统计(400、3xx等) 可以在 http..dynamodb.table.. 命名空间找到按数据库表为维度的统计信息。 大多数的 DynamoDB 操作仅仅涉及一个数据库表，而 BatchGetItem 以及 BatchWriteItem 会包括多个数据库表。 只有在该批次的所有操作中使用的数据库表都是相同时，Envoy 才会跟踪每个数据库表的统计数据。 名称 类型 描述 upstream_rq_total Counter 对数据库表 的请求总数 upstream_rq_time Histogram 在数据库表 上耗费的时间 upstream_rq_total_xxx Counter 对数据库表 的请求总数，以响应代码为维度统计(503、2xx等) upstream_rq_time_xxx Histogram 在数据库表 上耗费的时间，以响应代码为维度统计(400、3xx等) 免责声明：请注意，这是尚未广泛使用的预发布 Amazon DynamoDB 功能。 按每个分区及操作为维度的统计信息可以在 http..dynamodb.table.. 命名空间中找到。 对于批量操作，只有在该批次的所有操作中使用的数据库表都是相同时，只有在该批次的所有操作中使用的数据库表都是相同时，Envoy 才会按分区及操作追踪统计信息。 名称 类型 描述 capacity..__partition_id= Counter 指定分区 上的 数据库表 上的操作 的总容量 其他详细统计信息: 对于4xx响应和不完整的批处理操作失败，将在 http..dynamodb.error.. 命名空间内对指定数据库表以及故障的失败总数进行追踪。 名称 类型 描述 Counter 对指定数据库表 发生故障 的总次数 BatchFailureUnprocessedKeys Counter 对指定数据库表 发生不完整的批处理操作失败的次数 运行时 DynamoDB 过滤器支持以下运行时设置: dynamodb.filter_enabled 启用过滤器的请求的百分比。 默认值是100％。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 15:44:51 "},"configuration/http_filters/fault_filter.html":{"url":"configuration/http_filters/fault_filter.html","title":"故障注入","keywords":"","body":"故障注入 The fault injection filter can be used to test the resiliency of microservices to different forms of failures. The filter can be used to inject delays and abort requests with user-specified error codes, thereby providing the ability to stage different failure scenarios such as service failures, service overloads, high network latency, network partitions, etc. Faults injection can be limited to a specific set of requests based on the (destination) upstream cluster of a request and/or a set of pre-defined request headers. The scope of failures is restricted to those that are observable by an application communicating over the network. CPU and disk failures on the local host cannot be emulated. Currently, the fault injection filter has the following limitations: Abort codes are restricted to HTTP status codes only Delays are restricted to fixed duration. Future versions will include support for restricting faults to specific routes, injecting gRPC and HTTP/2 specific error codes and delay durations based on distributions. Configuration Note The fault injection filter must be inserted before any other filter, including the router filter. v1 API reference v2 API reference Runtime The HTTP fault injection filter supports the following global runtime settings: fault.http.abort.abort_percent % of requests that will be aborted if the headers match. Defaults to the abort_percent specified in config. If the config does not contain an abort block, then abort_percent defaults to 0. fault.http.abort.http_status HTTP status code that will be used as the of requests that will be aborted if the headers match. Defaults to the HTTP status code specified in the config. If the config does not contain an abortblock, then http_status defaults to 0. fault.http.delay.fixed_delay_percent % of requests that will be delayed if the headers match. Defaults to the delay_percent specified in the config or 0 otherwise. fault.http.delay.fixed_duration_ms The delay duration in milliseconds. If not specified, the fixed_duration_ms specified in the config will be used. If this field is missing from both the runtime and the config, no delays will be injected. Note, fault filter runtime settings for the specific downstream cluster override the default ones if present. The following are downstream specific runtime keys: fault.http..abort.abort_percent fault.http..abort.http_status fault.http..delay.fixed_delay_percent fault.http..delay.fixed_duration_ms Downstream cluster name is taken from the HTTP x-envoy-downstream-service-cluster header. If the following settings are not found in the runtime it defaults to the global runtime settings which defaults to the config settings. Statistics The fault filter outputs statistics in the http..fault. namespace. The stat prefix comes from the owning HTTP connection manager. Name Type Description delays_injected Counter Total requests that were delayed aborts_injected Counter Total requests that were aborted .delays_injected Counter Total delayed requests for the given downstream cluster .aborts_injected Counter Total aborted requests for the given downstream cluster Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:25:15 "},"configuration/http_filters/grpc_http1_bridge_filter.html":{"url":"configuration/http_filters/grpc_http1_bridge_filter.html","title":"gRPC HTTP/1.1 bridge","keywords":"","body":"gRPC HTTP/1.1 bridge gRPC architecture overview v1 API reference v2 API reference This is a simple filter which enables the bridging of an HTTP/1.1 client which does not support response trailers to a compliant gRPC server. It works by doing the following: When a request is sent, the filter sees if the connection is HTTP/1.1 and the request content type is application/grpc. If so, when the response is received, the filter buffers it and waits for trailers and then checks the grpc-status code. If it is not zero, the filter switches the HTTP response code to 503. It also copies the grpc-status and grpc-message trailers into the response headers so that the client can look at them if it wishes. The client should send HTTP/1.1 requests that translate to the following pseudo headers: :method: POST :path: content-type: application/grpc The body should be the serialized grpc body which is: 1 byte of zero (not compressed). network order 4 bytes of proto message length. serialized proto message. Because this scheme must buffer the response to look for the grpc-status trailer it will only work with unary gRPC APIs. This filter also collects stats for all gRPC requests that transit, even if those requests are normal gRPC requests over HTTP/2. More info: wire format in gRPC over HTTP/2. Statistics The filter emits statistics in the cluster..grpc. namespace. Name Type Description ..success Counter Total successful service/method calls ..failure Counter Total failed service/method calls ..total Counter Total service/method calls Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:26:59 "},"configuration/http_filters/grpc_json_transcoder_filter.html":{"url":"configuration/http_filters/grpc_json_transcoder_filter.html","title":"gRPC-JSON 转码","keywords":"","body":"gRPC-JSON 转码 gRPC architecture overview v1 API reference v2 API reference This is a filter which allows a RESTful JSON API client to send requests to Envoy over HTTP and get proxied to a gRPC service. The HTTP mapping for the gRPC service has to be defined by custom options. How to generate proto descriptor set Envoy has to know the proto descriptor of your gRPC service in order to do the transcoding. To generate a protobuf descriptor set for the gRPC service, you’ll also need to clone the googleapis repository from GitHub before running protoc, as you’ll need annotations.proto in your include path, to define the HTTP mapping. git clone https://github.com/googleapis/googleapis GOOGLEAPIS_DIR= Then run protoc to generate the descriptor set from bookstore.proto: protoc -I$(GOOGLEAPIS_DIR) -I. --include_imports --include_source_info \\ --descriptor_set_out=proto.pb test/proto/bookstore.proto If you have more than one proto source files, you can pass all of them in one command. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:28:01 "},"configuration/http_filters/grpc_web_filter.html":{"url":"configuration/http_filters/grpc_web_filter.html","title":"gRPC-Web","keywords":"","body":"gRPC-Web gRPC 架构概述 v1 API reference v2 API reference 这个过滤器通过以下步骤可以将 gRPC-Web 客户端桥接到兼容的 gRPC 服务 https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-WEB.md 。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 15:49:08 "},"configuration/http_filters/gzip_filter.html":{"url":"configuration/http_filters/gzip_filter.html","title":"Gzip","keywords":"","body":"Gzip Gzip 是一个 http 过滤器。该过滤器允许 Envoy 压缩从上游服务基于客户端请求分发的数据。在大量数据需要传输而响应时间被限定的情况下，压缩操作是有用的。 Configuration v2 API reference 注意 window bits 是一个数字。这个数字告诉压缩器，算法应该在文本之前多大范围来寻找重复的字符序列。由于在zlib库里的一个已知bug，window bits 的值如果是 8，则这个值不能按照预期运行。因此任何小于 8 的值，应该被自动置为 9。这个问题在库的未来版本将被解决 它是如何工作的 当 gzip 过滤器被置为 enabled 状态，请求和响应头都被检测，从而决定是否内容需要被压缩。如果请求或响应允许的话，内容被压缩并随后带着相应的http头发送给客户端 在默认情况下，压缩操作被忽略。这些情况如下所列： 请求不包含头 accept-encoding 请求包含头 accept-encoding，但是不包含 “gzip” 响应包含头 content-encoding 响应包含头 cache-control，该头的值包含 \"no-transform\" 响应包含头 transfer-encoding，该头的值包含 \"gzip\" 响应的 content-type 值，不和如下的 mime-type 值相匹配，这些 mime-type 值包含：application/javascript, application/json, application/xhtml+xml, image/svg+xml, text/css, text/html, text/plain, text/xml 响应中及不包含 content-type 头，也不包含 transfer-encoding 头 响应长度小于 30 个字节（仅应用于 transfer-encoding 没有被分成大块） 在以下情况，压缩操作被使用： content-length 被从响应头中移除 响应头包含 “transfer-encoding: chunked” 以及 “content-encoding: gzip” 头 “vary: accept-encoding” 被插入到了每一个响应中 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 21:44:58 "},"configuration/http_filters/health_check_filter.html":{"url":"configuration/http_filters/health_check_filter.html","title":"健康检查","keywords":"","body":"健康检查 健康检查过滤器架构概述 v1 API 参考 v2 API 参考 注意 请注意如果调用了 /healthcheck/fail 管理端点， 过滤器将会自动地在健康检查中失败并标识 x-envoy-immediate-health-check-fail 标头。 (/healthcheck/ok 管理端点可以反转此行为)。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 16:28:00 "},"configuration/http_filters/ip_tagging_filter.html":{"url":"configuration/http_filters/ip_tagging_filter.html","title":"IP Tagging","keywords":"","body":"IP 标签 HTTP IP 标签过滤器使用来自可信地址的 x-forwarded-for 的值来设置标签头 x-envoy-ip-tags。如果没有，则不设置。 IP 标签的实施提供了一种可扩展的方式来高效地将 IP 地址与大量的 CIDR 列表进行范围比较。用于存储标签和 IP 地址子网的基础算法在 S.Nilsson 和 G.Karlsson 的 IP-address lookup using LC-tries 论文中阐述。 配置 v2 API reference 统计 IP标签过滤器会在命名空间 http.stat_prefix.ip_tagging. 中输出统计信息。stat_prefix 来自对应的 HTTP 链接管理器。 名称 类型 描述 tag_name.hit Counter 已应用 tag_name 的请求总数 no_hit Counter 没有适用IP标签的请求总数 total Counter IP标签过滤器运行的请求总数 运行时 IP标签过滤器支持以下运行时设置: ip_tagging.http_filter_enabled 启用过滤器的请求的百分比，默认值是100。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 21:38:59 "},"configuration/http_filters/lua_filter.html":{"url":"configuration/http_filters/lua_filter.html","title":"Lua","keywords":"","body":"Lua Attention By default Envoy is built without exporting symbols that you may need when interacting with Lua modules installed as shared objects. Envoy may need to be built with support for exported symbols. Please see the Bazel docs for more information. Overview The HTTP Lua filter allows Lua scripts to be run during both the request and response flows. LuaJITis used as the runtime. Because of this, the supported Lua version is mostly 5.1 with some 5.2 features. See the LuaJIT documentation for more details. The filter only supports loading Lua code in-line in the configuration. If local filesystem code is desired, a trivial in-line script can be used to load the rest of the code from the local environment. The design of the filter and Lua support at a high level is as follows: All Lua environments are per worker thread. This means that there is no truly global data. Any globals create and populated at load time will be visible from each worker thread in isolation. True global support may be added via an API in the future. All scripts are run as coroutines. This means that they are written in a synchronous style even though they may perform complex asynchronous tasks. This makes the scripts substantially easier to write. All network/async processing is performed by Envoy via a set of APIs. Envoy will yield the script as appropriate and resume it when async tasks are complete. Do not perform blocking operations from scripts. It is critical for performance that Envoy APIs are used for all IO. Currently supported high level features NOTE: It is expected that this list will expand over time as the filter is used in production. The API surface has been kept small on purpose. The goal is to make scripts extremely simple and safe to write. Very complex or high performance use cases are assumed to use the native C++ filter API. Inspection of headers, body, and trailers while streaming in either the request flow, response flow, or both. Modification of headers and trailers. Blocking and buffering the full request/response body for inspection. Performing an outbound async HTTP call to an upstream host. Such a call can be performed while buffering body data so that when the call completes upstream headers can be modified. Performing a direct response and skipping further filter iteration. For example, a script could make an upstream HTTP call for authentication, and then directly respond with a 403 response code. Configuration v1 API reference v2 API reference Script examples This section provides some concrete examples of Lua scripts as a more gentle introduction and quick start. Please refer to the stream handle API for more details on the supported API. -- Called on the request path. function envoy_on_request(request_handle) -- Wait for the entire request body and add a request header with the body size. request_handle:headers():add(\"request_body_size\", request_handle:body():length()) end -- Called on the response path. function envoy_on_response(response_handle) -- Wait for the entire response body and a response header with the the body size. response_handle:headers():add(\"response_body_size\", response_handle:body():length()) -- Remove a response header named 'foo' response_handle:headers():remove(\"foo\") end function envoy_on_request(request_handle) -- Make an HTTP call to an upstream host with the following headers, body, and timeout. local headers, body = request_handle:httpCall( \"lua_cluster\", { [\":method\"] = \"POST\", [\":path\"] = \"/\", [\":authority\"] = \"lua_cluster\" }, \"hello world\", 5000) -- Add information from the HTTP call into the headers that are about to be sent to the next -- filter in the filter chain. request_handle:headers():add(\"upstream_foo\", headers[\"foo\"]) request_handle:headers():add(\"upstream_body_size\", #body) end function envoy_on_request(request_handle) -- Make an HTTP call. local headers, body = request_handle:httpCall( \"lua_cluster\", { [\":method\"] = \"POST\", [\":path\"] = \"/\", [\":authority\"] = \"lua_cluster\" }, \"hello world\", 5000) -- Response directly and set a header from the HTTP call. No further filter iteration -- occurs. request_handle:respond( {[\":status\"] = \"403\", [\"upstream_foo\"] = headers[\"foo\"]}, \"nope\") end Stream handle API When Envoy loads the script in the configuration, it looks for two global functions that the script defines: function envoy_on_request(request_handle) end function envoy_on_response(response_handle) end A script can define either or both of these functions. During the request path, Envoy will run envoy_on_request as a coroutine, passing an API handle. During the response path, Envoy will run envoy_on_response as a coroutine, passing an API handle. Attention It is critical that all interaction with Envoy occur through the passed stream handle. The stream handle should not be assigned to any global variable and should not be used outside of the coroutine. Envoy will fail your script if the handle is used incorrectly. The following methods on the stream handle are supported: headers() headers = handle:headers() Returns the stream’s headers. The headers can be modified as long as they have not been sent to the next filter in the header chain. For example, they can be modified after an httpCall() or after a body() call returns. The script will fail if the headers are modified in any other situation. Returns a header object. body() body = handle:body() Returns the stream’s body. This call will cause Envoy to yield the script until the entire body has been buffered. Note that all buffering must adhere to the flow control policies in place. Envoy will not buffer more data than is allowed by the connection manager. Returns a buffer object. bodyChunks() iterator = handle:bodyChunks() Returns an iterator that can be used to iterate through all received body chunks as they arrive. Envoy will yield the script in between chunks, but will not buffer them. This can be used by a script to inspect data as it is streaming by. for chunk in request_handle:bodyChunks() do request_handle:log(0, chunk:length()) end Each chunk the iterator returns is a buffer object. trailers() trailers = handle:trailers() Returns the stream’s trailers. May return nil if there are no trailers. The trailers may be modified before they are sent to the next filter. Returns a header object. log*() handle:logTrace(message) handle:logDebug(message) handle:logInfo(message) handle:logWarn(message) handle:logErr(message) handle:logCritical(message) Logs a message using Envoy’s application logging. message is a string to log. httpCall() headers, body = handle:httpCall(cluster, headers, body, timeout) Makes an HTTP call to an upstream host. Envoy will yield the script until the call completes or has an error. cluster is a string which maps to a configured cluster manager cluster. headers is a table of key/value pairs to send. Note that the :method, :path, and :authority headers must be set. body is an optional string of body data to send. timeout is an integer that specifies the call timeout in milliseconds. Returns headers which is a table of response headers. Returns body which is the string response body. May be nil if there is no body. respond() handle:respond(headers, body) Respond immediately and do not continue further filter iteration. This call is only valid in the request flow. Additionally, a response is only possible if request headers have not yet been passed to subsequent filters. Meaning, the following Lua code is invalid: function envoy_on_request(request_handle) for chunk in request_handle:bodyChunks() do request_handle:respond( {[\":status\"] = \"100\"}, \"nope\") end end headers is a table of key/value pairs to send. Note that the :status header must be set. body is a string and supplies the optional response body. May be nil. metadata() metadata = handle:metadata() Returns the current route entry metadata. Note that the metadata should be specified under the filter name i.e. envoy.lua. Below is an example of a metadata in a route entry. metadata: filter_metadata: envoy.lua: foo: bar baz: - bad - baz Returns a metadata object. Header object API add() headers:add(key, value) Adds a header. key is a string that supplies the header key. value is a string that supplies the header value. Attention Envoy treats certain headers specially. These are known as the O(1) or inline headers. The list of inline headers can be found here. If an inline header is already present in the header map, add()will have no effect. If attempting to add() a non-inline header, the additional header will be added so that the resultant headers contains multiple header entries with the same name. Consider using the replace function if want to replace a header with another value. Note also that we understand this behavior is confusing and we may change it in a future release. get() headers:get(key) Gets a header. key is a string that supplies the header key. Returns a string that is the header value or nil if there is no such header. __pairs() for key, value in pairs(headers) do end Iterates through every header. key is a string that supplies the header key. value is a string that supplies the header value. Attention In the currently implementation, headers cannot be modified during iteration. Additionally, if it is desired to modify headers after iteration, the iteration must be completed. Meaning, do not use break or any other mechanism to exit the loop early. This may be relaxed in the future. remove() headers:remove(key) Removes a header. key supplies the header key to remove. replace() headers:replace(key, value) Replaces a header. key is a string that supplies the header key. value is a string that supplies the header value. If the header does not exist, it is added as per the add() function. Buffer API length() size = buffer:length() Gets the size of the buffer in bytes. Returns an integer. getBytes() buffer:getBytes(index, length) Get bytes from the buffer. By default Envoy will not copy all buffer bytes to Lua. This will cause a buffer segment to be copied. index is an integer and supplies the buffer start index to copy. lengthis an integer and supplies the buffer length to copy. index + length must be less than the buffer length. Metadata object API get() metadata:get(key) Gets a metadata. key is a string that supplies the metadata key. Returns the corresponding value of the given metadata key. The type of the value can be: null, boolean, number, string and table. __pairs() for key, value in pairs(metadata) do end Iterates through every metadata entry. key is a string that supplies a metadata key. value is metadata entry value. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:31:38 "},"configuration/http_filters/rate_limit_filter.html":{"url":"configuration/http_filters/rate_limit_filter.html","title":"速率限制","keywords":"","body":"频率限制 全局频率限制架构概览 v1 API 参考 v2 API 参考 如果一个请求的路由或者虚拟主机中设置了一个或多个符合过滤条件的频率限制，HTTP 频率限制过滤器会调用频率限制服务。路由能够可选地包含虚拟主机的频率限制配置。一个请求中可以被施加一个或者多个路由限制。每个配置都会生成一个发给频率限制服务的描述符。 调用频率限制服务之后，如果任何描述符得到了超限的返回值，那么这个服务就会生成一个 429 的状态码。 编写 Action 注意 这一节是使用 v1 API 进行的，但其中的概念也是适用于 v2 API 的。未来会面向 v2 重写这部分内容。 每个路由或者虚拟主机上的的频率限制 Action 都会生成一个描述符条目。描述符条目的向量组成一个描述符。Action 可以用任何顺序编写，可以创建更加复杂的频率限制描述符。描述符会按照配置中的 Action 顺序执行。 例一 例如，要生成下面的描述符： (\"generic_key\", \"some_value\") (\"source_cluster\", \"from_cluster\") 就需要编写这样的配置代码： { \"actions\" : [ { \"type\" : \"generic_key\", \"descriptor_value\" : \"some_value\" }, { \"type\" : \"source_cluster\" } ] } 例二 如果一个 Action 没有加入描述符条目，那么这一配置就不会生成描述符。 例如下面的配置： { \"actions\" : [ { \"type\" : \"generic_key\", \"descriptor_value\" : \"some_value\" }, { \"type\" : \"remote_address\" }, { \"type\" : \"souce_cluster\" } ] } 如果一个请求没有设置 x-forwarded-for，不会生成描述符。 如果请求中设置了 x-forwarded-for，会生成如下的描述符： (\"generic_key\", \"some_value\") (\"remote_address\", \"\") (\"source_cluster\", \"from_cluster\") 统计 缓存过滤器输出会在 cluster..ratelimit. 命名空间输出统计数据。429 这一响应码也会出现在动态 HTTP 统计数据中。 名称 类型 描述 ok Counter 来自于频率限制服务的所有限制内响应数量 error Counter 联系频率限制服务时的错误总数 over_limit Counter 来自频率限制服务的所有超限响应数量 运行时 HTTP 频率限制过滤器支持如下的运行时配置： ratelimit.http_filter_enabled 调用频率限制服务的请求的百分比，缺省为 100。 ratelimit.http_filter_enforcing 调用频率限制服务并实施决策的请求的百分比。缺省为 100。这个选项可以用来在完全实施限制之前进行测试，从而了解频率限制实施产生的后果。 ratelimit..http_filter_enabled 在频率限制配置中指定的 route_key，利用这个数值调用频率限制的请求的百分比。缺省值为 100。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 10:49:14 "},"configuration/http_filters/router_filter.html":{"url":"configuration/http_filters/router_filter.html","title":"路由","keywords":"","body":"路由 HTTP 转发是用路由过滤器实现的。在 Envoy 环境中，几乎所有 HTTP 代理场景下都会使用到这一过滤器。该过滤器的主要职能就是执行路由表中的指令。在重定向和转发这两个主要任务之外，路由过滤器还需要处理重试、统计之类的任务。 v1 API 参考 v2 API 参考 HTTP 头 不管是外发/请求，还是接收/响应的过程中，HTTP 头都是由路由过滤器消费和设置的，下面会介绍这些内容： x-envoy-expected-rq-timeout-ms x-envoy-max-retries x-envoy-retry-on x-envoy-retry-grpc-on x-envoy-upstream-alt-stat-name x-envoy-upstream-canary x-envoy-upstream-rq-timeout-alt-response x-envoy-upstream-rq-timeout-ms x-envoy-upstream-rq-per-try-timeout-ms x-envoy-upstream-service-time x-envoy-original-path x-envoy-immediate-health-check-fail x-envoy-overloaded x-envoy-decorator-operation x-envoy-expected-rq-timeout-ms 以毫秒为单位的时间，路由器要求请求在这一时长内完成。Envoy 把这个信息写入 HTTP Header，这样被代理的主机收到这一请求之后，就可以根据这一节的内容来判断是否超时，也就是快速失败。这一信息会用在内部请求中，按照 x-envoy-upstream-rq-timeout-ms 头或路由超时的顺序执行。 x-envoy-max-retries 如果使用了重试策略，且没有指定重试次数，Envoy 缺省会重试一次。重试次数可以显式的在路由重试配置或者 x-envoy-max-retries 头中进行设置。如果重试策略没有配置，并且 x-envoy-retry-on 或者 x-envoy-retry-grpc-on 都没有开启，Envoy 就不会重试失败的请求。 Envoy 的重试功能，有一些需要注意的： 路由超时（通过 x-envoy-upstream-rq-timeout-ms 或者路由配置进行设置）是包含所有重试的。如果设置请求超时为 3 秒钟，并且第一次请求消耗了 2.7 秒，那么重试（包含补偿）需要在 0.3 秒之内完成。这一设计的目的是避免重试和超时的同步激增。 Envoy 使用了一种随机的递增算法，以 25 毫秒为基础单位进行补偿。首次重试会随机的选择 0 到 24 毫秒范围内的延时，第二次会在 0 到 74 毫秒之间，第三次就会发生 0 到 175 毫秒之间的延时。 如果最大重试次数同时在 HTTP 头和路由配置中都有设置，那么请求的最大重试次数会使用两个配置之中的最大值作为有效值。 x-envoy-retry-on 如果外发请求中设置了这个 Header，Envoy 就会试着重试失败的请求（重试次数缺省为 1，可以使用 x-envoy-max-retries 或者路由重试配置进行控制）。x-envoy-retry-on 的值用于表达重试策略。可以使用 , 作为分隔符，同时支持多个条件，其中包括： 5xx 如果上游服务器响应了 5xx 的状态码，或者完全不响应（断开、复位或者读超时）。（包含 connect-failure 以及 refused-stream）。 注意：如果一个请求超出了 x-envoy-expected-rq-timeout-ms（会出现 504 错误码），Envoy 是不会重试的。如果在某个尝试消耗太多时间的时候继续重试，可以使用 x-envoy-upstream-rq-timeout-ms，这是一个请求的外缘时间限制，包含了相关的重试时间。 gateway-error 这个策略和 5xx 类似，但是只会在收到 502、503 或者 504 时进行重试。 connect-failure Envoy 会在连接上游服务器失败的情况下进行重试（连接超时之类）（包含在 5xx 内）。 注意：连接失败/超时是 TCP 层而非请求层的问题。并不包含使用 x-envoy-upstream-rq-timeout-ms 或者路由配置进行设置的请求超时。 retriable-4xx 如果上游服务器响应了一个可以重试的 4xx 状态码，Envoy 就会进行重试。目前这一类别只包含一个 409 状态。 注意：这种策略要当心。有时 409 是说明有乐观锁需要更新。这种情况下调用者不应该重试，而是应该读取然后重试其他的写入，否则自动重试可能会导致 409 的持续出现。 refused-stream 如果上游服务器使用 REFUSED_STREAM 错误码复位了数据流，Envoy 就会进行重试。这种复位类型的意义就是说明这种请求是可以安全的进行重试的（包含在 5xx 范围之内） 重试次数可以使用 x-envoy-max-retries 或者重试策略进行控制。 注意重试策略还可以在路由层使用。 缺省情况下，除非按照上述方案进行配置，否则 Envoy 不会进行重试操作。 x-envoy-retry-grpc-on 为外发请求设置这个 Header，Envoy 在请求失败时就会重试（重试次数缺省为 1，可以使用 x-envoy-max-retries 或者路由重试配置进行控制）。gRPC 重试目前仅支持状态码包含在响应头中的情况。在尾部包含 gRPC 状态码的情况不会触发重试逻辑。可以使用 , 分隔的列表来指定多个策略。支持策略包括： cancelled 如果 gRPC 响应头中的状态码是 cancelled（1），Envoy 会尝试进行重试。 deadline-exceeded 如果 gRPC 响应头中的状态码是 deadline-exceeded（4），Envoy 会尝试进行重试。 resource-exhausted 如果 gRPC 响应头中的状态码是 resource-exhausted（8），Envoy 会尝试进行重试。 在使用 x-envoy-retry-grpc-on header 的同时，可以使用 x-envoy-max-retries header 进行重试次数的限制。 注意重试策略还可以在路由层使用。 缺省情况下，除非按照上述方案进行配置，否则 Envoy 不会进行重试操作。 x-envoy-upstream-alt-stat-name 在外发请求中设置这些 Header，让 Envoy 将上游的响应码和计时统计发送到另外的统计树中，这对于 Envoy 以外的应用级别分类统计很有帮助。这方面的细节，在输出树文档中有进一步的解释。 这个概念比较容易和定义集群时指定的 alt_stat_name 混淆，它指定的是在统计树中根目录下集群的备用名称。 x-envoy-upstream-canary 如果一个上游主机设置了这个 Header，路由会生成金丝雀服务专用的统计。输出树文档中有更详细的说明。 x-envoy-upstream-rq-timeout-alt-response 在外发请求中设置这个 Header，在请求超时的情况下 Envoy 设置一个 204 的返回码（而不是 504）。这个 Header 的值会被忽略，也就是说只要 Header 存在就可以了。可以参看 x-envoy-upstream-rq-timeout-ms。 x-envoy-upstream-rq-timeout-ms 在外发请求中设置这个 Header，Envoy 会覆盖路由配置，超时时间使用毫秒为单位。参看 x-envoy-upstream-rq-per-try-timeout-ms。 x-envoy-upstream-rq-per-try-timeout-ms 在外发请求中设置这个 Header，Envoy 会为路由的请求设置一个每次尝试的超时，这个超时必须不大于全局的路由超时（参看 x-envoy-upstream-rq-timeout-ms），否则就会被忽略。这使得调用者可以在进行重试的时候，设置一个更加严格的超时时间，从而控制整体的超时情况。 x-envoy-upstream-service-time 上游主机处理请求所消耗的时间，单位是毫秒。如果客户端希望知道服务处理时间和网络延迟，这个信息就非常有用了。这个 Header 是在响应中设置的。 x-envoy-original-path 如果路由使用了 prefix_rewrite，Envoy 就会把原有路径的 Header 保存到这里，以便记录和除错。 x-envoy-immediate-health-check-fail 如果上游主机返回了这个 Header（可以使任何值），Envoy 会立刻假设这个主机的主动健康检查已经失败（如果集群已经配置了主动健康检查）。这个功能可以通过标准数据面的处理，让上游主机进入故障状态，而无需等待下一次的健康检查。健康检查会让主机重新进入健康状态。健康检查概述中讲述了更多这方面内容。 x-envoy-overloaded 如果上游设置了这个 Header，Envoy 就不会进行重试了。目前这个 Header 的值是无意义的，只要 Header 本身存在即可。另外 Envoy 如果遇到管理模式或者上游熔断的情况，就会在下游响应中设置这个 Header。 x-envoy-decorator-operation 如果入站请求中有这个 Header，他的值会覆盖任何本地由跟踪系统定义的这一值。类似的如果这个 Header 存在于一个外发响应中，他的值也会覆盖客户端的定义。 统计 路由器会在集群命名空间中（依赖集群在所选路由的定义）输出很多统计数据。参考集群统计一节有更多这方面的信息。 路由过滤器在 http.. 命名空间输出统计信息。前缀定义来自所属的 HTTP 连接管理器。 名称 类型 描述 no_route Counter 所有没有路由，返回 404 的请求总数 no_cluster Counter 目标集群不存在并返回 404 的请求总数 rq_redirect Counter 收到重定向响应的请求总数 rq_direct_response Counter 直接响应的请求总数 rq_total Counter 被路由的请求总数 虚拟机群统计会被输出到 vhost..vcluster.. 命名空间，包括了下列内容： 名称 类型 描述 upstreamrq Counter HTTP 响应码（也就是 2xx、3xx 等）的聚合 upstreamrq Counter 指定的 HTTP 响应码（也就是 201、302 等） upstream_rq_time Histogram 请求时间的毫秒数 运行时 路由过滤器支持下列运行时设置： upstream.base_retry_backoff_ms 基础的重试时间计数，HTTP 路由一节有更多讲述。缺省为 25 毫秒。 upstream.maintenance_mode. 会立即得到 503 响应的请求的百分比。他会覆盖所有定义了集群名称的路由行为。可以用于负载清洗、故障注入等。缺省是禁用的。 upstream.use_retry 要进行重试的请求的百分比。这一配置会在任何重试配置之前检查，可以在需要的时候完全禁止任何重试。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:12:40 "},"configuration/http_filters/squash_filter.html":{"url":"configuration/http_filters/squash_filter.html","title":"Squash","keywords":"","body":"Squash Squash is an HTTP filter which enables Envoy to integrate with Squash microservices debugger. Code: https://github.com/solo-io/squash, API Docs: https://squash.solo.io/ Overview The main use case for this filter is in a service mesh, where Envoy is deployed as a sidecar. Once a request marked for debugging enters the mesh, the Squash Envoy filter reports its ‘location’ in the cluster to the Squash server - as there is a 1-1 mapping between Envoy sidecars and application containers, the Squash server can find and attach a debugger to the application container. The Squash filter also holds the request until a debugger is attached (or a timeout occurs). This enables developers (via Squash) to attach a native debugger to the container that will handle the request, before the request arrive to the application code, without any changes to the cluster. Configuration v1 API reference v2 API reference How it works When the Squash filter encounters a request containing the header ‘x-squash-debug’ it will: Delay the incoming request. Contact the Squash server and request the creation of a DebugAttachment On the Squash server side, Squash will attempt to attach a debugger to the application Envoy proxies to. On success, it changes the state of the DebugAttachment to attached. Wait until the Squash server updates the DebugAttachment object’s state to attached (or error state) Resume the incoming request Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:36:32 "},"configuration/cluster_manager/cluster_manager.html":{"url":"configuration/cluster_manager/cluster_manager.html","title":"集群管理器","keywords":"","body":"集群管理器 集群管理器 架构概览 v1 API 参考 v2 API 参考 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:56:46 "},"configuration/cluster_manager/cluster_stats.html":{"url":"configuration/cluster_manager/cluster_stats.html","title":"统计","keywords":"","body":"统计 概要 健康检查统计 异常点检测统计 动态 HTTP 统计 可变树动态 HTTP 统计 每个服务区域动态 HTTP 统计 负载均衡统计 负载均衡子集统计 概要 集群管理器有一个以 cluster_manager. 为根的统计树。有以下统计项。统计名称中的任何: 字符将会被替换成_。 名称 类型 描述 cluster_added Counter 添加的所有集群数 （通过静态配置或 CDS） cluster_modified Counter 修改的所有集群数 （通过 CDS） cluster_removed Counter 移除的所有集群数 （通过 CDS） active_clusters Gauge 当前活跃的（预热的）所有集群数 warming_clusters Gauge 当前正在预热的活动（非活跃的）所有集群数 每一个集群有一个以 cluster.. 为根的统计树。有以下统计项: 名称 类型 描述 upstream_cx_total Counter 总连接数 upstream_cx_active Gauge 总激活的连接数 upstream_cx_http1_total Counter HTTP/1.1 总连接数 upstream_cx_http2_total Counter Total HTTP/2 总连接数 upstream_cx_connect_fail Counter 总连接失败数 upstream_cx_connect_timeout Counter 总连接超时数 upstream_cx_idle_timeout Counter 总连接空闲超时 upstream_cx_connect_attempts_exceeded Counter 超过配置连接尝试的总连续连接失败数 upstream_cx_overflow Counter 机群连接断路器溢出的总次数 upstream_cx_connect_ms Histogram 连接建立毫秒 upstream_cx_length_ms Histogram 连接长度毫秒 upstream_cx_destroy Counter 完全破坏连接 upstream_cx_destroy_local Counter 本地连接破坏 upstream_cx_destroy_remote Counter 远程连接完全销毁数 upstream_cx_destroy_with_active_rq Counter 用1 +主动请求销毁总连接数 upstream_cx_destroy_local_with_active_rq Counter 用1 +主动请求销毁本地总连接数 upstream_cx_destroy_remote_with_active_rq Counter 用1 +主动请求远程销毁总连接 upstream_cx_close_notify Counter 通过 HTTP/1.1 连接关闭报头或HTTP/2 GOAWAY 关闭的总连接数 upstream_cx_rx_bytes_total Counter 接收到的总连接字节数 upstream_cx_rx_bytes_buffered Gauge 当前缓冲的接收连接字节 upstream_cx_tx_bytes_total Counter 发送的连接字节总数 upstream_cx_tx_bytes_buffered Gauge 发送当前缓冲的连接字节 upstream_cx_protocol_error Counter 总连接协议错误 upstream_cx_max_requests Counter 由于最大请求关闭总连接 upstream_cx_none_healthy Counter 由于没有健康主机，连接没有建立的总次数 upstream_rq_total Counter 总请求量 upstream_rq_active Gauge 总活动请求 upstream_rq_pending_total Counter 等待连接池连接的总请求 upstream_rq_pending_overflow Counter 溢出连接池电路中断和失败的总请求 upstream_rq_pending_failure_eject Counter 由于连接池连接失败而导致的总请求失败 upstream_rq_pending_active Gauge 挂起连接池连接的全部活动请求 upstream_rq_cancelled Counter 在获得连接池连接之前取消的总请求 upstream_rq_maintenance_mode Counter 由于维护模式导致的请求总数为 503 upstream_rq_timeout Counter 等待响应的总请求 upstream_rq_per_try_timeout Counter 每次尝试超时的总请求 upstream_rq_rx_reset Counter 远程重置的总请求 upstream_rq_tx_reset Counter 本地重置的总请求 upstream_rq_retry Counter 总请求重试 upstream_rq_retry_success Counter 总请求重试成功率 upstream_rq_retry_overflow Counter 由于电路断开而未重试的总请求 upstream_flow_control_paused_reading_total Counter 上游流量控制暂停读取的总次数 upstream_flow_control_resumed_reading_total Counter 从上游读取的流量控制的总次数 upstream_flow_control_backed_up_total Counter 上游连接备份和暂停读取的总次数从下游读取 upstream_flow_control_drained_total Counter 从上游读取和恢复上游连接的总次数 membership_change Counter 总簇成员变化 membership_healthy Gauge 当前集群健康总量（包括健康检查和异常值检测） membership_total Gauge 当前群集成员总数 retry_or_shadow_abandoned Counter 由于缓冲区限制，阴影或重试缓冲的总次数被取消。 config_reload Counter 由于配置不同，导致的 API 重新加载导致配置重新加载。 update_attempt Counter 总簇成员更新尝试 update_success Counter 总簇成员更新成功率 update_failure Counter 总簇成员更新失败 update_empty Counter 使用空集群负载分配结束的总群集成员更新，并继续使用以前配置 update_no_rebuild Counter 没有导致任何集群负载均衡结构重建的完全成功的群集成员更新 version Gauge 从最后一次成功的 API 获取内容的哈希 max_host_weight Gauge 集群中任意主机的最大权重 bind_errors Counter 将套接字绑定到已配置的源地址的总错误 健康检查统计 如果配置了健康检查，则该集群具有根于根 cluster..health_check. 的附加统计树。有如下统计: 名称 类型 描述 attempt Counter 健康检查次数 success Counter 健康检查成功次数 failure Counter 立即失效的健康检查（例如HTTP 503）以及网络故障的次数 passive_failure Counter 由于被动事件引起的健康检查失败的次数（例如X-Enviv-即时健康检查失败） network_failure Counter 网络错误引起的健康检查失败次数 verify_cluster Counter 尝试群集名称验证的健康检查数量 healthy Gauge 健康会员人数 异常点检测统计 如果为集群配置了异常点检测，统计将以 cluster..outlier_detection. 为根，包含如下: 名称 类型 描述 ejections_enforced_total Counter 由于异常值类型强制执行的驱逐次数 ejections_active Gauge 当前驱逐主机的数量 ejections_overflow Counter 由于最大驱逐率而中止的驱逐数 ejections_enforced_consecutive_5xx Counter 强制执行的连续 5xx 驱逐数 ejections_detected_consecutive_5xx Counter 检测到的连续 5xx 驱逐数（即使未强制执行） ejections_enforced_success_rate Counter 强制成功率异常离群数 ejections_detected_success_rate Counter 检测成功率离群值的数量（即使未执行） ejections_enforced_consecutive_gateway_failure Counter 强制连续网关失败驱逐数 ejections_detected_consecutive_gateway_failure Counter 检测到的连续网关故障驱逐数目（即使未强制执行） ejections_total Counter 不赞成的由于异常值类型引起的驱逐次数（即使未执行） ejections_consecutive_5xx Counter 不赞成的连续5xx驱逐数（即使未强制执行） 动态 HTTP 统计 如果使用 HTTP，动态 HTTP 响应代码统计也是可用的。 这些是由各种内部系统发出的，以及一些过滤器，如路由器过滤器和速率限制过滤器。统计将以 cluster.. 为根，包含如下: 名称 类型 描述 upstreamrq Counter HTTP响应代码汇总（例如，2xx 3xx，等。） upstreamrq Counter 特异性的HTTP响应代码（例如，201、302等。） upstream_rq_time Histogram 请求时间ms canary.upstreamrq Counter 上游的 canary 聚合 HTTP 响应代码 canary.upstreamrq Counter 上游的 canary HTTP 特定响应代码 canary.upstream_rq_time Histogram 上游的 canary 请求时间 ms internal.upstreamrq Counter 内部原始聚合 HTTP 响应代码 internal.upstreamrq Counter 内部原始特定 HTTP 响应代码 internal.upstream_rq_time Histogram 内部原始请求时间 ms external.upstreamrq Counter HTTP响应代码聚集外部性 external.upstreamrq Counter HTTP响应代码和特异性 external.upstream_rq_time Histogram 外部原始请求时间 ms 可变树动态 HTTP 统计 如果配置了可变树统计信息，则它们将存在于 cluster...命名空间。所产生的统计数据与动态 HTTP 统计部分以上所记录的数据相同。 每个服务区域动态 HTTP 统计 如果服务区域可用于本地服务 (通过 --service-zone) 和 上游集群, Envoy 在将跟踪 cluster..zone... 命名空间的以下统计数据. 名称 类型 描述 upstreamrq Counter 聚合 HTTP 响应代码（例如，2xx 3xx，等。） upstreamrq Counter 特定的 HTTP 响应代码（例如，201 等） upstream_rq_time Histogram 请求时间 ms 负载均衡统计 负载均衡器决策监测统计。统计以 cluster.. 为根，包含如下统计: 名称 类型 描述 lb_recalculate_zone_structures Counter 重新生成局部感知路由结构的次数，用于上游位置选择的快速决策 lb_healthy_panic Counter 在恐慌模式下与负载平衡器平衡的总请求负载 lb_zone_cluster_too_small Counter 由于上游簇大小小，无区域感知路由 lb_zone_routing_all_directly Counter 将所有请求直接发送到同一区域 lb_zone_routing_sampled Counter 向同一区域发送一些请求 lb_zone_routing_cross_zone Counter 区域感知路由模式，但必须发送跨区域 lb_local_cluster_not_ok Counter 本地主机集未设置或是本地集群的恐慌模式 lb_zone_number_differs Counter 本地和上游集群的区域数目不同 lb_zone_no_capacity_left Counter 舍入误差导致随机区域选择结束的次数 负载均衡子集统计 负载均衡器子集 决策的统计监测。统计以 cluster.. 为根，包含如下统计: 名称 类型 描述 lb_subsets_active Gauge 当前可用子集的数目 lb_subsets_created Counter 创建的子集数 lb_subsets_removed Counter 由于没有主机而删除的子集数 lb_subsets_selected Counter 选择任何子集的负载平衡次数 lb_subsets_fallback Counter 调用回退策略的次数 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 10:30:01 "},"configuration/cluster_manager/cluster_runtime.html":{"url":"configuration/cluster_manager/cluster_runtime.html","title":"运行时","keywords":"","body":"运行时 上游集群支持以下运行时设置： 主动健康检查 health_check.min_interval 健康检查的最小值 interval。默认值是0。健康检查 interval 的值将位于 min_interval 和 max_interval之间。 health_check.max_interval 健康检查的最大值 interval。默认值是 MAX_INT。健康检查interval的值将位于 min_interval 和 max_interval 之间。 health_check.verify_cluster 当健康检查过滤器将远程服务集群写入响应时，将对预期的上游服务验证什么百分比的健康检查请求。 异常点检测 查看异常点检测架构概览 取得更多异常点检测的信息。异常点检测支持的运行时参数和 静态配置参数一样，分别为： outlier_detection.consecutive_5xx consecutive_5XX 在异常点检测中的配置 outlier_detection.consecutive_gateway_failure consecutive_gateway_failure 在异常点检测中的配置 outlier_detection.interval_ms interval_ms 在异常点检测中的配置 outlier_detection.base_ejection_time_ms base_ejection_time_ms 在异常点检测中的配置 outlier_detection.max_ejection_percent max_ejection_percent 在异常点检测中的配置 outlier_detection.enforcing_consecutive_5xx enforcing_consecutive_5xx 在异常点检测中的配置 outlier_detection.enforcing_consecutive_gateway_failure enforcing_consecutive_gateway_failure 在异常点检测中的配置 outlier_detection.enforcing_success_rate enforcing_success_rate 在异常点检测中的配置 outlier_detection.success_rate_minimum_hosts success_rate_minimum_hosts 在异常点检测中的配置 outlier_detection.success_rate_request_volume success_rate_request_volume 在异常点检测中的配置 outlier_detection.success_rate_stdev_factor success_rate_stdev_factor 在异常点检测中的配置 核心 upstream.healthy_panic_threshold 设置恐慌阈百分比. 默认达到 50%. upstream.use_http2 如果配置的话，集群是否使用 http2 特征 。设置为0以禁用HTTP / 2，即使配置了该功能.默认值是关闭。 upstream.weight_enabled 用来打开或者关闭权重负载均衡的二级制开关。如果设置成非0数值，按权重负载均衡的功能是打开的。默认值是打开。 区域感知负载均衡 upstream.zone_routing.enabled 多大百分比的请求将会被路由到相同上游区域。默认是 100% 请求。 upstream.zone_routing.min_cluster_size 某个上游集群能被区域感知尝试的最小值。 默认值是 6。如果上游集群数值比 min_cluster_size 小，区域路由感知将不被执行。 断路 circuit_breakers...max_connections 断路器设置最大连接数 circuit_breakers...max_pending_requests 断路器设置最大等待数 circuit_breakers...max_requests 断路器设置最大请求数 circuit_breakers...max_retries 断路器设置最大重试次数 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 10:45:22 "},"configuration/cluster_manager/cds.html":{"url":"configuration/cluster_manager/cds.html","title":"集群发现服务（CDS）","keywords":"","body":"集群发现服务（CDS） 集群发现服务（CDS）是一个可选的API，Envoy将调用该API来动态获取集群管理成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加，修改或删除已知的集群。 注意 在 Envoy 配置中静态定义的任何群集都不能通过 CDS API 进行修改或删除。 v1 CDS API v2 CDS API 统计 CDS 的统计树以 cluster_manager.cds. 为根，统计如下： 名字 类型 描述 config_reload 计数器 因配置不同而导致配置重新加载的总次数 update_attempt 计数器 尝试调用配置加载API的总次数 update_success 计数器 调用配置加载API成功的总次数 update_failure 计数器 调用配置加载API失败的总次数（网络或参数错误） version 测量 来自上次成功调用配置加载API的内容哈希 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 23:35:28 "},"configuration/cluster_manager/cluster_hc.html":{"url":"configuration/cluster_manager/cluster_hc.html","title":"健康检查","keywords":"","body":"健康检查 健康检查架构概览。 如果为集群配置运行状况检查，会有额外的数据发出。记录在这里。 v1 API 参考文档。 v2 API 参考文档。 TCP 健康检查 注意 本节是为 v1 API 编写的，但这些概念也适用于 v2 API。针对 V2 API 将在未来的正式版中重新定义。 执行的匹配类型如下（这是MongoDB运行状况检查请求和响应）： { \"send\": [ {\"binary\": \"39000000\"}, {\"binary\": \"EEEEEEEE\"}, {\"binary\": \"00000000\"}, {\"binary\": \"d4070000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"746573742e\"}, {\"binary\": \"24636d6400\"}, {\"binary\": \"00000000\"}, {\"binary\": \"FFFFFFFF\"}, {\"binary\": \"13000000\"}, {\"binary\": \"01\"}, {\"binary\": \"70696e6700\"}, {\"binary\": \"000000000000f03f\"}, {\"binary\": \"00\"} ], \"receive\": [ {\"binary\": \"EEEEEEEE\"}, {\"binary\": \"01000000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"0000000000000000\"}, {\"binary\": \"00000000\"}, {\"binary\": \"11000000\"}, {\"binary\": \"01\"}, {\"binary\": \"6f6b\"}, {\"binary\": \"00000000000000f03f\"}, {\"binary\": \"00\"} ] } 在每个运行状况检查周期中，所有 \"send\" 字节都会发送到目标服务器。每个二进制块的长度可以是任意的，并且在发送时只是连接在一起。（分离成多个块可用于可读性）。 在检查响应时，执行“模糊”匹配，以便每个二进制块必须被找到，并且按照指定的顺序，但不一定是连续的。因此，在上面的示例中，可以在 “EEEEEEEE” 和 “01000000” 之间的响应中插入 “FFFFFFFF”，并且该检查仍然会通过。这样做是为了支持将非确定性数据（如时间）插入到响应中的协议。 健康检查需要更复杂的模式，如发送/接收/发送/接收目前不可能。 如果 “receive ” 是一个空数组，则 Envoy 将执行 \"connect only\" TCP 健康检查。在每个周期中，Envoy 将尝试连接到上游主机，并且如果连接成功，则认为它是成功的。每个健康检查周期都会创建一个新连接。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-22 11:13:17 "},"configuration/cluster_manager/cluster_circuit_breakers.html":{"url":"configuration/cluster_manager/cluster_circuit_breakers.html","title":"断路","keywords":"","body":"断路 断路架构概览 v1 API 文档 v2 API 文档 运行时 ​ 所有的断路设置都是运行时可配置的，是基于集群名称的优先级定义。 他们遵循以下命名方案 circuit_breakers...。 cluster_name 是每个群集配置中的名称字, 设置在envoy的 配置文件中。 可用的运行时设置将覆盖envoy配置文件中的配置。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:56:38 "},"configuration/health_checkers/redis.html":{"url":"configuration/health_checkers/redis.html","title":"Redis","keywords":"","body":"Redis Redis 健康检查器是一个用于检查 Redis 上游主机的定制检查器。主要通过发送 PING 命令和接收 PONG 命令进行工作。上游 Redis 主机可以使用 PONG 以外的任何其他响应来导致立即激活的运行状况检查失败。或者，Envoy 可以在用户指定的密钥上执行 EXISTS。如果密钥不存在，则认为它是合格的健康检查。这允许用户通过将执行的密钥设置为任意值并等待流量耗尽来标记 Redis 实例以进行维护。 v2 API reference Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 18:14:12 "},"configuration/access_log.html":{"url":"configuration/access_log.html","title":"访问记录","keywords":"","body":"访问记录 Configuration Access logs are configured as part of the HTTP connection manager config or TCP Proxy. v1 API reference v2 API reference Format rules The access log format string contains either command operators or other characters interpreted as a plain string. The access log formatter does not make any assumptions about a new line separator, so one has to specified as part of the format string. See the default format for an example. Note that the access log line will contain a ‘-‘ character for every not set/empty value. The same format strings are used by different types of access logs (such as HTTP and TCP). Some fields may have slightly different meanings, depending on what type of log it is. Differences are noted. The following command operators are supported: %START_TIME% HTTPRequest start time including milliseconds.TCPDownstream connection start time including milliseconds.START_TIME can be customized using a format string, for example: %START_TIME(%Y/%m/%dT%H:%M:%S%z %s)% %BYTES_RECEIVED% HTTPBody bytes received.TCPDownstream bytes received on connection. %PROTOCOL% HTTPProtocol. Currently either HTTP/1.1 or HTTP/2.TCPNot implemented (“-“). %RESPONSE_CODE% HTTPHTTP response code. Note that a response code of ‘0’ means that the server never sent the beginning of a response. This generally means that the (downstream) client disconnected.TCPNot implemented (“-“). %BYTES_SENT% HTTPBody bytes sent.TCPDownstream bytes sent on connection. %DURATION% HTTPTotal duration in milliseconds of the request from the start time to the last byte out.TCPTotal duration in milliseconds of the downstream connection. %RESPONSE_FLAGS% Additional details about the response or connection, if any. For TCP connections, the response codes mentioned in the descriptions do not apply. Possible values are:HTTP and TCPUH: No healthy upstream hosts in upstream cluster in addition to 503 response code.UF: Upstream connection failure in addition to 503 response code.UO: Upstream overflow (circuit breaking) in addition to 503 response code.NR: No route configured for a given request in addition to 404 response code.HTTP onlyLH: Local service failed health check request in addition to 503 response code.UT: Upstream request timeout in addition to 504 response code.LR: Connection local reset in addition to 503 response code.UR: Upstream remote reset in addition to 503 response code.UC: Upstream connection termination in addition to 503 response code.DI: The request processing was delayed for a period specified via fault injection.FI: The request was aborted with a response code specified via fault injection.RL: The request was ratelimited locally by the HTTP rate limit filter in addition to 429 response code. %UPSTREAM_HOST% Upstream host URL (e.g., tcp://ip:port for TCP connections). %UPSTREAM_CLUSTER% Upstream cluster to which the upstream host belongs to. %UPSTREAM_LOCAL_ADDRESS% Local address of the upstream connection. If the address is an IP address it includes both address and port. %DOWNSTREAM_REMOTE_ADDRESS% Remote address of the downstream connection. If the address is an IP address it includes both address and port.NoteThis may not be the physical remote address of the peer if the address has been inferred from proxy proto or x-forwarded-for. %DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT% Remote address of the downstream connection. If the address is an IP address the output doesnot include port.NoteThis may not be the physical remote address of the peer if the address has been inferred from proxy proto or x-forwarded-for. %DOWNSTREAM_LOCAL_ADDRESS% Local address of the downstream connection. If the address is an IP address it includes both address and port. If the original connection was redirected by iptables REDIRECT, this represents the original destination address restored by the Original Destination Filter using SO_ORIGINAL_DST socket option. If the original connection was redirected by iptables TPROXY, and the listener’s transparent option was set to true, this represents the original destination address and port. %DOWNSTREAM_LOCAL_ADDRESS_WITHOUT_PORT% Same as %DOWNSTREAM_LOCAL_ADDRESS% excluding port if the address is an IP address. %REQ(X?Y):Z% HTTPAn HTTP request header where X is the main HTTP header, Y is the alternative one, and Z is an optional parameter denoting string truncation up to Z characters long. The value is taken from the HTTP request header named X first and if it’s not set, then request header Y is used. If none of the headers are present ‘-‘ symbol will be in the log.TCPNot implemented (“-“). %RESP(X?Y):Z% HTTPSame as %REQ(X?Y):Z% but taken from HTTP response headers.TCPNot implemented (“-“). %TRAILER(X?Y):Z% HTTPSame as %REQ(X?Y):Z% but taken from HTTP response trailers.TCPNot implemented (“-“). %DYNAMIC_METADATA(NAMESPACE:KEY*):Z% HTTPDynamic Metadata info, where NAMESPACE is the the filter namespace used when setting the metadata, KEY is an optional lookup up key in the namespace with the option of specifying nested keys separated by ‘:’, and Z is an optional parameter denoting string truncation up to Z characters long. Dynamic Metadata can be set by filters using the RequestInfo API: setDynamicMetadata. The data will be logged as a JSON string. For example, for the following dynamic metadata:com.test.my_filter: {\"test_key\": \"foo\", \"test_object\": {\"inner_key\": \"bar\"}}%DYNAMIC_METADATA(com.test.my_filter)% will log: {\"test_key\": \"foo\", \"test_object\": {\"inner_key\": \"bar\"}}%DYNAMIC_METADATA(com.test.my_filter:test_key)% will log: \"foo\"%DYNAMIC_METADATA(com.test.my_filter:test_object)% will log: {\"inner_key\": \"bar\"}%DYNAMIC_METADATA(com.test.my_filter:test_object:inner_key)% will log: \"bar\"%DYNAMIC_METADATA(com.unknown_filter)% will log: -%DYNAMIC_METADATA(com.test.my_filter:unknown_key)% will log: -%DYNAMIC_METADATA(com.test.my_filter):25% will log (truncation at 25 characters): {\"test_key\": \"foo\", \"testTCPNot implemented (“-“). Default format If custom format is not specified, Envoy uses the following default format: [%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\\n Example of the default Envoy access log format: [2016-04-15T20:17:00.310Z] \"POST /api/v1/locations HTTP/2\" 204 - 154 0 226 100 \"10.0.35.28\" \"nsq2http\" \"cc21d9b0-cf5c-432b-8c7e-98aeb7988cd2\" \"locations\" \"tcp://10.0.2.1:80\" Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:41:10 "},"configuration/rate_limit.html":{"url":"configuration/rate_limit.html","title":"速率限制服务","keywords":"","body":"速率限制服务 The rate limit service configuration specifies the global rate limit service Envoy should talk to when it needs to make global rate limit decisions. If no rate limit service is configured, a “null” service will be used which will always return OK if called. v1 API reference v2 API reference gRPC service IDL Envoy expects the rate limit service to support the gRPC IDL specified in/source/common/ratelimit/ratelimit.proto. See the IDL documentation for more information on how the API works. See Lyft’s reference implementation here. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:41:40 "},"configuration/runtime.html":{"url":"configuration/runtime.html","title":"运行时","keywords":"","body":"运行时 运行时配置指定了包含可以重载配置元素的本地文件系统数的位置。值可以在 /runtime admin endpoint 查看。值可以在 /runtime_modify admin endpoint 修改和追加. 如果没用进行运行时配置，则会使用空提供程序，该提供程序会使用代码中内置的除了通过 /runtime_modify 添加的值之外的所有缺省值。 注意 要小心使用 /runtime_modify 端点。 其变更是立即生效的。管理接口的妥善的保护是非常重要的。 v1 API 参考 v2 API 参考 文件系统设计 配置指南的各个部分描述了可用的运行时设置。 例如， 这里是上游集群的运行时配置。 假定文件夹 /srv/runtime/v1 指向存储全局运行时配置的实际文件系统路径。以下是运行时的典型配置参数: symlink_root: /srv/runtime/current subdirectory: envoy override_subdirectory: envoy_override 这里/srv/runtime/current是到/srv/runtime/v1的符号链接。 运行时配置键中的每一个‘.’ 都代表层次结构中的一个新的目录， 扎根于symlink_root +subdirectory。例如， 键health_check.min_interval将具有以下完整文件系统路径（使用符号链接）： /srv/runtime/current/envoy/health_check/min_interval 路径的末端的部分是文件。文件的内容构成运行时值。从文件读取数值时，空格和新行将被忽略。 override_subdirectory 与 --service-cluster 在命令行界面操作时一起使用的。假如 --service-cluster 被设置成为了my-cluster。Envoy将首先从下面完整的文件系统路径中找 health_check.min_interval项： /srv/runtime/current/envoy_override/my-cluster/health_check/min_interval 如果找到了，该值将覆盖在主查找路径中找到的任何值。这允许用户在全局默认值之上自定义单个群集的运行时值。 注释 行首为#的行是注释。 注释可以提供现有值的上下文。注释在其他空文件中也很有用，其可以在需要的时候保留占位符以进行部署。 通过符号链接交换更新运行时值 更新运行时值总共有两步。第一步， 创建整个运行时树的硬拷贝并更新所需的运行时值。第二步， 使用以下命令的等价物，将旧树中的符号链接根自动交换到新的运行时树： /srv/runtime:~$ ln -s /srv/runtime/v2 new && mv -Tf new current 关于如何部署文件系统数据，如何收集垃圾等操作超出了本文的范围。 统计 文件系统运行时提供程序在运行时发出一些统计信息。命名空间。 名称 类型 描述 load_error Counter 错误重新尝试加载的总数 override_dir_not_exists Counter 未使用覆盖目录的加载总数 override_dir_exists Counter 使用覆盖目录的加载总数 load_success Counter 成功加载的尝试总数 num_keys Gauge 当前加载的键数 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 19:33:47 "},"configuration/statistics.html":{"url":"configuration/statistics.html","title":"统计","keywords":"","body":"统计 发出一些统计数据来统计系统行为: 名称 类型 描述 stats.overflow Counter 由于共享内存不足，Envoy 无法分配统计的总次数 Server 根植与server. 与服务器相关的统计信息，统计信息如下: 名称 类型 描述 uptime Gauge 当前 server 的启动时间 memory_allocated Gauge 当前分配的内存大小，以字节为单位 memory_heap_size Gauge 当前堆预留的内存大小，以字节为单位 live Gauge 1：当前server还没有耗尽；0：其他 parent_connections Gauge 在热启动中所有就的 Envoy 进程的连接数 total_connections Gauge 所有的 Envoy 进程的连接数，包括新的、旧的 version Gauge 整型表示基于 SCM 修订的版本号 days_until_first_cert_expiring Gauge 下一个证书到期的天数 File system 在 filesystem. 中发出的与文件系统相关的统计信息，名称空间 名称 类型 描述 write_buffered Counter 文件数据被移动到 Envoy 内部缓冲区的总次数 write_completed Counter 文件被写入的总次数 flushed_by_timer Counter 由于刷新超时而导致内部刷新缓冲区写入文件的总次数 reopen_failed Counter 文件被打开失败的总次数 write_total_buffered Gauge 当前内部数据缓冲区的总大小，以字节为单位 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:11:43 "},"configuration/tools/router_check.html":{"url":"configuration/tools/router_check.html","title":"路由表检查工具","keywords":"","body":"路由表检查工具 NOTE: The following configuration is for the route table check tool only and is not part of the Envoy binary. The route table check tool is a standalone binary that can be used to verify Envoy’s routing for a given configuration file. The following specifies input to the route table check tool. The route table check tool checks if the route returned by a router matches what is expected. The tool can be used to check cluster name, virtual cluster name, virtual host name, manual path rewrite, manual host rewrite, path redirect, and header field matches. Extensions for other test cases can be added. Details about installing the tool and sample tool input/output can be found at installation. The route table check tool config is composed of an array of json test objects. Each test object is composed of three parts. Test name This field specifies the name of each test object. Input values The input value fields specify the parameters to be passed to the router. Example input fields include the :authority, :path, and :method header fields. The :authority and :path fields specify the url sent to the router and are required. All other input fields are optional. Validate The validate fields specify the expected values and test cases to check. At least one test case is required. A simple tool configuration json has one test case and is written as follows. The test expects a cluster name match of “instant-server”.: [ { \"test_name: \"Cluster_name_test\", \"input\": { \":authority\":\"api.lyft.com\", \":path\": \"/api/locations\" }, \"validate\": { \"cluster_name\": \"instant-server\" } } ] [ { \"test_name\": \"...\", \"input\": { \":authority\": \"...\", \":path\": \"...\", \":method\": \"...\", \"internal\" : \"...\", \"random_value\" : \"...\", \"ssl\" : \"...\", \"additional_headers\": [ { \"field\": \"...\", \"value\": \"...\" }, { \"...\" } ] }, \"validate\": { \"cluster_name\": \"...\", \"virtual_cluster_name\": \"...\", \"virtual_host_name\": \"...\", \"host_rewrite\": \"...\", \"path_rewrite\": \"...\", \"path_redirect\": \"...\", \"header_fields\" : [ { \"field\": \"...\", \"value\": \"...\" }, { \"...\" } ] } }, { \"...\" } ] test_name (required, string) The name of a test object. input (required, object) Input values sent to the router that determine the returned route.:authority(required, string) The url authority. This value along with the path parameter define the url to be matched. An example authority value is “api.lyft.com”.:path(required, string) The url path. An example path value is “/foo”.:method(optional, string) The request method. If not specified, the default method is GET. The options are GET, PUT, or POST.internal(optional, boolean) A flag that determines whether to set x-envoy-internal to “true”. If not specified, or if internal is equal to false, x-envoy-internal is not set.random_value(optional, integer) An integer used to identify the target for weighted cluster selection. The default value of random_value is 0.ssl(optional, boolean) A flag that determines whether to set x-forwarded-proto to https or http. By setting x-forwarded-proto to a given protocol, the tool is able to simulate the behavior of a client issuing a request via http or https. By default ssl is false which corresponds to x-forwarded-proto set to http.additional_headers(optional, array) Additional headers to be added as input for route determination. The “:authority”, “:path”, “:method”, “x-forwarded-proto”, and “x-envoy-internal” fields are specified by the other config options and should not be set here.field(required, string) The name of the header field to add.value(required, string) The value of the header field to add. validate (required, object) The validate object specifies the returned route parameters to match. At least one test parameter must be specificed. Use “” (empty string) to indicate that no return value is expected. For example, to test that no cluster match is expected use {“cluster_name”: “”}.cluster_name(optional, string) Match the cluster name.virtual_cluster_name(optional, string) Match the virtual cluster name.virtual_host_name(optional, string) Match the virtual host name.host_rewrite(optional, string) Match the host header field after rewrite.path_rewrite(optional, string) Match the path header field after rewrite.path_redirect(optional, string) Match the returned redirect path.header_fields(optional, array) Match the listed header fields. Examples header fields include the “:path”, “cookie”, and “date” fields. The header fields are checked after all other test cases. Thus, the header fields checked will be those of the redirected or rewriten routes when applicable.field(required, string) The name of the header field to match.value(required, string) The value of the header field to match. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 19:43:20 "},"operations/cli.html":{"url":"operations/cli.html","title":"命令行选项","keywords":"","body":"命令行选项 Envoy is driven both by a JSON configuration file as well as a set of command line options. The following are the command line options that Envoy supports. -c , --config-path (optional) The path to the v1 or v2 JSON/YAML/proto3 configuration file. If this flag is missing, --config-yaml is required. This will be parsed as a v2 bootstrap configuration file and on failure, subject to --v2-config-only, will be considered as a v1 JSON configuration file. For v2 configuration files, valid extensions are .json, .yaml, .pb and .pb_text, which indicate JSON, YAML, binary proto3 and text proto3 formats respectively. --config-yaml (optional) The YAML string for a v2 bootstrap configuration. If --config-path is also set,the values in this YAML string will override and merge with the bootstrap loaded from --config-path. Because YAML is a superset of JSON, a JSON string may also be passed to --config-yaml. --config-yaml is not compatible with bootstrap v1.Example overriding the node id on the command line:./envoy -c bootstrap.yaml –config-yaml “node: {id: ‘node1’}” --v2-config-only (optional) This flag determines whether the configuration file should only be parsed as a v2 bootstrap configuration file. If false (default), when a v2 bootstrap config parse fails, a second attempt to parse the config as a v1 JSON configuration file will be made. --mode (optional) One of the operating modes for Envoy:serve: (default) Validate the JSON configuration and then serve traffic normally.validate: Validate the JSON configuration and then exit, printing either an “OK” message (in which case the exit code is 0) or any errors generated by the configuration file (exit code 1). No network traffic is generated, and the hot restart process is not performed, so no other Envoy process on the machine will be disturbed. --admin-address-path (optional) The output file path where the admin address and port will be written. --local-address-ip-version (optional) The IP address version that is used to populate the server local IP address. This parameter affects various headers including what is appended to the X-Forwarded-For (XFF) header. The options are v4 or v6. The default is v4. --base-id (optional) The base ID to use when allocating shared memory regions. Envoy uses shared memory regions during hot restart. Most users will never have to set this option. However, if Envoy needs to be run multiple times on the same machine, each running Envoy will need a unique base ID so that the shared memory regions do not conflict. --concurrency (optional) The number of worker threads to run. If not specified defaults to the number of hardware threads on the machine. -l , --log-level (optional) The logging level. Non developers should generally never set this option. See the help text for the available log levels and the default. --log-path (optional) The output file path where logs should be written. This file will be re-opened when SIGUSR1 is handled. If this is not set, log to stderr. --log-format (optional) The format string to use for laying out the log message metadata. If this is not set, a default format string \"[%Y-%m-%d %T.%e][%t][%l][%n] %v\" is used.The supported format flags are (with example output):%v:The actual message to log (“some user text”)%t:Thread id (“1232”)%P:Process id (“3456”)%n:Logger’s name (“filter”)%l:The log level of the message (“debug”, “info”, etc.)%L:Short log level of the message (“D”, “I”, etc.)%a:Abbreviated weekday name (“Tue”)%A:Full weekday name (“Tuesday”)%b:Abbreviated month name (“Mar”)%B:Full month name (“March”)%c:Date and time representation (“Tue Mar 27 15:25:06 2018”)%C:Year in 2 digits (“18”)%Y:Year in 4 digits (“2018”)%D, %x:Short MM/DD/YY date (“03/27/18”)%m:Month 01-12 (“03”)%d:Day of month 01-31 (“27”)%H:Hours in 24 format 00-23 (“15”)%I:Hours in 12 format 01-12 (“03”)%M:Minutes 00-59 (“25”)%S:Seconds 00-59 (“06”)%e:Millisecond part of the current second 000-999 (“008”)%f:Microsecond part of the current second 000000-999999 (“008789”)%F:Nanosecond part of the current second 000000000-999999999 (“008789123”)%p:AM/PM (“AM”)%r:12-hour clock (“03:25:06 PM”)%R:24-hour HH:MM time, equivalent to %H:%M (“15:25”)%T, %X:ISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S (“13:25:06”)%z:ISO 8601 offset from UTC in timezone ([+/-]HH:MM) (“-07:00”)%%:The % sign (“%”) --restart-epoch (optional) The hot restart epoch. (The number of times Envoy has been hot restarted instead of a fresh start). Defaults to 0 for the first start. This option tells Envoy whether to attempt to create the shared memory region needed for hot restart, or whether to open an existing one. It should be incremented every time a hot restart takes place. The hot restart wrapper sets the RESTART_EPOCH environment variable which should be passed to this option in most cases. --hot-restart-version (optional) Outputs an opaque hot restart compatibility version for the binary. This can be matched against the output of the GET /hot_restart_version admin endpoint to determine whether the new binary and the running binary are hot restart compatible. --service-cluster (optional) Defines the local service cluster name where Envoy is running. The local service cluster name is first sourced from the Bootstrap node message’s cluster field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if any of the following features are used: statsd, health check cluster verification, runtime override directory, user agent addition, HTTP global rate limiting, CDS, and HTTP tracing, either via this CLI option or in the bootstrap configuration. --service-node (optional) Defines the local service node name where Envoy is running. The local service node name is first sourced from the Bootstrap node message’s id field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if any of the following features are used: statsd, CDS, and HTTP tracing, either via this CLI option or in the bootstrap configuration. --service-zone (optional) Defines the local service zone where Envoy is running. The local service zone is first sourced from the Bootstrap node message’s locality.zone field. This CLI option provides an alternative method for specifying this value and will override any value set in bootstrap configuration. It should be set if discovery service routing is used and the discovery service exposes zone data, either via this CLI option or in the bootstrap configuration. The meaning of zone is context dependent, e.g. Availability Zone (AZ) on AWS, Zone on GCP, etc. --file-flush-interval-msec (optional) The file flushing interval in milliseconds. Defaults to 10 seconds. This setting is used during file creation to determine the duration between flushes of buffers to files. The buffer will flush every time it gets full, or every time the interval has elapsed, whichever comes first. Adjusting this setting is useful when tailing access logs in order to get more (or less) immediate flushing. --drain-time-s (optional) The time in seconds that Envoy will drain connections during a hot restart. See thehot restart overview for more information. Defaults to 600 seconds (10 minutes). Generally the drain time should be less than the parent shutdown time set via the --parent-shutdown-time-soption. How the two settings are configured depends on the specific deployment. In edge scenarios, it might be desirable to have a very long drain time. In service to service scenarios, it might be possible to make the drain and shutdown time much shorter (e.g., 60s/90s). --parent-shutdown-time-s (optional) The time in seconds that Envoy will wait before shutting down the parent process during a hot restart. See the hot restart overview for more information. Defaults to 900 seconds (15 minutes). --max-obj-name-len (optional) The maximum name length (in bytes) of the name field in a cluster/route_config/listener. This setting is typically used in scenarios where the cluster names are auto generated, and often exceed the built-in limit of 60 characters. Defaults to 60.AttentionThis setting affects the output of --hot-restart-version. If you started envoy with this option set to a non default value, you should use the same option (and same value) for subsequent hot restarts. --max-stats (optional) The maximum number of stats that can be shared between hot-restarts. This setting affects the output of --hot-restart-version; the same value must be used to hot restart. Defaults to 16384. --disable-hot-restart (optional) This flag disables Envoy hot restart for builds that have it enabled. By default, hot restart is enabled. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-20 22:49:46 "},"operations/hot_restarter.html":{"url":"operations/hot_restarter.html","title":"热重启 Python 包装器","keywords":"","body":"热重启 Python 包装器 通常情况下，Envoy 将会以热重启的方式进行配置变更和二进制更新。但是，在很多情况下，用户会希望使用标准进程管理器，例如 monit、runit 等。我们提供 /restarter/hot-restarter.py 来使这个过程简单明了。 调用重启程序方式如下: hot-restarter.py start_envoy.sh start_envoy.sh 可能像这样定义（使用 salt/jinja 类似的语法）： #!/bin/bash ulimit -n {{ pillar.get('envoy_max_open_files', '102400') }} exec /usr/sbin/envoy -c /etc/envoy/envoy.cfg --restart-epoch $RESTART_EPOCH --service-cluster {{ grains['cluster_name'] }} --service-node {{ grains['service_node'] }} --service-zone {{ grains.get('ec2_availability-zone', 'unknown') }} 在每次重启时，RESTART_EPOCH 环境变量是由重启程序设置，并且可以传递给 --restart-epoch 选项 重启程序可以处理以下信号： SIGTERM：将干净地终止所有子进程并退出。 SIGHUP：将重新调用作为第一个参数传递给热重启程序的脚本，来进行热重启。 SIGCHLD：如果任何子进程意外关闭，那么重启脚本将关闭所有内容并退出以避免处于意外状态。随后，控制进程管理器应该重新启动重启脚本以再次启动Envoy。 SIGUSR1：将作为重新打开所有访问日志的信号，转发给Envoy。可用于原子移动以及重新打开日志轮转。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-16 22:42:07 "},"operations/admin.html":{"url":"operations/admin.html","title":"管理接口","keywords":"","body":"管理接口 Envoy 在本地提供了一个管理界面，可以使用这一界面查询或修改服务器的各种数据。 v1 API 参考 v2 API 参考 注意 目前管理界面可以进行破坏性操作（例如关闭服务器），也可能暴露私有数据（例如统计数据、集群名称、证书信息等）。将管理界面限制到只能在安全网络之内进行访问是 非常必要 的。同时还要注意，提供管理界面服务的网络接口只接入到安全网络之中（防止 CSRF 攻击等目的）。要实现这些目标，可以进行相应的防火墙设置，或者只允许 localhost 访问。可以使用如下的 v2 配置来完成： admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 } 未来还会在管理界面中加入更多的安全相关的选项。这部分工作的进度在 Github Issue #2763 上进行跟踪。 所有的变更操作都应该通过 HTTP POST 方式进行。一段时间之内还是允许 HTTP GET 访问的，但是会有一条 Warning 日志。 GET / 渲染一个 HTML 主页，其中包含指向所有可用选项的链接。 GET /help 以字符表格的形式输出所有可用选项。 GET /certs 列出所有载入的 TLS 认证，包括文件名、序列号以及过期时间。 GET /clusters 列出集群管理器中配置的所有集群。这种信息中包含了已被发现的每个集群中的所有上游主机，以及每个主机的统计信息。如果服务发现出现问题需要除错，这些信息就很有帮助了。 集群管理器信息 version_info：string，最近载入的 CDS 的版本信息字符串。如果 Envoy 没有设置 CDS，则会输出来自于 version_info::static。 集群层信息 各优先级的断路器设置。 如果设置了外部检测，则显示相关信息。目前包含了平均成功率以及驱逐阈值的检测器。上一个检测周期中所获取的数据量不足，这些变量的值会被设置为-1。 added_via_api 标志：如果是静态配置添加的集群，这个项目的值就是 false；如果是使用 CDS API 添加的集群，这个值就会设置为 true。 主机统计数据 名称 类型 描述 cx_total Counter 连接总数 cx_active Gauge 活动连接总数 cx_connect_fail Counter 连接失败总数 rq_total Counter 请求总数 rq_timeout Counter 超时请求总数 rq_success Counter 收到非 5xx 响应的请求总数 rq_error Counter 收到 5xx 响应的请求总数 rq_active Gauge 活动请求总数 healthy String 主机健康状态，下面会有讲解 weight Integer 负载均衡权重（0-100） zone String 服务区域 canary Boolean 本主机是否为金丝雀 success_rate Double 请求成功率（0 - 100）。如果统计周期内没有足够的请求量进行运算，则返回 -1 主机健康状态 如果主机是健康的，那么 healthy 的输出为 healthy。 如果主机不健康，则 healthy 返回的是下面几个状态之一： /failed_active_hc：主动健康监测失败。 /failed_eds_health：EDS 标记该主机不健康。 /failed_outlier_check：外部检测失败。 GET /config_dump 用 JSON 序列化格式从多种 Envoy 组件中导出当前的配置。可以延伸阅读 response definition 的内容，来获得更详细的信息。 POST /cpuprofiler 启用或停用 CPU Profiler。编译时需要启用 gperftools。 POST /healthcheck/fail 设置健康状况为失败。需要配合 HTTP 健康检查过滤器来使用。这一功能可以停用一个服务器而无需进行关闭或者重启动操作。这个命令执行之后，不论过滤器如何设置，都会把全局范围内的健康检查设为失败。 POST /healthcheck/ok POST /healthcheck/fail 的逆向操作。同样需要配合 HTTP 健康检查过滤器来使用。 GET /hot_restart_version 参考 --hot-restart-version。 POST /logging 在不同的子组件上启用或者禁用不同级别的日志。一般只会在开发期间使用。 POST /quitquitquit 完全退出服务。 POST /reset_counters 把所有的计数器复位为 0。这在使用 GET /stats 协助调试的时候是很有用的功能。注意这个功能并不会删除任何发送给 statsd 的数据，它只会对 GET /stats 命令的输出造成影响。 GET /server_info 输出关于服务器的信息，内容格式类似： envoy 267724/RELEASE live 1571 1571 0 其中的字段包括： 进程名称 编译的 SHA 以及 Build 类型 当前热启动周期的启动时间 总的启动时间（包括所有的热启动周期） 当前的热启动周期 GET /stats 按需输出所有的统计内容。这个命令对于本地调试非常有用。Histograms 能够计算分位数并进行输出，即 P0，P25，P50，P75，P90，P99，P99.9和P100。每个分位数都是一种（区间值，累计值）的形式，区间值代表的是上次刷新以后的数值，累计值代表的是该实例启动以后的总计值。统计概览章节中提供了更多这方面的内容。 GET /stats?format=json 使用 JSON 格式输出 /stats，编程访问统计信息时可以使用这一格式。Counter 和 Gauge 会以（键，值）的形式出现。Histograms 会放在 \"histograms\" 节点之下，其中包含了 \"supported_quantiles\" 节点，其中列出了支持的分位数，以及这些分位数的计算结果。只有包含值的 Histograms 才会输出。 如果一个 Histogram 在这一区间没有更新，那么这一区间的所有分位数输出都是空的。 下面是一个输出样例： { \"histograms\": { \"supported_quantiles\": [ 0, 25, 50, 75, 90, 95, 99, 99.9, 100 ], \"computed_quantiles\": [ { \"name\": \"cluster.external_auth_cluster.upstream_cx_length_ms\", \"values\": [ {\"interval\": 0, \"cumulative\": 0}, {\"interval\": 0, \"cumulative\": 0}, {\"interval\": 1.0435787, \"cumulative\": 1.0435787}, {\"interval\": 1.0941565, \"cumulative\": 1.0941565}, {\"interval\": 2.0860023, \"cumulative\": 2.0860023}, {\"interval\": 3.0665233, \"cumulative\": 3.0665233}, {\"interval\": 6.046609, \"cumulative\": 6.046609}, {\"interval\": 229.57333,\"cumulative\": 229.57333}, {\"interval\": 260,\"cumulative\": 260} ] }, { \"name\": \"http.admin.downstream_rq_time\", \"values\": [ {\"interval\": null, \"cumulative\": 0}, {\"interval\": null, \"cumulative\": 0}, {\"interval\": null, \"cumulative\": 1.0435787}, {\"interval\": null, \"cumulative\": 1.0941565}, {\"interval\": null, \"cumulative\": 2.0860023}, {\"interval\": null, \"cumulative\": 3.0665233}, {\"interval\": null, \"cumulative\": 6.046609}, {\"interval\": null, \"cumulative\": 229.57333}, {\"interval\": null, \"cumulative\": 260} ] } ] } } GET /stats?format=prometheus 或者换个方式 GET /stats/prometheus，使用 Prometheus v0.0.4 格式输出统计数据。这样就可以和 Prometheus 进行集成了。当前只有 Counter 和 Gauge 会进行输出。未来的更新中会输出 Histogram。 GET /runtime 使用 JSON 格式按需输出所有运行时数值。运行时配置一节中，更详细的讲述了这些值的配置和使用。输出内容包括活动的重载后的运行时数值，以及每个键的堆栈。空字符串代表没有值，来自堆栈的最终有效值会用单独的键来做出标识，例如下面的输出： { \"layers\": [ \"disk\", \"override\", \"admin\", ], \"entries\": { \"my_key\": { \"layer_values\": [ \"my_disk_value\", \"\", \"\" ], \"final_value\": \"my_disk_value\" }, \"my_second_key\": { \"layer_values\": [ \"my_second_disk_value\", \"my_disk_override_value\", \"my_admin_override_value\" ], \"final_value\": \"my_admin_override_value\" } } } POST /runtime_modify?key1=value1&key2=value2&keyN=valueN 通过提交的参数对运行时数值进行添加或修改。要删除一个之前加入的键，只需要使用一个空值即可。注意这种删除操作，只适用于这一端点中使用重载方式加入的值；从磁盘中载入的值是能通过重载进行修改，无法删除。 注意 使用 /runtime_modify 端点要当心，这一变更是即时生效的。保障管理界面的安全性至关重要。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 10:56:23 "},"operations/stats_overview.html":{"url":"operations/stats_overview.html","title":"统计概览","keywords":"","body":"统计概览 Envoy 基于服务器的配置输出数字统计信息。统计信息在本地可以通过 GET /stats 命令查看，通常情况下，统计信息会发送到 statsd cluster。输出的统计信息记录在配置指南的相关部分中。一些更重要的统计信息总是会被使用到，这些信息可以查阅以下章节： HTTP 链接管理器 Upstream 集群 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-17 13:36:54 "},"operations/runtime.html":{"url":"operations/runtime.html","title":"运行时","keywords":"","body":"运行时 运行时配置 可被用于修改各项服务器设置而不必重启 Envoy。可用的运行时设置取决于服务器是如何配置的。它们在配置指导的相关章节中说明。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-19 16:21:24 "},"operations/fs_flags.html":{"url":"operations/fs_flags.html","title":"文件系统标志","keywords":"","body":"文件系统标志 Envoy 支持文件系统 \"标志\"在启动之后改变状态。在需要的情况下，该功能用于保持重启之间的变化。标志文件应该被放置在 flags_path 配置选项指定的目录中。当前支持的标志文件是： drain 如果这个文件存在，Envoy 将以 HC 失败模式启动，类似于 POST /healthcheck/fail命令被执行之后。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-20 22:28:10 "},"operations/traffic_capture.html":{"url":"operations/traffic_capture.html","title":"流量捕获","keywords":"","body":"流量捕获 Envoy 当前提供了一个实验性的传输套接字扩展，用于拦截流量并写入一个 protobuf 文件中。 警告 这个功能是实验性的，并存在一个已知的问题，当在给定的 socket 上出现很长的跟踪调用的时候会 OOM。 如果担心存在安全问题，也可以在构建时禁用它，请参阅https://github.com/envoyproxy/envoy/blob/master/bazel/README.md#disabling-extensions. 配置 捕获行为可以被配置在 Listener 和 Cluster 上，提供了一种能力，可以分别针对上行流量和下行流量进行拦截。 要配置流量捕获, 添加一个envoy.transport_sockets.capture配置到 listener 或 cluster 上. 如： transport_socket: name: envoy.transport_sockets.capture config: file_sink: path_prefix: /some/capture/path transport_socket: name: raw_buffer 若支持 TLS, 如： transport_socket: name: envoy.transport_sockets.capture config: file_sink: path_prefix: /some/capture/path transport_socket: name: ssl config: 这里的 TLS 配置会分别替换现有在 listener 或 cluster 上 下行流量 或 上行流量 的 TLS 配置. 任一的 socket 实例都会生成一个包含path前缀的跟踪文件. 如：/some/capture/path_0.pb PCAP 传播 生成的跟踪文件可以被转成 libpcap format, 可以使用如 Wireshark 和 capture2pcap 这样的工具进行分析, 如: bazel run @envoy_api//tools:capture2pcap /some/capture/path_0.pb path_0.pcap tshark -r path_0.pcap -d \"tcp.port==10000,http2\" -P 1 0.000000 127.0.0.1 → 127.0.0.1 HTTP2 157 Magic, SETTINGS, WINDOW_UPDATE, HEADERS 2 0.013713 127.0.0.1 → 127.0.0.1 HTTP2 91 SETTINGS, SETTINGS, WINDOW_UPDATE 3 0.013820 127.0.0.1 → 127.0.0.1 HTTP2 63 SETTINGS 4 0.128649 127.0.0.1 → 127.0.0.1 HTTP2 5586 HEADERS 5 0.130006 127.0.0.1 → 127.0.0.1 HTTP2 7573 DATA 6 0.131044 127.0.0.1 → 127.0.0.1 HTTP2 3152 DATA, DATA Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:49:20 "},"extending/extending.html":{"url":"extending/extending.html","title":"为自定义用例扩展 Envoy","keywords":"","body":"为自定义用例扩展 Envoy The Envoy architecture makes it fairly easily extensible via both network filters and HTTP filters. An example of how to add a network filter and structure the repository and build dependencies can be found at envoy-filter-example. Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 18:22:20 "},"faq/how_fast_is_envoy.html":{"url":"faq/how_fast_is_envoy.html","title":"Envoy 有多快？","keywords":"","body":"Envoy 有多快？ 我们经常被问到 到底 Enovy 有多快? 或是 Envoy 将在我的请求上加多少的延时? 答案是：看情况。 性能极度依赖于哪些 Envoy 特性被使用以及 Envoy 运行在如何的环境上。除此之外，运作准确完备的性能测试是一个非常有难度的任务， 而我们的项目组目前并没有相应的资源去支撑这个任务。 其实我们在关键路径上对 Envoy 已经做了非常多的性能调优工作，同时我们相信这些调优工作都卓有成效，但因为上诉观点我们并没有 对此发布一个官方的基准测试报告。我们会希望用户在他们自己的环境对 Envoy 使用类同于他们计划在生产环境的配置，然后做相应的基准测试。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-20 22:28:10 "},"faq/binaries.html":{"url":"faq/binaries.html","title":"从哪里能获得二进制文件？","keywords":"","body":"从哪里能获得二进制文件？ 请参考 这里。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-15 18:29:27 "},"faq/sni.html":{"url":"faq/sni.html","title":"如何设置 SNI？","keywords":"","body":"如何设置 SNI？ SNI 仅被 v2 配置/API 支持。 目前的实现中要求所有 过滤器链 中的 过滤器 必须是相同的。在以后的发布中,这个约束将会放宽, 我们将可以将SNI运用到完全不同的过滤器链中。 我们还可以将 域名匹配 运用到HTTP 连接管理中去选择不同的路由路线。 这是截至目前最常见的SNI使用场景。 以下是一个如何满足上诉条件的 YAML 范例。 address: socket_address: { address: 127.0.0.1, port_value: 1234 } filter_chains: - filter_chain_match: sni_domains: \"example.com\" tls_context: common_tls_context: tls_certificates: - certificate_chain: { filename: \"example_com_cert.pem\" } private_key: { filename: \"example_com_key.pem\" } filters: - name: envoy.http_connection_manager config: route_config: virtual_hosts: - routes: - match: { prefix: \"/\" } route: { cluster: service_foo } - filter_chain_match: sni_domains: \"www.example.com\" tls_context: common_tls_context: tls_certificates: - certificate_chain: { filename: \"www_example_com_cert.pem\" } private_key: { filename: \"www_example_com_key.pem\" } filters: - name: envoy.http_connection_manager config: route_config: virtual_hosts: - routes: - match: { prefix: \"/\" } route: { cluster: service_foo } Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-21 17:49:34 "},"faq/zone_aware_routing.html":{"url":"faq/zone_aware_routing.html","title":"如何设置 zone 可感知路由？","keywords":"","body":"如何设置 zone 可感知路由？ 在源服务（“cluster_a”）和目标服务（“cluster_b”）之间启用区域感知路由需要执行几个步骤。 源服务上的 Envoy 配置 本节介绍与源服务一起运行的 Envoy 的具体配置。要求如下： Envoy 必须使用 --service-zone 选项启动，该选项为当前主机定义区域。 源和目的地集群的定义都必须具有 sds 类型。 必须将 local_cluster_name 设置为源集群。 以下配置中仅列出了集群管理器的主要部分。 { \"sds\": \"{...}\", \"local_cluster_name\": \"cluster_a\", \"clusters\": [ { \"name\": \"cluster_a\", \"type\": \"sds\", }, { \"name\": \"cluster_b\", \"type\": \"sds\" } ] } 目的地服务上的 Envoy 配置 没有必要与目的地服务并排运行 Envoy，但重要的是目的地集群中的每台主机都注册源服务 Envoy 查询的发现服务。区域信息必须作为该响应的一部分提供。 下面的应答中只列出了与区域相关的数据。 { \"tags\": { \"az\": \"us-east-1d\" } } 基础设施搭建 上述配置对于区域感知路由是必需的，但是在不执行区域感知路由时存在某些情况。 验证步骤 使用每区域 Envoy 统计信息来监控跨区域流量。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 14:45:29 "},"faq/zipkin_tracing.html":{"url":"faq/zipkin_tracing.html","title":"如何设置 Zipkin 追踪？","keywords":"","body":"如何设置 Zipkin 追踪？ 可在 zipkin sandbox 配置中查看 zipkin 追踪配置范例。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-20 22:28:10 "},"faq/lb_panic_threshold.html":{"url":"faq/lb_panic_threshold.html","title":"我设置了健康检查，但是当有节点失败时，Envoy 又路由到那些节点，这是怎么回事？","keywords":"","body":"我设置了健康检查，但是当有节点失败时，Envoy 又路由到那些节点，这是怎么回事？ 这个功能因为负载均衡中恐慌阈值为人所知。 在上游站点发生大量的健康检查失败的时候，它被用来防止级联故障。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-20 22:28:10 "},"faq/concurrency_lb.html":{"url":"faq/concurrency_lb.html","title":"为什么 Round Robin 负载均衡看起来不起作用？","keywords":"","body":"为什么 Round Robin 负载均衡看起来不起作用？ Envoy 使用隔离方式的线程模型。这意味着工作线程以及相应的负载均衡器彼此不能互相协助。当使用例如 round robin 的负载均衡策略时，该策略也许不能非常好地操作多个工作线程。此时我们可以使用 --concurrency选项来调整需要运行的工作线程数。 隔离方式的运行模型也引致了如下情形，我们时常需要为每一个上游建立多个 HTTP/2 连接；且工作线程间无法共享连接池。 Copyright © ServiceMesher 2018 all right reserved，powered by Gitbook 最后更新于 2018-05-18 11:24:46 "}}